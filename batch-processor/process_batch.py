#!/usr/bin/env python3
"""
AWS Batch Î©îÏù∏ Ï≤òÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏
S3ÏóêÏÑú ÏÇ¨Ïö©Ïûê ÌñâÎèô Îç∞Ïù¥ÌÑ∞Î•º ÏùΩÏñ¥ÏÑú OpenCLIP Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÍ≥† PostgreSQLÏóê Ï†ÄÏû•
"""
import os
import sys
import json
import logging
import traceback
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import gzip
from pathlib import Path

import boto3
import pandas as pd
import numpy as np
from langchain_experimental.open_clip import OpenCLIPEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import requests
from dotenv import load_dotenv

env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    print(f"‚úÖ Loaded .env file from {env_path}")
else:
    print(f"‚ö†Ô∏è .env file not found at {env_path}")

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/tmp/batch_processing.log')
    ]
)
logger = logging.getLogger(__name__)

# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
AWS_REGION = os.getenv('AWS_REGION', 'ap-northeast-2')
S3_BUCKET = os.getenv('S3_BUCKET', os.getenv('S3_EVENTS_BUCKET', 'user-actions-data'))
S3_PREFIX = os.getenv('S3_PREFIX', 'user-actions/')
DATABASE_URL = os.getenv('DATABASE_URL')
WEBHOOK_URL = os.getenv('WEBHOOK_URL')  # Main EC2 webhook URL
BATCH_ID = os.getenv('BATCH_ID', f'batch_{int(datetime.now().timestamp())}')
JOB_NAME = os.getenv('AWS_BATCH_JOB_NAME', 'witple-vectorization-job')
JOB_ID = os.getenv('AWS_BATCH_JOB_ID', 'unknown')

# ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò ÏÑ§Ï†ï (Ïô∏Î∂ÄÌôî)
TIME_DECAY_LAMBDA = float(os.getenv('TIME_DECAY_LAMBDA', '0.0231'))  # 30Ïùº ÌõÑ 50% Í∞êÏá†

# ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî Î™®Îç∏ ÏÑ§Ï†ï (HuggingFace MiniLM)
TEXT_MODEL_NAME = "sentence-transformers/all-MiniLM-L12-v2"
TEXT_VECTOR_DIM = 384

# Ïù¥ÎØ∏ÏßÄ Î≤°ÌÑ∞Ìôî Î™®Îç∏ ÏÑ§Ï†ï (OpenCLIP)
OPENCLIP_IMAGE_MODEL_NAME = "ViT-B-32"  # Ïù¥ÎØ∏ÏßÄÏö© (512Ï∞®Ïõê)
OPENCLIP_IMAGE_CHECKPOINT = "laion2b_s34b_b79k"
IMAGE_VECTOR_DIM = 512

# Î°úÎìúÎêú ÌôòÍ≤ΩÎ≥ÄÏàò ÌôïÏù∏
logger = logging.getLogger(__name__)
print(f"üîß Environment Variables:")
print(f"  AWS_REGION: {AWS_REGION}")
print(f"  S3_BUCKET: {S3_BUCKET}")
print(f"  S3_PREFIX: {S3_PREFIX}")
print(f"  DATABASE_URL: {'***' if DATABASE_URL else 'NOT SET'}")
print(f"  WEBHOOK_URL: {WEBHOOK_URL}")
print(f"  AWS_ACCESS_KEY_ID: {'***' if os.getenv('AWS_ACCESS_KEY_ID') else 'NOT SET'}")
print(f"  TIME_DECAY_LAMBDA: {TIME_DECAY_LAMBDA} (30-day decay: {np.exp(-TIME_DECAY_LAMBDA * 30):.2f})")
print(f"  TEXT_VECTOR_DIM: {TEXT_VECTOR_DIM} (MiniLM ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞ Ï∞®Ïõê)")
print(f"  IMAGE_VECTOR_DIM: {IMAGE_VECTOR_DIM} (CLIP Ïù¥ÎØ∏ÏßÄ Î≤°ÌÑ∞ Ï∞®Ïõê)")

class BatchProcessor:
    def __init__(self):
        logger.info("üöÄ Initializing Batch Processor")
        
        # AWS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
        self.s3_client = boto3.client('s3', region_name=AWS_REGION)
        
        # ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî Î™®Îç∏ Î°úÎìú (HuggingFace MiniLM)
        logger.info(f"üì• Loading HuggingFace text model: {TEXT_MODEL_NAME}")
        self.text_embedding_model = HuggingFaceEmbeddings(
            model_name=TEXT_MODEL_NAME
        )

        # Ïù¥ÎØ∏ÏßÄ Î≤°ÌÑ∞Ìôî Î™®Îç∏ Î°úÎìú (OpenCLIP)
        logger.info(f"üì• Loading OpenCLIP image model: {OPENCLIP_IMAGE_MODEL_NAME}")
        self.image_embedding_model = OpenCLIPEmbeddings(
            model_name=OPENCLIP_IMAGE_MODEL_NAME,
            checkpoint=OPENCLIP_IMAGE_CHECKPOINT
        )

        # Î≤°ÌÑ∞ Ï∞®Ïõê ÏÑ§Ï†ï
        self.text_vector_dimension = TEXT_VECTOR_DIM
        self.image_vector_dimension = IMAGE_VECTOR_DIM
        logger.info(f"‚úÖ Text and image models loaded successfully.")
        logger.info(f"üéØ Text vectors (MiniLM): {self.text_vector_dimension} dimensions")
        logger.info(f"üéØ Image vectors (CLIP): {self.image_vector_dimension} dimensions")
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
        if DATABASE_URL:
            self.engine = create_engine(DATABASE_URL)
            self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
            logger.info("‚úÖ Database connection established")
        else:
            logger.warning("‚ö†Ô∏è DATABASE_URL not provided, database operations will be skipped")
            self.engine = None
            self.SessionLocal = None
        
        # NEW: DB Ï°∞Ìöå Í≤∞Í≥ºÎ•º Ï∫êÏã±ÌïòÏó¨ ÏÑ±Îä• Ìñ•ÏÉÅ
        self.place_overview_cache = {}

        # NEW: OpenCLIP Ïù∏ÏΩîÎî© Í≤∞Í≥º Ï∫êÏã±ÌïòÏó¨ Ï§ëÎ≥µ Ïó∞ÏÇ∞ Î∞©ÏßÄ
        self.bert_encoding_cache = {}
        self._cache_hits = 0
        self._cache_attempts = 0

        # ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Ï¥àÍ∏∞Ìôî
        self.stats = {
            'processed_files': 0,
            'processed_actions': 0,
            'processed_users': 0,
            'processed_places': 0,
            'errors': 0,
            'time_weighted_users': 0,  # ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©Îêú ÏÇ¨Ïö©Ïûê Ïàò
            'fallback_users': 0,       # ÌÖçÏä§Ìä∏ Í∏∞Î∞ò fallback ÏÇ¨Ïö©Ïûê Ïàò
            'start_time': datetime.now(),
            'end_time': None
        }

    # NEW: place_idÏôÄ categoryÎ°ú DBÏóêÏÑú overviewÎ•º Ï°∞ÌöåÌïòÎäî Ìï®Ïàò
    def _get_overview_from_db(self, numeric_id: int, category: str) -> Optional[str]:
        """Ï£ºÏñ¥ÏßÑ Ïà´Ïûê IDÏôÄ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú DBÏóêÏÑú overviewÎ•º Ï°∞ÌöåÌïòÍ≥† Ï∫êÏãúÏóê Ï†ÄÏû•Ìï©ÎãàÎã§."""
        cache_key = f"{numeric_id}:{category}"
        if cache_key in self.place_overview_cache:
            return self.place_overview_cache[cache_key]

        if not self.SessionLocal:
            return None

        db = self.SessionLocal()
        try:
            # place_recommendations ÌÖåÏù¥Î∏îÏùÑ Ï°∞ÌöåÌïúÎã§Í≥† Í∞ÄÏ†ï
            query = text("""
                SELECT overview FROM place_recommendations
                WHERE place_id = :numeric_id AND table_name = :category
                LIMIT 1
            """)
            result = db.execute(query, {'numeric_id': numeric_id, 'category': category}).scalar_one_or_none()
            
            self.place_overview_cache[cache_key] = result
            return result
        except Exception as e:
            logger.error(f"‚ùå DB lookup failed for ID={numeric_id}, Category={category}: {e}")
            self.place_overview_cache[cache_key] = None  # Ïã§Ìå®Ìïú Ï°∞ÌöåÎèÑ Ï∫êÏã±ÌïòÏó¨ Î∞òÎ≥µ Î∞©ÏßÄ
            return None
        finally:
            db.close()

    def _encode_text_with_cache(self, text: str) -> List[float]:
        """ÌÖçÏä§Ìä∏Î•º OpenCLIPÏúºÎ°ú Ïù∏ÏΩîÎî©ÌïòÎêò Ï∫êÏãúÎ•º ÌôúÏö©ÌïòÏó¨ Ï§ëÎ≥µ Ïó∞ÏÇ∞ Î∞©ÏßÄ"""
        # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
        self._cache_attempts += 1

        # ÌÖçÏä§Ìä∏Î•º Ï†ïÍ∑úÌôîÌïòÏó¨ Ï∫êÏãú ÌÇ§Î°ú ÏÇ¨Ïö©
        cache_key = text.strip()[:500]  # ÎÑàÎ¨¥ Í∏¥ ÌÖçÏä§Ìä∏Îäî ÏûòÎùºÏÑú ÏÇ¨Ïö©

        if cache_key in self.bert_encoding_cache:
            self._cache_hits += 1
            logger.debug(f"üî• Cache hit for text encoding: {cache_key[:50]}...")
            return self.bert_encoding_cache[cache_key]

        # Ï∫êÏãú ÎØ∏Ïä§ Ïãú MiniLM ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎî© ÏàòÌñâ (384Ï∞®Ïõê)
        vector = self.text_embedding_model.embed_query(text)
        # Î≤°ÌÑ∞ Ï∞®Ïõê Í≤ÄÏ¶ù Î∞è Ìå®Îî©/Ìä∏Î¶¨Î∞ç
        if len(vector) > TEXT_VECTOR_DIM:
            vector = vector[:TEXT_VECTOR_DIM]  # Ìä∏Î¶¨Î∞ç
        elif len(vector) < TEXT_VECTOR_DIM:
            vector = vector + [0.0] * (TEXT_VECTOR_DIM - len(vector))  # Ï†úÎ°ú Ìå®Îî©
        self.bert_encoding_cache[cache_key] = vector
        logger.debug(f"üß† Generated new encoding for: {cache_key[:50]}...")

        return vector

    def _encode_image_rgb(self, image_path_or_url: str) -> List[float]:
        """Ïù¥ÎØ∏ÏßÄÎ•º RGB Í∏∞Ï§Ä 512Ï∞®Ïõê Î≤°ÌÑ∞Î°ú Ïù∏ÏΩîÎî©"""
        cache_key = f"image:{image_path_or_url}"
        self._cache_attempts += 1

        # Ï∫êÏãú Ï°∞Ìöå
        if cache_key in self.bert_encoding_cache:
            self._cache_hits += 1
            logger.debug(f"üî• Cache hit for image encoding: {cache_key[:50]}...")
            return self.bert_encoding_cache[cache_key]

        try:
            # CLIPÏùÑ ÌÜµÌïú Ïù¥ÎØ∏ÏßÄ Ïù∏ÏΩîÎî© (512Ï∞®Ïõê)
            # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ ÌååÏùºÏù¥ÎÇò URLÏùÑ Ï≤òÎ¶¨ÌïòÎäî Í≤ΩÏö∞ embed_image Î©îÏÑúÎìú ÏÇ¨Ïö©
            # ÌòÑÏû¨Îäî Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö ÌÖçÏä§Ìä∏Î•º Ïù¥ÎØ∏ÏßÄ Î™®Îç∏Î°ú Ïù∏ÏΩîÎî©
            vector = self.image_embedding_model.embed_query(f"image: {image_path_or_url}")

            # Î≤°ÌÑ∞ Ï∞®Ïõê Í≤ÄÏ¶ù Î∞è Ìå®Îî©/Ìä∏Î¶¨Î∞ç (512Ï∞®Ïõê)
            if len(vector) > IMAGE_VECTOR_DIM:
                vector = vector[:IMAGE_VECTOR_DIM]  # Ìä∏Î¶¨Î∞ç
            elif len(vector) < IMAGE_VECTOR_DIM:
                vector = vector + [0.0] * (IMAGE_VECTOR_DIM - len(vector))  # Ï†úÎ°ú Ìå®Îî©

            self.bert_encoding_cache[cache_key] = vector
            logger.debug(f"üñºÔ∏è Generated new CLIP image encoding for: {cache_key[:50]}...")
            return vector

        except Exception as e:
            logger.error(f"‚ùå Error encoding image {image_path_or_url}: {e}")
            # ÏóêÎü¨ Ïãú Ï†úÎ°ú Î≤°ÌÑ∞ Î∞òÌôò
            return [0.0] * IMAGE_VECTOR_DIM

    def _generate_time_weighted_user_vector(self, user_id: str, data: Dict[str, Any], place_vectors: Dict[str, Any]) -> List[float]:
        """ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπòÎ•º Ï†ÅÏö©Ìïú ÏÇ¨Ïö©Ïûê Î≤°ÌÑ∞ ÏÉùÏÑ± (ÏïàÏ†ÑÏÑ± Í∞ïÌôî)"""
        try:
            positive_actions_info = []
            current_time_utc = datetime.now(timezone.utc)

            # 1. Í∏çÏ†ïÏ†Å ÌñâÎèô(Ï¢ãÏïÑÏöî/Î∂ÅÎßàÌÅ¨)ÏóêÏÑú ÏãúÍ∞Ñ Ï†ïÎ≥¥ÏôÄ Î≤°ÌÑ∞ ÏàòÏßë
            for action in data['actions']:
                if action.get('action_type') in ['like', 'bookmark']:
                    place_key = f"{action.get('place_category')}:{action.get('place_id')}"
                    action_time = action.get('action_time')

                    # ÏãúÍ∞Ñ Ï†ïÎ≥¥ÏôÄ Ïû•ÏÜå Î≤°ÌÑ∞Í∞Ä Î™®Îëê ÏûàÎäî Í≤ΩÏö∞Îßå Ï≤òÎ¶¨
                    if action_time and place_key in place_vectors:
                        try:
                            # ÏïàÏ†ÑÌïú ÏãúÍ∞Ñ ÌååÏã± (Îã§ÏñëÌïú ÌòïÏãù ÏßÄÏõê)
                            timestamp_str = str(action_time)
                            if timestamp_str.endswith('Z'):
                                timestamp_str = timestamp_str.replace('Z', '+00:00')

                            timestamp = datetime.fromisoformat(timestamp_str)

                            # naive datetimeÏùÑ UTCÎ°ú Ï≤òÎ¶¨
                            if timestamp.tzinfo is None:
                                timestamp = timestamp.replace(tzinfo=timezone.utc)

                            vector = place_vectors[place_key]['behavior_vector']
                            positive_actions_info.append({
                                'vector': vector,
                                'timestamp': timestamp,
                                'action_type': action.get('action_type')
                            })

                        except (ValueError, TypeError, AttributeError) as e:
                            logger.debug(f"Failed to parse timestamp '{action_time}' for user {user_id}: {e}")
                            continue

            # 2. ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©Ìïú Î≤°ÌÑ∞ ÌèâÍ∑† Í≥ÑÏÇ∞
            if positive_actions_info:
                vectors = np.array([item['vector'] for item in positive_actions_info])

                # ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (ÏßÄÏàò Í∞êÏá† Î™®Îç∏)
                weights = []
                total_weight = 0

                for item in positive_actions_info:
                    time_diff_days = (current_time_utc - item['timestamp']).total_seconds() / 86400
                    time_weight = np.exp(-TIME_DECAY_LAMBDA * max(0, time_diff_days))  # ÏùåÏàò Î∞©ÏßÄ

                    # ÌñâÎèô Ïú†ÌòïÎ≥Ñ Ï∂îÍ∞Ä Í∞ÄÏ§ëÏπò (Î∂ÅÎßàÌÅ¨ > Ï¢ãÏïÑÏöî)
                    action_weight = 1.5 if item['action_type'] == 'bookmark' else 1.0

                    final_weight = time_weight * action_weight
                    weights.append(final_weight)
                    total_weight += final_weight

                if total_weight > 0:
                    # Í∞ÄÏ§ë ÌèâÍ∑† Í≥ÑÏÇ∞
                    vector = np.average(vectors, axis=0, weights=weights).tolist()
                    logger.info(f"Generated time-weighted vector for user {user_id} from {len(vectors)} actions (total_weight: {total_weight:.3f})")
                    self.stats['time_weighted_users'] += 1
                    return vector

            # 3. Fallback: ÏãúÍ∞Ñ Ï†ïÎ≥¥Í∞Ä ÏóÜÍ±∞ÎÇò Í∏çÏ†ïÏ†Å ÌñâÎèôÏù¥ ÏóÜÎäî Í≤ΩÏö∞
            logger.info(f"User {user_id}: No time-weighted actions available, using behavior text fallback")
            behavior_text = self.create_user_behavior_text(data)
            self.stats['fallback_users'] += 1
            return self._encode_text_with_cache(behavior_text)

        except Exception as e:
            # 4. ÏôÑÏ†ÑÌïú ÏòàÏô∏ Ï≤òÎ¶¨: Î™®Îì† Ïã§Ìå® Ïãú Í∏∞Î≥∏ ÌÖçÏä§Ìä∏ Í∏∞Î∞ò Î≤°ÌÑ∞ ÏÇ¨Ïö©
            logger.error(f"Time-weighted vector generation failed for user {user_id}: {e}")
            behavior_text = self.create_user_behavior_text(data)
            self.stats['fallback_users'] += 1
            return self._encode_text_with_cache(behavior_text)

    def list_s3_files(self, max_files: int = 100) -> List[Dict[str, Any]]:
        """S3ÏóêÏÑú Ï≤òÎ¶¨Ìï† ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
        logger.info(f"üîç Searching for files in s3://{S3_BUCKET}/{S3_PREFIX}")
        
        files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')
        
        try:
            for page in paginator.paginate(
                Bucket=S3_BUCKET,
                Prefix=S3_PREFIX,
                MaxKeys=1000
            ):
                if 'Contents' not in page:
                    continue
                    
                for obj in page['Contents']:
                    # .json ÌååÏùºÎßå Ï≤òÎ¶¨ (batch- Ï†ëÎëêÏÇ¨Í∞Ä ÏûàÎäî ÌååÏùºÎì§)
                    if obj['Key'].endswith('.json') and 'batch-' in obj['Key']:
                        files.append({
                            'key': obj['Key'],
                            'size': obj['Size'],
                            'last_modified': obj['LastModified']
                        })
                        
                        if len(files) >= max_files:
                            break
                            
                if len(files) >= max_files:
                    break
                    
        except Exception as e:
            logger.error(f"‚ùå Failed to list S3 files: {str(e)}")
            raise
            
        logger.info(f"üìã Found {len(files)} files to process")
        return sorted(files, key=lambda x: x['last_modified'])
    
    def download_and_parse_s3_file(self, s3_key: str) -> List[Dict[str, Any]]:
        """S3 ÌååÏùºÏùÑ Îã§Ïö¥Î°úÎìúÌïòÍ≥† ÌååÏã±"""
        logger.info(f"üì• Downloading {s3_key}")
        
        try:
            # S3 Í∞ùÏ≤¥ Îã§Ïö¥Î°úÎìú
            response = self.s3_client.get_object(Bucket=S3_BUCKET, Key=s3_key)
            content = response['Body'].read()
            
            # GZIP ÏïïÏ∂ï Ïó¨Î∂Ä ÌôïÏù∏
            if response.get('ContentEncoding') == 'gzip':
                content = gzip.decompress(content)
            
            # JSON ÌååÏã±
            data = json.loads(content.decode('utf-8'))
            
            # Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ÏóêÏÑú actions Ï∂îÏ∂ú
            if 'actions' in data:
                actions = data['actions']
            else:
                # Îã®Ïùº Ïï°ÏÖò ÌååÏùºÏù∏ Í≤ΩÏö∞
                actions = [data] if isinstance(data, dict) else data
                
            logger.info(f"‚úÖ Parsed {len(actions)} actions from {s3_key}")
            return actions
            
        except Exception as e:
            logger.error(f"‚ùå Failed to download/parse {s3_key}: {str(e)}")
            self.stats['errors'] += 1
            return []
    
    def process_user_actions(self, actions: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌñâÎèô Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÍ≥† Î≤°ÌÑ∞ ÏÉùÏÑ±"""
        logger.info(f"üß† Processing {len(actions)} actions for vectorization")
        
        user_data = {}
        place_data = {}
        
        # 1. ÏÇ¨Ïö©ÏûêÎ≥Ñ/Ïû•ÏÜåÎ≥Ñ ÌñâÎèô ÏßëÍ≥Ñ
        for action in actions:
            user_id = action.get('user_id')
            place_id = action.get('place_id')
            place_category = action.get('place_category')
            action_type = action.get('action_type')
            
            if not all([user_id, place_id, place_category, action_type]):
                continue
                
            # ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ ÏßëÍ≥Ñ
            if user_id not in user_data:
                user_data[user_id] = {
                    'actions': [],
                    'total_likes': 0,
                    'total_bookmarks': 0,
                    'total_clicks': 0,
                    'places_visited': set(),
                    'categories_visited': set()
                }
            
            user_data[user_id]['actions'].append(action)
            user_data[user_id]['places_visited'].add(f"{place_category}:{place_id}")
            user_data[user_id]['categories_visited'].add(place_category)
            
            if action_type == 'like':
                user_data[user_id]['total_likes'] += 1
            elif action_type == 'bookmark':
                user_data[user_id]['total_bookmarks'] += 1
            elif action_type == 'click':
                user_data[user_id]['total_clicks'] += 1
            
            # Ïû•ÏÜå Îç∞Ïù¥ÌÑ∞ ÏßëÍ≥Ñ
            place_key = f"{place_category}:{place_id}"
            if place_key not in place_data:
                place_data[place_key] = {
                    'place_id': place_id,
                    'place_category': place_category,
                    'total_likes': 0,
                    'total_bookmarks': 0,
                    'total_clicks': 0,
                    'unique_users': set()
                }
            
            place_data[place_key]['unique_users'].add(user_id)
            
            if action_type == 'like':
                place_data[place_key]['total_likes'] += 1
            elif action_type == 'bookmark':
                place_data[place_key]['total_bookmarks'] += 1
            elif action_type == 'click':
                place_data[place_key]['total_clicks'] += 1
        
        # 2. Ïû•ÏÜåÎ≥Ñ Î≤°ÌÑ∞ ÏÉùÏÑ± (ÏôÑÏ†ÑÌûà Î®ºÏ†Ä ÏÉùÏÑ±ÌïòÏó¨ ÏÇ¨Ïö©Ïûê Î≤°ÌÑ∞ÏóêÏÑú Ï∞∏Ï°∞)
        place_vectors = {}
        logger.info(f"üè¢ Generating vectors for {len(place_data)} places")

        for place_key, data in place_data.items():
            try:
                # MODIFIED START
                place_full_id = data['place_id']
                place_category = data['place_category']

                try:
                    numeric_id = int(place_full_id.split('_')[1])
                except (IndexError, ValueError):
                    logger.warning(f"Could not parse numeric ID from {place_full_id}. Skipping overview lookup.")
                    numeric_id = None

                overview = None
                if numeric_id and self.SessionLocal:
                    overview = self._get_overview_from_db(numeric_id, place_category)

                if overview:
                    place_text = overview
                else:
                    place_text = f"Place category: {place_category} with {len(data['unique_users'])} visitors"

                # MiniLM ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞ ÏÉùÏÑ± (Ï∫êÏãú ÌôúÏö©ÌïòÏó¨ Ï§ëÎ≥µ Ïó∞ÏÇ∞ Î∞©ÏßÄ)
                vector = self._encode_text_with_cache(place_text)
                
                # Ïù∏Í∏∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞
                total_interactions = data['total_likes'] + data['total_bookmarks'] + data['total_clicks']
                popularity_score = min(total_interactions * 2, 100)  # Í∞ÑÎã®Ìïú Ïù∏Í∏∞ÎèÑ Í≥µÏãù
                engagement_score = min((data['total_likes'] + data['total_bookmarks']) * 5, 100)
                
                # ============================================================================
                # üöÄ ENHANCED SCORING SYSTEM (ÌåÄ Í≤ÄÌÜ† ÌõÑ Ï†ÅÏö©)
                # ============================================================================
                """
                # Í∞úÏÑ†Îêú Í∞ÄÏ§ëÏπò Í∏∞Î∞ò Ï†êÏàò Í≥ÑÏÇ∞
                ACTION_WEIGHTS = {
                    'click': 1.0,      # Í∏∞Î≥∏ Í¥ÄÏã¨ÎèÑ
                    'like': 3.0,       # 3Î∞∞ Í∞ÄÏ§ëÏπò (Í∏çÏ†ïÏ†Å Î∞òÏùë)
                    'bookmark': 5.0    # 5Î∞∞ Í∞ÄÏ§ëÏπò (Í∞ïÌïú ÏÑ†Ìò∏)
                }
                
                # 1. Í∞ÄÏ§ëÏπò Í∏∞Î∞ò Ïù∏Í∏∞ÎèÑ Ï†êÏàò
                weighted_score = (
                    data['total_clicks'] * ACTION_WEIGHTS['click'] +
                    data['total_likes'] * ACTION_WEIGHTS['like'] + 
                    data['total_bookmarks'] * ACTION_WEIGHTS['bookmark']
                )
                
                # Ï†ïÍ∑úÌôî (0-100 Ïä§ÏºÄÏùº) - Í∏∞Ï§Ä: click 50Í∞ú + like 10Í∞ú + bookmark 5Í∞ú = 100Ï†ê
                max_reference_score = (50 * 1.0) + (10 * 3.0) + (5 * 5.0)  # = 105
                enhanced_popularity_score = min((weighted_score / max_reference_score) * 100, 100)
                
                # 2. Ï∞∏Ïó¨ÎèÑ Ï†êÏàò (Ïï°ÏÖòÏùò Ïßà Ï§ëÏã¨)
                if total_interactions > 0:
                    high_value_actions = data['total_likes'] + data['total_bookmarks'] 
                    engagement_ratio = high_value_actions / total_interactions
                    base_engagement = engagement_ratio * 100
                    # Ï†àÎåÄÍ∞í Î≥¥Ï†ï (ÏµúÏÜåÌïúÏùò like/bookmarkÏù¥ ÏûàÏñ¥Ïïº ÎÜíÏùÄ Ï†êÏàò)
                    min_threshold_bonus = min(high_value_actions * 5, 20)  # ÏµúÎåÄ 20Ï†ê Î≥¥ÎÑàÏä§
                    enhanced_engagement_score = min(base_engagement + min_threshold_bonus, 100)
                else:
                    enhanced_engagement_score = 0.0
                
                # Í∏∞Ï°¥ Ï†êÏàòÏôÄ Í∞úÏÑ†Îêú Ï†êÏàòÎ•º Î™®Îëê Ï†ÄÏû•ÌïòÏó¨ A/B ÌÖåÏä§Ìä∏ Í∞ÄÎä•
                # popularity_score = round(enhanced_popularity_score, 2)
                # engagement_score = round(enhanced_engagement_score, 2)
                """
                
                place_vectors[place_key] = {
                    'place_id': data['place_id'],
                    'place_category': data['place_category'],
                    'behavior_vector': vector,
                    'combined_vector': vector,  # ÎèôÏùºÌïú Î≤°ÌÑ∞ ÏÇ¨Ïö©
                    'total_likes': data['total_likes'],
                    'total_bookmarks': data['total_bookmarks'],
                    'total_clicks': data['total_clicks'],
                    'unique_users': len(data['unique_users']),
                    'popularity_score': round(popularity_score, 2),
                    'engagement_score': round(engagement_score, 2)
                }
                
            except Exception as e:
                logger.error(f"‚ùå Failed to process place {place_key}: {str(e)}")
                self.stats['errors'] += 1
                continue

        logger.info(f"‚úÖ Completed place vector generation for {len(place_vectors)} places")

        # 3. ÏÇ¨Ïö©ÏûêÎ≥Ñ Î≤°ÌÑ∞ ÏÉùÏÑ± (Ïû•ÏÜå Î≤°ÌÑ∞ ÏôÑÏÑ± ÌõÑ)
        user_vectors = {}
        logger.info(f"üë§ Generating vectors for {len(user_data)} users")
        for user_id, data in user_data.items():
            try:
                # Í∞úÏÑ†Îêú ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò Î≤°ÌÑ∞ ÏÉùÏÑ±
                vector = self._generate_time_weighted_user_vector(user_id, data, place_vectors)

                # ÌñâÎèô Ï†êÏàò Í≥ÑÏÇ∞ (0-100 Ïä§ÏºÄÏùº)
                total_actions = len(data['actions'])
                like_score = min((data['total_likes'] / max(total_actions, 1)) * 100, 100)
                bookmark_score = min((data['total_bookmarks'] / max(total_actions, 1)) * 100, 100)
                click_score = min((data['total_clicks'] / max(total_actions, 1)) * 100, 100)

                # Îã§ÏñëÏÑ± Ï†êÏàò (Î∞©Î¨∏Ìïú Ïπ¥ÌÖåÍ≥†Î¶¨ Ïàò Í∏∞Î∞ò)
                diversity_score = min(len(data['categories_visited']) * 10, 100)

                user_vectors[user_id] = {
                    'user_id': user_id,
                    'behavior_vector': vector,
                    'like_score': round(like_score, 2),
                    'bookmark_score': round(bookmark_score, 2),
                    'click_score': round(click_score, 2),
                    'dwell_time_score': 0.0,  # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
                    'total_actions': total_actions,
                    'total_likes': data['total_likes'],
                    'total_bookmarks': data['total_bookmarks'],
                    'total_clicks': data['total_clicks'],
                    'last_action_date': datetime.now()
                }

            except Exception as e:
                logger.error(f"‚ùå Failed to process user {user_id}: {str(e)}")
                self.stats['errors'] += 1
                continue

        # Ï∫êÏãú Ìö®Ïú®ÏÑ± ÌÜµÍ≥Ñ
        total_cache_entries = len(self.bert_encoding_cache)
        cache_hit_ratio = 0
        if hasattr(self, '_cache_hits') and hasattr(self, '_cache_attempts'):
            cache_hit_ratio = (self._cache_hits / max(self._cache_attempts, 1)) * 100

        logger.info(f"‚úÖ Generated vectors for {len(user_vectors)} users and {len(place_vectors)} places")
        logger.info(f"üî• Text encoding cache: {total_cache_entries} entries, {cache_hit_ratio:.1f}% hit ratio")
        logger.info(f"‚è∞ Time-weighted vectors: {self.stats['time_weighted_users']} users, Fallback: {self.stats['fallback_users']} users")

        self.stats['processed_users'] = len(user_vectors)
        self.stats['processed_places'] = len(place_vectors)
        
        return {
            'user_vectors': user_vectors,
            'place_vectors': place_vectors
        }
    
    def create_user_behavior_text(self, user_data: Dict[str, Any]) -> str:
        """ÏÇ¨Ïö©Ïûê ÌñâÎèô Îç∞Ïù¥ÌÑ∞Î•º MiniLM ÏûÖÎ†•Ïö© ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò"""
        categories = list(user_data['categories_visited'])
        total_actions = len(user_data['actions'])
        
        # ÌñâÎèô Ìå®ÌÑ¥ÏùÑ ÏûêÏó∞Ïñ¥Î°ú ÌëúÌòÑ
        behavior_parts = []
        
        if user_data['total_likes'] > 0:
            behavior_parts.append(f"likes {user_data['total_likes']} places")
        if user_data['total_bookmarks'] > 0:
            behavior_parts.append(f"bookmarks {user_data['total_bookmarks']} places")
        if user_data['total_clicks'] > 0:
            behavior_parts.append(f"clicks {user_data['total_clicks']} places")
        
        behavior_text = f"User interested in {', '.join(categories)} categories, " + \
                       f"performed {total_actions} actions: " + \
                       ', '.join(behavior_parts)
        
        return behavior_text[:512]  # MiniLM ÌÖçÏä§Ìä∏ ÏûÖÎ†• Í∏∏Ïù¥ Ï†úÌïú
    
    def save_to_database(self, vectors_data: Dict[str, Dict[str, Any]]) -> bool:
        """Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•"""
        if not self.SessionLocal:
            logger.warning("‚ö†Ô∏è Database not available, skipping database save")
            return False
            
        logger.info("üíæ Saving vectors to database")
        
        db = self.SessionLocal()
        user_success_count = 0
        place_success_count = 0
        user_failures = []
        place_failures = []
        
        try:
            user_vectors = vectors_data['user_vectors']
            place_vectors = vectors_data['place_vectors']
            
            logger.info(f"üìä Processing {len(user_vectors)} user vectors and {len(place_vectors)} place vectors")
            
            # ÏÇ¨Ïö©Ïûê Î≤°ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏/ÏÇΩÏûÖ
            for user_id, data in user_vectors.items():
                try:
                    # UPSERT ÏøºÎ¶¨ Ïã§Ìñâ
                    result = db.execute(text("""
                        INSERT INTO user_behavior_vectors 
                        (user_id, behavior_vector, like_score, bookmark_score, click_score, dwell_time_score,
                         total_actions, total_likes, total_bookmarks, total_clicks, last_action_date, vector_updated_at)
                        VALUES (:user_id, :behavior_vector, :like_score, :bookmark_score, :click_score, :dwell_time_score,
                                :total_actions, :total_likes, :total_bookmarks, :total_clicks, :last_action_date, NOW())
                        ON CONFLICT (user_id) DO UPDATE SET
                            behavior_vector = EXCLUDED.behavior_vector,
                            like_score = EXCLUDED.like_score,
                            bookmark_score = EXCLUDED.bookmark_score,
                            click_score = EXCLUDED.click_score,
                            dwell_time_score = EXCLUDED.dwell_time_score,
                            total_actions = EXCLUDED.total_actions,
                            total_likes = EXCLUDED.total_likes,
                            total_bookmarks = EXCLUDED.total_bookmarks,
                            total_clicks = EXCLUDED.total_clicks,
                            last_action_date = EXCLUDED.last_action_date,
                            vector_updated_at = NOW()
                    """), {
                        'user_id': user_id,
                        'behavior_vector': data['behavior_vector'],
                        'like_score': data['like_score'],
                        'bookmark_score': data['bookmark_score'],
                        'click_score': data['click_score'],
                        'dwell_time_score': data['dwell_time_score'],
                        'total_actions': data['total_actions'],
                        'total_likes': data['total_likes'],
                        'total_bookmarks': data['total_bookmarks'],
                        'total_clicks': data['total_clicks'],
                        'last_action_date': data['last_action_date']
                    })
                    user_success_count += 1
                    logger.debug(f"‚úÖ User vector {user_id} saved successfully")
                except Exception as e:
                    user_failures.append(f"{user_id}: {str(e)}")
                    logger.error(f"‚ùå Failed to save user vector {user_id}: {str(e)}")
                    continue
            
            # Ïû•ÏÜå Î≤°ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏/ÏÇΩÏûÖ
            for place_key, data in place_vectors.items():
                try:
                    result = db.execute(text("""
                        INSERT INTO place_vectors 
                        (place_id, place_category, behavior_vector, combined_vector,
                         total_likes, total_bookmarks, total_clicks, unique_users,
                         popularity_score, engagement_score, vector_updated_at, stats_updated_at)
                        VALUES (:place_id, :place_category, :behavior_vector, :combined_vector,
                                :total_likes, :total_bookmarks, :total_clicks, :unique_users,
                                :popularity_score, :engagement_score, NOW(), NOW())
                        ON CONFLICT (place_id, place_category) DO UPDATE SET
                            behavior_vector = EXCLUDED.behavior_vector,
                            combined_vector = EXCLUDED.combined_vector,
                            total_likes = EXCLUDED.total_likes,
                            total_bookmarks = EXCLUDED.total_bookmarks,
                            total_clicks = EXCLUDED.total_clicks,
                            unique_users = EXCLUDED.unique_users,
                            popularity_score = EXCLUDED.popularity_score,
                            engagement_score = EXCLUDED.engagement_score,
                            vector_updated_at = NOW(),
                            stats_updated_at = NOW()
                    """), {
                        'place_id': data['place_id'],
                        'place_category': data['place_category'],
                        'behavior_vector': data['behavior_vector'],
                        'combined_vector': data['combined_vector'],
                        'total_likes': data['total_likes'],
                        'total_bookmarks': data['total_bookmarks'],
                        'total_clicks': data['total_clicks'],
                        'unique_users': data['unique_users'],
                        'popularity_score': data['popularity_score'],
                        'engagement_score': data['engagement_score']
                    })
                    place_success_count += 1
                    logger.debug(f"‚úÖ Place vector {place_key} saved successfully")
                except Exception as e:
                    place_failures.append(f"{place_key}: {str(e)}")
                    logger.error(f"‚ùå Failed to save place vector {place_key}: {str(e)}")
                    continue
            
            # Ìä∏ÎûúÏû≠ÏÖò Ïª§Î∞ã
            db.commit()
            
            # ÏÉÅÏÑ∏Ìïú Í≤∞Í≥º Î°úÍπÖ
            logger.info(f"üìä Database save results:")
            logger.info(f"  - User vectors: {user_success_count}/{len(user_vectors)} saved successfully")
            logger.info(f"  - Place vectors: {place_success_count}/{len(place_vectors)} saved successfully")
            
            if user_failures:
                logger.warning(f"‚ö†Ô∏è User vector failures: {len(user_failures)}")
                for failure in user_failures[:5]:  # ÏµúÎåÄ 5Í∞úÎßå Î°úÍ∑∏
                    logger.warning(f"  - {failure}")
                if len(user_failures) > 5:
                    logger.warning(f"  - ... and {len(user_failures) - 5} more failures")
            
            if place_failures:
                logger.warning(f"‚ö†Ô∏è Place vector failures: {len(place_failures)}")
                for failure in place_failures[:5]:  # ÏµúÎåÄ 5Í∞úÎßå Î°úÍ∑∏
                    logger.warning(f"  - {failure}")
                if len(place_failures) > 5:
                    logger.warning(f"  - ... and {len(place_failures) - 5} more failures")
            
            # ÏÑ±Í≥µ Í∏∞Ï§Ä: Ï†ÑÏ≤¥Ïùò 80% Ïù¥ÏÉÅÏù¥ ÏÑ±Í≥µÌï¥Ïïº Ìï®
            total_expected = len(user_vectors) + len(place_vectors)
            total_success = user_success_count + place_success_count
            success_rate = total_success / total_expected if total_expected > 0 else 0
            
            if success_rate >= 0.8:
                logger.info(f"‚úÖ Database save completed successfully (success rate: {success_rate:.1%})")
                return True
            else:
                logger.error(f"‚ùå Database save failed (success rate: {success_rate:.1%} < 80%)")
                return False
            
        except Exception as e:
            logger.error(f"‚ùå Database save failed with exception: {str(e)}")
            db.rollback()
            return False
        finally:
            db.close()
    
    def send_webhook_notification(self, success: bool, error_message: Optional[str] = None):
        """Main EC2Ïóê Ï≤òÎ¶¨ ÏôÑÎ£å ÏïåÎ¶º Ï†ÑÏÜ°"""
        if not WEBHOOK_URL:
            logger.warning("‚ö†Ô∏è WEBHOOK_URL not provided, skipping webhook notification")
            return
            
        logger.info(f"üì° Sending webhook notification to {WEBHOOK_URL}")
        
        try:
            webhook_data = {
                'job_id': JOB_ID,
                'job_name': JOB_NAME,
                'job_status': 'SUCCEEDED' if success else 'FAILED',
                'batch_id': BATCH_ID,
                'processed_records': self.stats['processed_actions'],
                'processing_time_seconds': (self.stats['end_time'] - self.stats['start_time']).total_seconds(),
                's3_input_path': f's3://{S3_BUCKET}/{S3_PREFIX}',
                'error_message': error_message,
                'metadata': {
                    'processed_files': self.stats['processed_files'],
                    'processed_users': self.stats['processed_users'],
                    'processed_places': self.stats['processed_places'],
                    'errors': self.stats['errors']
                }
            }
            
            response = requests.post(
                WEBHOOK_URL,
                json=webhook_data,
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                logger.info("‚úÖ Webhook notification sent successfully")
            else:
                logger.error(f"‚ùå Webhook notification failed: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to send webhook: {str(e)}")
    
    def run(self):
        """Î©îÏù∏ Ï≤òÎ¶¨ Î°úÏßÅ Ïã§Ìñâ"""
        logger.info("üéØ Starting batch processing")
        
        try:
            # 1. S3 ÌååÏùº Î™©Î°ù Ï°∞Ìöå
            files = self.list_s3_files(max_files=50)  # ÌïúÎ≤àÏóê ÏµúÎåÄ 50Í∞ú ÌååÏùº Ï≤òÎ¶¨
            
            if not files:
                logger.info("‚úÖ No files to process")
                self.send_webhook_notification(success=True)
                return
            
            # 2. ÌååÏùºÎ≥ÑÎ°ú Ï≤òÎ¶¨
            all_actions = []
            
            for file_info in files:
                actions = self.download_and_parse_s3_file(file_info['key'])
                if actions:
                    all_actions.extend(actions)
                    self.stats['processed_files'] += 1
            
            if not all_actions:
                logger.info("‚úÖ No actions to process")
                self.send_webhook_notification(success=True)
                return
                
            self.stats['processed_actions'] = len(all_actions)
            logger.info(f"üìä Total actions to process: {len(all_actions)}")
            
            # 3. Î≤°ÌÑ∞Ìôî Ï≤òÎ¶¨
            vectors_data = self.process_user_actions(all_actions)
            
            # 4. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
            db_success = self.save_to_database(vectors_data)
            
            # 5. Ï≤òÎ¶¨ ÏôÑÎ£å ÏïåÎ¶º
            self.stats['end_time'] = datetime.now()
            
            if db_success:
                logger.info("üéâ Batch processing completed successfully")
                self.send_webhook_notification(success=True)
            else:
                logger.error("‚ùå Database save failed")
                self.send_webhook_notification(success=False, error_message="Database save failed")
                
        except Exception as e:
            logger.error(f"‚ùå Batch processing failed: {str(e)}")
            logger.error(traceback.format_exc())
            
            self.stats['end_time'] = datetime.now()
            self.send_webhook_notification(success=False, error_message=str(e))
            raise

def main():
    """Î©îÏù∏ ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏"""
    logger.info("üöÄ Witple Batch Processor Starting")
    
    # ÌôòÍ≤Ω Î≥ÄÏàò Í≤ÄÏ¶ù
    required_env_vars = ['DATABASE_URL']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"‚ùå Missing required environment variables: {missing_vars}")
        sys.exit(1)
    
    try:
        processor = BatchProcessor()
        processor.run()
        logger.info("üéâ Batch processing completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {str(e)}")
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()