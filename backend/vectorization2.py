# íŒŒì¼ëª…: vectorization2.py (ì™„ì „ ê°œì„  ë²„ì „)

import numpy as np
import asyncpg
import asyncio
import logging
from typing import Dict, List, Any, Optional
from contextlib import asynccontextmanager
from dataclasses import dataclass
import json
import time
import re
# Faiss ì„ íƒì  ì„í¬íŠ¸ (Docker í™˜ê²½ í˜¸í™˜ì„±)
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    faiss = None
    print("âš ï¸ Faiss not available - ANN features will be disabled")

import pickle
import os
from threading import Lock

# í†µí•© ì„¤ì • íŒŒì¼ ì‚¬ìš© (backend í™˜ê²½ ëŒ€ì‘)
try:
    from recommendation_config import config as CONFIG
except ImportError:
    try:
        from .recommendation_config import config as CONFIG
    except ImportError:
        # Fallback ì„¤ì •
        from dataclasses import dataclass
        @dataclass
        class FallbackConfig:
            database_url: str = "postgresql://user:pass@localhost/db"
            min_pool_size: int = 3
            max_pool_size: int = 15
            db_timeout: int = 5
            candidate_limit: int = 2000
            similarity_weight: float = 0.5
            popularity_weight: float = 0.5
            min_similarity_threshold: float = 0.1
            vector_cache_size: int = 1000
            cache_ttl_seconds: int = 300
            action_weights: Dict[str, float] = None
            preference_weights: Dict[str, float] = None
            travel_style_bonuses: Dict[str, Dict[str, float]] = None
            def __post_init__(self):
                if self.action_weights is None:
                    self.action_weights = {'click': 1.0, 'like': 3.0, 'bookmark': 5.0}
                if self.preference_weights is None:
                    self.preference_weights = {'region': 0.4, 'category': 0.3, 'tag': 0.3, 'max_tag_score': 10.0, 'popularity_normalizer': 1000.0}
                if self.travel_style_bonuses is None:
                    self.travel_style_bonuses = {'luxury': {'accommodation': 0.1, 'restaurants': 0.1}}
        CONFIG = FallbackConfig()
        CONFIG.__post_init__()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================

def safe_cosine_similarity(X: np.ndarray, Y: np.ndarray, use_ann: bool = False, faiss_manager=None) -> np.ndarray:
    """ì•ˆì „í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ANN ì§€ì› ë²„ì „)"""
    try:
        # ANN ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ìš”ì²­ëœ ê²½ìš° (ì•ˆì „í•˜ê²Œ ê²€ì‚¬)
        if (use_ann and
            faiss_manager is not None and
            hasattr(faiss_manager, 'index') and
            faiss_manager.index is not None and
            hasattr(faiss_manager, 'search')):
            try:
                # Yì˜ ê¸¸ì´ í™•ì¸
                target_length = len(Y) if hasattr(Y, '__len__') else Y.shape[0] if hasattr(Y, 'shape') else 100
                search_k = min(target_length, 100)

                # ANN ê²€ìƒ‰ ìˆ˜í–‰ (XëŠ” ì¿¼ë¦¬ ë²¡í„°)
                place_ids, scores = faiss_manager.search(X, k=search_k)

                # ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
                similarity_dict = {place_id: score for place_id, score in zip(place_ids, scores)}

                # Yì˜ ê¸¸ì´ì— ë§ì¶° ì ìˆ˜ ë°°ì—´ ìƒì„±
                if len(scores) >= target_length:
                    return np.array(scores[:target_length])
                else:
                    # ë¶€ì¡±í•œ ê²½ìš° 0.0ìœ¼ë¡œ íŒ¨ë”©
                    padded_scores = scores + [0.0] * (target_length - len(scores))
                    return np.array(padded_scores)

            except Exception as e:
                logger.warning(f"âš ï¸ ANN search failed, falling back to cosine similarity: {e}")

        # ê¸°ì¡´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (Fallback)
        # None ê°’ ê²€ì¦
        if X is None or Y is None:
            return np.array([0.0])

        # ì…ë ¥ì„ numpy ë°°ì—´ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        X = np.asarray(X, dtype=np.float32)
        Y = np.asarray(Y, dtype=np.float32)

        # ë¹ˆ ë°°ì—´ ê²€ì¦
        if X.size == 0 or Y.size == 0:
            return np.array([0.0])

        # ì°¨ì› ë§ì¶”ê¸°
        if X.ndim == 1:
            X = X.reshape(1, -1)
        if Y.ndim == 1:
            Y = Y.reshape(1, -1)

        # ì°¨ì› ì¼ì¹˜ í™•ì¸
        if X.shape[1] != Y.shape[1]:
            logger.warning(f"Vector dimension mismatch: X={X.shape[1]}, Y={Y.shape[1]}")
            return np.zeros(Y.shape[0])

        # ë²¡í„°í™”ëœ NaN/Inf ì²˜ë¦¬ (ë” ë¹ ë¦„)
        X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=-1.0)
        Y = np.nan_to_num(Y, nan=0.0, posinf=1.0, neginf=-1.0)

        # L2 norm ê³„ì‚° (axis=1ì—ì„œ keepdims=Trueë¡œ íš¨ìœ¨ì )
        X_norm = np.linalg.norm(X, axis=1, keepdims=True)
        Y_norm = np.linalg.norm(Y, axis=1, keepdims=True)

        # 0 ë²¡í„° ë§ˆìŠ¤í¬ ìƒì„± (í•œ ë²ˆì— ì²˜ë¦¬)
        zero_mask_X = (X_norm == 0).flatten()
        zero_mask_Y = (Y_norm == 0).flatten()

        if np.any(zero_mask_X) or np.any(zero_mask_Y):
            return np.zeros(Y.shape[0])

        # ì •ê·œí™” (in-place ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        X = X / X_norm
        Y = Y / Y_norm

        # í–‰ë ¬ ê³± (ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ë²•)
        similarities = np.einsum('ij,kj->ik', X, Y).flatten()

        # ìµœì¢… NaN/Inf ê°’ ì²˜ë¦¬
        similarities = np.nan_to_num(similarities, nan=0.0, posinf=1.0, neginf=-1.0)

        # ìœ ì‚¬ë„ ë²”ìœ„ í´ë¦¬í•‘ (-1 ~ 1)
        similarities = np.clip(similarities, -1.0, 1.0)

        return similarities

    except Exception as e:
        logger.error(f"âŒ Cosine similarity calculation failed: {e}")
        # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
        try:
            return np.zeros(Y.shape[0] if hasattr(Y, 'shape') and Y.ndim > 1 else 1)
        except:
            return np.array([0.0])


def calculate_weighted_popularity_score(place_data: Dict[str, int]) -> float:
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ì •ê·œí™”)"""
    try:
        weighted_score = (
            place_data.get('total_clicks', 0) * CONFIG.action_weights['click'] +
            place_data.get('total_likes', 0) * CONFIG.action_weights['like'] +
            place_data.get('total_bookmarks', 0) * CONFIG.action_weights['bookmark']
        )

        # ë™ì  ì •ê·œí™” (ìƒìœ„ 1% ê¸°ì¤€ì  ì‚¬ìš©)
        reference_score = 100  # ê¸°ë³¸ ê¸°ì¤€ì , ì‹¤ì œë¡œëŠ” í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê³„ì‚° ê°€ëŠ¥
        normalized_score = min((weighted_score / reference_score) * 100, 100)

        return round(normalized_score, 2)

    except Exception as e:
        logger.error(f"âŒ Popularity score calculation failed: {e}")
        return 0.0


def calculate_engagement_score(place_data: Dict[str, int]) -> float:
    """ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚° (like/bookmark ë¹„ìœ¨ ê¸°ë°˜)"""
    try:
        total_interactions = (
            place_data.get('total_clicks', 0) +
            place_data.get('total_likes', 0) +
            place_data.get('total_bookmarks', 0)
        )

        if total_interactions == 0:
            return 0.0

        high_value_actions = place_data.get('total_likes', 0) + place_data.get('total_bookmarks', 0)
        engagement_ratio = high_value_actions / total_interactions

        # ì ˆëŒ€ê°’ ë³´ì • ì¶”ê°€
        min_threshold_bonus = min(high_value_actions * 2, 20)  # ìµœëŒ€ 20ì  ë³´ë„ˆìŠ¤
        final_score = min((engagement_ratio * 100) + min_threshold_bonus, 100)

        return round(final_score, 2)

    except Exception as e:
        logger.error(f"âŒ Engagement score calculation failed: {e}")
        return 0.0


def safe_json_dumps(data: Any, **kwargs) -> str:
    """UTF-8 ì•ˆì „ JSON ì§ë ¬í™”"""
    try:
        # ê¸°ë³¸ ì„¤ì •
        default_kwargs = {
            'ensure_ascii': False,
            'separators': (',', ':'),
            'default': str
        }
        default_kwargs.update(kwargs)

        # ë°ì´í„° ì •ë¦¬ - ì˜ëª»ëœ ë¬¸ì ì œê±°
        cleaned_data = _clean_json_data(data)

        return json.dumps(cleaned_data, **default_kwargs)
    except (UnicodeDecodeError, UnicodeEncodeError) as e:
        logger.warning(f"âš ï¸ UTF-8 encoding issue, falling back to ASCII: {e}")
        # ASCII ëª¨ë“œë¡œ í´ë°±
        return json.dumps(data, ensure_ascii=True, default=str, **kwargs)
    except Exception as e:
        logger.error(f"âŒ JSON serialization failed: {e}")
        return "{}"


def _clean_json_data(data: Any) -> Any:
    """JSON ë°ì´í„°ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ìë“¤ì„ ì •ë¦¬"""
    if isinstance(data, str):
        # ì˜ëª»ëœ UTF-8 ë¬¸ìë‚˜ surrogate ë¬¸ì ì œê±°
        try:
            # ìœ íš¨í•˜ì§€ ì•Šì€ UTF-8 ë¬¸ì ì œê±°
            cleaned = data.encode('utf-8', errors='ignore').decode('utf-8')
            # ì œì–´ ë¬¸ì ì œê±° (íƒ­, ì¤„ë°”ê¿ˆì€ ìœ ì§€)
            cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned)
            return cleaned
        except Exception:
            return str(data)
    elif isinstance(data, dict):
        return {k: _clean_json_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [_clean_json_data(item) for item in data]
    else:
        return data


def validate_vector_data(vector_data: Any) -> Optional[np.ndarray]:
    """ë²¡í„° ë°ì´í„° ê²€ì¦ ë° ë³€í™˜ (PostgreSQL vector íƒ€ì… ë° ARRAY íƒ€ì… ì§€ì›)"""
    try:
        if vector_data is None:
            return None

        # PostgreSQL ARRAY íƒ€ì… ì²˜ë¦¬ (user_behavior_vectors.behavior_vector)
        if isinstance(vector_data, list):
            vector = np.array(vector_data, dtype=np.float32)

        # PostgreSQL vector íƒ€ì… ë¬¸ìì—´ ì²˜ë¦¬ (place_recommendations.vector, posts.image_vector)
        elif isinstance(vector_data, str):
            # vector íƒ€ì…ì€ "[1,2,3]" í˜•íƒœì˜ ë¬¸ìì—´
            if vector_data.startswith('[') and vector_data.endswith(']'):
                # PostgreSQL vector íƒ€ì… íŒŒì‹±
                vector_str = vector_data.strip('[]')
                if vector_str:
                    vector_list = [float(x.strip()) for x in vector_str.split(',')]
                    vector = np.array(vector_list, dtype=np.float32)
                else:
                    return None
            else:
                # JSON ë¬¸ìì—´ ì‹œë„
                vector_data = json.loads(vector_data)
                vector = np.array(vector_data, dtype=np.float32)

        # ì´ë¯¸ numpy ë°°ì—´ì¸ ê²½ìš°
        elif isinstance(vector_data, np.ndarray):
            vector = vector_data.astype(np.float32)

        # ê¸°íƒ€ ìˆ«ì íƒ€ì…
        else:
            vector = np.array(vector_data, dtype=np.float32)

        # ì°¨ì› ë° ìœ íš¨ì„± ê²€ì‚¬
        if vector.size == 0:
            return None

        if np.isnan(vector).any() or np.isinf(vector).any():
            logger.warning("Vector contains NaN or Inf values")
            return None

        return vector

    except Exception as e:
        logger.error(f"âŒ Vector validation failed: {e}")
        return None


# ============================================================================
# ğŸ” ANN (Approximate Nearest Neighbor) ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤
# ============================================================================

class FaissIndexManager:
    """Faiss ê¸°ë°˜ ANN ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤ (ì„ íƒì  ì‚¬ìš©)"""

    def __init__(self, vector_dim: int = 768, index_type: str = "IVF"):
        self.vector_dim = vector_dim
        self.index_type = index_type
        self.index = None
        self.place_ids = []  # ì¸ë±ìŠ¤ì˜ ë²¡í„°ì™€ ë§¤í•‘ë˜ëŠ” place_id ëª©ë¡
        self.is_trained = False
        self.lock = Lock()
        self.index_file_path = "faiss_index.bin"
        self.metadata_file_path = "faiss_metadata.pkl"
        self.available = FAISS_AVAILABLE

    def _create_index(self, n_vectors: int) -> faiss.Index:
        """ë²¡í„° ê°œìˆ˜ì— ë”°ë¥¸ ìµœì  ì¸ë±ìŠ¤ ìƒì„±"""
        if n_vectors < 1000:
            # ì‘ì€ ë°ì´í„°ì…‹: Flat (ì •í™•)
            return faiss.IndexFlatIP(self.vector_dim)
        elif n_vectors < 10000:
            # ì¤‘ê°„ ë°ì´í„°ì…‹: IVF
            nlist = min(int(np.sqrt(n_vectors)), 100)
            quantizer = faiss.IndexFlatIP(self.vector_dim)
            return faiss.IndexIVFFlat(quantizer, self.vector_dim, nlist)
        else:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹: IVF + PQ
            nlist = min(int(np.sqrt(n_vectors)), 1000)
            quantizer = faiss.IndexFlatIP(self.vector_dim)
            m = 64  # PQ segments
            return faiss.IndexIVFPQ(quantizer, self.vector_dim, nlist, m, 8)

    def build_index(self, vectors: np.ndarray, place_ids: List[int]):
        """ë²¡í„° ë°ì´í„°ë¡œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not self.available:
            logger.warning("âš ï¸ Faiss not available, skipping index build")
            return

        with self.lock:
            try:
                n_vectors = len(vectors)
                logger.info(f"ğŸ”¨ Building Faiss index for {n_vectors} vectors")

                # ë²¡í„° ì •ê·œí™” (ë‚´ì  -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                faiss.normalize_L2(vectors)

                # ì¸ë±ìŠ¤ ìƒì„±
                self.index = self._create_index(n_vectors)
                self.place_ids = place_ids.copy()

                # í›ˆë ¨ í•„ìš”í•œ ì¸ë±ìŠ¤ ì²˜ë¦¬
                if hasattr(self.index, 'train'):
                    logger.info("ğŸ¯ Training index...")
                    self.index.train(vectors)
                    self.is_trained = True

                # ë²¡í„° ì¶”ê°€
                self.index.add(vectors)
                logger.info(f"âœ… Index built successfully: {self.index.ntotal} vectors")

                # ì¸ë±ìŠ¤ ì €ì¥
                self.save_index()

            except Exception as e:
                logger.error(f"âŒ Failed to build index: {e}")
                raise

    def search(self, query_vector: np.ndarray, k: int = 50) -> tuple:
        """ANN ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.available:
            return [], []

        with self.lock:
            if self.index is None:
                return [], []

            try:
                # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
                query_vector = query_vector.reshape(1, -1).astype(np.float32)
                faiss.normalize_L2(query_vector)

                # ê²€ìƒ‰ ìˆ˜í–‰
                scores, indices = self.index.search(query_vector, k)

                # ê²°ê³¼ í•„í„°ë§ (ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ)
                valid_mask = indices[0] != -1
                valid_indices = indices[0][valid_mask]
                valid_scores = scores[0][valid_mask]

                # place_id ë§¤í•‘
                place_ids = [self.place_ids[idx] for idx in valid_indices]

                return place_ids, valid_scores.tolist()

            except Exception as e:
                logger.error(f"âŒ ANN search failed: {e}")
                return [], []

    def save_index(self):
        """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            if self.index is not None:
                faiss.write_index(self.index, self.index_file_path)

                # ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata = {
                    'place_ids': self.place_ids,
                    'vector_dim': self.vector_dim,
                    'index_type': self.index_type,
                    'is_trained': self.is_trained
                }
                with open(self.metadata_file_path, 'wb') as f:
                    pickle.dump(metadata, f)

                logger.info("ğŸ’¾ Index saved to disk")
        except Exception as e:
            logger.error(f"âŒ Failed to save index: {e}")

    def load_index(self) -> bool:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        if not self.available:
            return False

        try:
            if os.path.exists(self.index_file_path) and os.path.exists(self.metadata_file_path):
                # ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(self.index_file_path)

                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(self.metadata_file_path, 'rb') as f:
                    metadata = pickle.load(f)

                self.place_ids = metadata['place_ids']
                self.vector_dim = metadata['vector_dim']
                self.index_type = metadata['index_type']
                self.is_trained = metadata['is_trained']

                logger.info(f"ğŸ“‚ Index loaded: {self.index.ntotal} vectors")
                return True
        except Exception as e:
            logger.error(f"âŒ Failed to load index: {e}")

        return False

    def update_index(self, new_vectors: np.ndarray, new_place_ids: List[int]):
        """ì¸ë±ìŠ¤ì— ìƒˆ ë²¡í„° ì¶”ê°€ (ê°„ë‹¨í•œ ì¬êµ¬ì¶• ë°©ì‹)"""
        with self.lock:
            try:
                # ê¸°ì¡´ ë²¡í„° ì¶”ì¶œ (Faissì—ì„œ ì§ì ‘ ì¶”ì¶œì€ ë³µì¡í•˜ë¯€ë¡œ ì¬êµ¬ì¶•)
                all_place_ids = self.place_ids + new_place_ids

                # ìƒˆ ë²¡í„° ì •ê·œí™”
                faiss.normalize_L2(new_vectors)

                # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ìƒˆ ë²¡í„°ë§Œ ì¶”ê°€
                if self.index is not None and hasattr(self.index, 'add'):
                    self.index.add(new_vectors)
                    self.place_ids.extend(new_place_ids)
                    logger.info(f"â• Added {len(new_vectors)} vectors to index")
                else:
                    logger.warning("Index type doesn't support incremental updates. Consider rebuilding.")

            except Exception as e:
                logger.error(f"âŒ Failed to update index: {e}")
                raise


# ============================================================================
# ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤
# ============================================================================

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì¿¼ë¦¬ ê´€ë¦¬"""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
        self._initialized = False

    async def initialize(self):
        """Connection Pool ì´ˆê¸°í™”"""
        try:
            self.pool = await asyncpg.create_pool(
                self.database_url,
                min_size=CONFIG.min_pool_size,
                max_size=CONFIG.max_pool_size,
                command_timeout=CONFIG.db_timeout,
                server_settings={
                    'application_name': 'unified_recommendation_engine',
                    'tcp_keepalives_idle': '60',    # TCP keepalive ì„¤ì •
                    'tcp_keepalives_interval': '30',
                    'tcp_keepalives_count': '3'
                },
                # ê³ ê¸‰ ìµœì í™” ì„¤ì •
                setup=self._setup_connection if hasattr(CONFIG, 'pool_pre_ping') and CONFIG.pool_pre_ping else None
            )
            self._initialized = True
            logger.info(f"âœ… Database pool initialized: {CONFIG.min_pool_size}-{CONFIG.max_pool_size} connections")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize database pool: {e}")
            raise

    async def _setup_connection(self, connection):
        """ì—°ê²° ì´ˆê¸° ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)"""
        try:
            await connection.execute("SET work_mem = '64MB'")  # ì‘ì—… ë©”ëª¨ë¦¬ ì¦ê°€
            await connection.execute("SET random_page_cost = 1.1")  # SSD ìµœì í™”
            await connection.execute("SET effective_cache_size = '512MB'")  # ìºì‹œ í¬ê¸°
        except Exception as e:
            logger.warning(f"Connection setup optimization failed: {e}")

    async def close(self):
        """Connection Pool ì •ë¦¬"""
        if self.pool:
            await self.pool.close()
            logger.info("ğŸ”Œ Database pool closed")

    @asynccontextmanager
    async def get_connection(self):
        """ì•ˆì „í•œ DB ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if not self._initialized or not self.pool:
            raise RuntimeError("Database pool not initialized. Call initialize() first.")

        connection = None
        try:
            connection = await self.pool.acquire()
            yield connection
        except Exception as e:
            logger.error(f"âŒ Database connection error: {e}")
            raise
        finally:
            if connection:
                await self.pool.release(connection)

    async def execute_query(self, query: str, *args) -> List[Dict]:
        """ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰"""
        async with self.get_connection() as conn:
            try:
                result = await conn.fetch(query, *args)
                return [dict(row) for row in result]
            except Exception as e:
                logger.error(f"âŒ Query execution failed: {query[:100]}... Error: {e}")
                raise

    async def execute_single_query(self, query: str, *args) -> Optional[Any]:
        """ë‹¨ì¼ ê°’ ì¿¼ë¦¬ ì‹¤í–‰"""
        async with self.get_connection() as conn:
            try:
                return await conn.fetchval(query, *args)
            except Exception as e:
                logger.error(f"âŒ Single query execution failed: {query[:100]}... Error: {e}")
                raise


# ============================================================================
# ğŸ¯ í†µí•© ì¶”ì²œ ì—”ì§„ (ì™„ì „ ê°œì„  ë²„ì „)
# ============================================================================

class UnifiedRecommendationEngine:
    """
    ì™„ì „íˆ ê°œì„ ëœ í†µí•© ì¶”ì²œ ì—”ì§„
    - Connection Pool ê´€ë¦¬
    - ë²¡í„° ìºì‹±
    - ì—ëŸ¬ ë³µêµ¬
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """

    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or CONFIG.database_url
        self.db_manager = DatabaseManager(self.database_url)

        # ANN ì¸ë±ìŠ¤ ë§¤ë‹ˆì € ì¶”ê°€
        self.faiss_manager = FaissIndexManager(vector_dim=768)
        self.ann_enabled = False

        # ê³„ì¸µì  ìºì‹± ì‹œìŠ¤í…œ (ê°œì„ )
        self.vector_cache: Dict[str, Dict] = {}  # ê¸°ë³¸ ë²¡í„° ìºì‹œ
        self.cache_timestamps: Dict[str, float] = {}

        # ì „ìš© ìºì‹œë“¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        self.user_data_cache: Dict[str, Dict] = {}  # ì‚¬ìš©ì í†µí•© ë°ì´í„°
        self.user_data_timestamps: Dict[str, float] = {}

        self.place_batch_cache: Dict[str, List[Dict]] = {}  # ì¥ì†Œ ë°°ì¹˜ ë°ì´í„°
        self.place_batch_timestamps: Dict[str, float] = {}

        self.similarity_cache: Dict[str, np.ndarray] = {}  # ìœ ì‚¬ë„ ê²°ê³¼
        self.similarity_timestamps: Dict[str, float] = {}

        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'personalized_requests': 0,
            'popular_requests': 0,
            'avg_response_time': 0.0,
            'ann_searches': 0,
            'cosine_searches': 0
        }

    def _convert_s3_urls_to_https(self, place: Dict) -> Dict:
        """ì´ë¯¸ì§€ URLì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (DBì˜ ë‹¤ì–‘í•œ í˜•íƒœ ì²˜ë¦¬)"""
        if not place.get('image_urls'):
            return place

        try:
            image_urls = place['image_urls']

            # 1. JSON ë°°ì—´ ë¬¸ìì—´ í˜•íƒœ: ["url1", "url2"]
            if isinstance(image_urls, str) and image_urls.startswith('['):
                import json
                urls_list = json.loads(image_urls)
                place['image_urls'] = urls_list
                logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ URL ë³€í™˜ ì™„ë£Œ - ì¥ì†Œ: {place.get('name')}, URLs: {len(urls_list)}ê°œ")

            # 2. PostgreSQL ë°°ì—´ í˜•íƒœ: {url1,url2,url3}
            elif isinstance(image_urls, str) and image_urls.startswith('{'):
                urls_str = image_urls.strip('{}')
                if urls_str:
                    urls = [url.strip().strip('"') for url in urls_str.split(',')]
                    https_urls = []
                    for url in urls:
                        if url.startswith('s3://'):
                            # S3 URLì„ HTTPSë¡œ ë³€í™˜
                            bucket = url.split('/')[2]
                            key = '/'.join(url.split('/')[3:])
                            https_url = f"https://{bucket}.s3.ap-northeast-2.amazonaws.com/{key}"
                            https_urls.append(https_url)
                        else:
                            https_urls.append(url)
                    place['image_urls'] = https_urls
                else:
                    place['image_urls'] = []

            # 3. ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            elif isinstance(image_urls, list):
                https_urls = []
                for url in image_urls:
                    if url and url.startswith('s3://'):
                        bucket = url.split('/')[2]
                        key = '/'.join(url.split('/')[3:])
                        https_url = f"https://{bucket}.s3.ap-northeast-2.amazonaws.com/{key}"
                        https_urls.append(https_url)
                    else:
                        https_urls.append(url)
                place['image_urls'] = https_urls

            # 4. ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš° (ë°°ì—´ì´ ì•„ë‹Œ)
            elif isinstance(image_urls, str) and (image_urls.startswith('http') or image_urls.startswith('s3://')):
                if image_urls.startswith('s3://'):
                    bucket = image_urls.split('/')[2]
                    key = '/'.join(image_urls.split('/')[3:])
                    https_url = f"https://{bucket}.s3.ap-northeast-2.amazonaws.com/{key}"
                    place['image_urls'] = [https_url]
                else:
                    place['image_urls'] = [image_urls]
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” í˜•íƒœ
                logger.warning(f"Unknown image_urls format for place {place.get('place_id')}: {type(image_urls)} - {str(image_urls)[:100]}")
                place['image_urls'] = []

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ URL ë³€í™˜ ì‹¤íŒ¨ for place {place.get('place_id')}: {e}")
            place['image_urls'] = []

        return place

        logger.info("ğŸš€ UnifiedRecommendationEngine v2.0 initialized")

    async def initialize(self):
        """ì—”ì§„ ì´ˆê¸°í™” (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í˜¸ì¶œ)"""
        await self.db_manager.initialize()

        # ANN ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„ (Faiss ê°€ìš©ì„±ì— ë”°ë¼)
        if FAISS_AVAILABLE:
            try:
                if self.faiss_manager.load_index():
                    self.ann_enabled = True
                    logger.info("âœ… ANN index loaded successfully")
                else:
                    logger.info("ğŸ“ ANN index not found. Will build when needed.")
            except Exception as e:
                logger.warning(f"âš ï¸ ANN index loading failed, disabling ANN: {e}")
                self.ann_enabled = False
        else:
            logger.info("âš ï¸ Faiss not available - ANN features disabled")
            self.ann_enabled = False

        logger.info("âœ… Recommendation engine fully initialized")

    async def build_ann_index(self):
        """ëª¨ë“  ì¥ì†Œ ë²¡í„°ë¡œ ANN ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            logger.info("ğŸ”¨ Building ANN index from database...")

            # ëª¨ë“  ì¥ì†Œì˜ ë²¡í„° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            query = """
                SELECT id, embedding_vector
                FROM locations
                WHERE embedding_vector IS NOT NULL
                ORDER BY id
            """

            results = await self.db_manager.execute_query(query)

            if not results:
                logger.warning("âŒ No vector data found for ANN index")
                return False

            # ë²¡í„° ë°ì´í„° ì¤€ë¹„
            vectors = []
            place_ids = []

            for row in results:
                vector = validate_vector_data(row['embedding_vector'])
                if vector is not None:
                    vectors.append(vector)
                    place_ids.append(row['id'])

            if len(vectors) == 0:
                logger.warning("âŒ No valid vectors found for ANN index")
                return False

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            vectors_array = np.vstack(vectors).astype(np.float32)

            # ì¸ë±ìŠ¤ êµ¬ì¶•
            self.faiss_manager.build_index(vectors_array, place_ids)
            self.ann_enabled = True

            logger.info(f"âœ… ANN index built successfully with {len(vectors)} vectors")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to build ANN index: {e}")
            self.ann_enabled = False
            return False

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.db_manager.close()
        self.vector_cache.clear()
        self.cache_timestamps.clear()

        # ANN ì¸ë±ìŠ¤ ì €ì¥
        if self.ann_enabled and self.faiss_manager.index is not None:
            self.faiss_manager.save_index()

        logger.info("ğŸ”Œ Recommendation engine closed")

    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        total_searches = self.stats['ann_searches'] + self.stats['cosine_searches']
        ann_ratio = self.stats['ann_searches'] / total_searches if total_searches > 0 else 0

        return {
            **self.stats,
            'total_searches': total_searches,
            'ann_usage_ratio': ann_ratio,
            'ann_enabled': self.ann_enabled,
            'index_size': self.faiss_manager.index.ntotal if self.faiss_manager.index else 0
        }

    def _is_cache_valid(self, cache_key: str, cache_type: str = 'vector') -> bool:
        """ê³„ì¸µì  ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        timestamps = {
            'vector': self.cache_timestamps,
            'user_data': self.user_data_timestamps,
            'place_batch': self.place_batch_timestamps,
            'similarity': self.similarity_timestamps
        }

        if cache_key not in timestamps.get(cache_type, {}):
            return False

        age = time.time() - timestamps[cache_type][cache_key]
        return age < CONFIG.cache_ttl_seconds

    def _update_cache(self, cache_key: str, data: Any, cache_type: str = 'vector'):
        """ê³„ì¸µì  ìºì‹œ ì—…ë°ì´íŠ¸ (LRU ì „ëµ)"""
        cache_configs = {
            'vector': (self.vector_cache, self.cache_timestamps, CONFIG.vector_cache_size),
            'user_data': (self.user_data_cache, self.user_data_timestamps, CONFIG.user_data_cache_size),
            'place_batch': (self.place_batch_cache, self.place_batch_timestamps, CONFIG.place_batch_cache_size),
            'similarity': (self.similarity_cache, self.similarity_timestamps, CONFIG.similarity_cache_size)
        }

        cache, timestamps, max_size = cache_configs.get(cache_type, cache_configs['vector'])

        # LRU ì œê±°
        if len(cache) >= max_size:
            oldest_key = min(timestamps.keys(), key=lambda k: timestamps[k])
            del cache[oldest_key]
            del timestamps[oldest_key]

        cache[cache_key] = data
        timestamps[cache_key] = time.time()

    async def get_recommendations(
        self,
        user_id: Optional[str],
        region: Optional[str] = None,
        category: Optional[str] = None,
        limit: int = 20,
        fast_mode: bool = False  # ë©”ì¸ í˜ì´ì§€ìš© ê³ ì† ëª¨ë“œ
    ) -> List[Dict]:
        """
        ë©”ì¸ ì¶”ì²œ API (ë™ì  ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì ìš©)
        - ì‹ ê·œ ê°€ì…ì: ìš°ì„ ìˆœìœ„ ì„ í˜¸ë„ íƒœê·¸ 100%
        - í–‰ë™ ë°ì´í„° ìˆëŠ” ì‚¬ìš©ì: ìš°ì„ ìˆœìœ„ 70%, í–‰ë™ ë°ì´í„° 30%
        """
        start_time = time.time()
        self.stats['total_requests'] += 1

        try:
            # íŒŒë¼ë¯¸í„° ê²€ì¦
            limit = max(1, min(limit, 100))  # 1-100 ì‚¬ì´ë¡œ ì œí•œ

            if not user_id:
                # ë¹„ë¡œê·¸ì¸ ì‚¬ìš©ì: ì¸ê¸° ì¶”ì²œë§Œ
                logger.info("ğŸŒŸ Popular recommendations for anonymous user")
                self.stats['popular_requests'] += 1
                return await self._get_popular_recommendations(region, category, limit, fast_mode)

            # ëª¨ë“  ë¡œê·¸ì¸ ì‚¬ìš©ìì—ê²Œ ì§€ì—­ë³„ ì„ í˜¸ë„ ì¶”ì²œ ì ìš©
            logger.info(f"ğŸŒ Regional preference recommendations for user {user_id}")

            # ì§€ì—­ ì§€ì • ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
            if region:
                # íŠ¹ì • ì§€ì—­ ì§€ì •ì‹œ: ìš°ì„ ìˆœìœ„ + ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í†µí•© ì¶”ì²œ
                logger.info(f"ğŸ¯ Regional diverse recommendations for user {user_id} in {region}")
                result = await self._get_regional_diverse_recommendations(user_id, region, category, limit)
                if not result:
                    # ë‹¤ì–‘í•œ ì¶”ì²œ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ìœ¼ë¡œ í´ë°±
                    logger.info(f"ğŸ“Š Fallback to priority-based recommendations for user {user_id}")
                    result = await self._get_preference_based_recommendations(user_id, region, category, limit)
                    if not result:
                        # ì„ í˜¸ë„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¸ê¸° ì¶”ì²œìœ¼ë¡œ í´ë°±
                        logger.info(f"ğŸ“Š Fallback to popular recommendations for user {user_id}")
                        result = await self._get_popular_recommendations(region, category, limit, fast_mode)
            else:
                # ì§€ì—­ ì§€ì • ì—†ì„ ë•Œ: ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ (ì „ì²´ ì§€ì—­ ëŒ€ìƒ)
                logger.info(f"ğŸ‘¤ User personalized recommendations for user {user_id}")
                result = await self._get_user_personalized_recommendations(user_id, limit)
                if not result:
                    # ì„ í˜¸ë„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¸ê¸° ì¶”ì²œìœ¼ë¡œ í´ë°±
                    logger.info(f"ğŸ“Š Fallback to popular recommendations for user {user_id}")
                    result = await self._get_popular_recommendations(region, category, limit, fast_mode)

            # ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            response_time = time.time() - start_time
            self._update_response_time(response_time)

            logger.info(f"âœ… Returned {len(result)} recommendations in {response_time:.3f}s")
            return result

        except Exception as e:
            logger.error(f"âŒ Recommendation failed: {e}")
            # ë¹ˆ ê²°ê³¼ë¼ë„ ì•ˆì „í•˜ê²Œ ë°˜í™˜
            return []

    async def _get_comprehensive_user_data_cached(self, user_id: str) -> Dict[str, Any]:
        """ìºì‹œëœ ì‚¬ìš©ì í†µí•© ë°ì´í„° ì¡°íšŒ (DB í˜¸ì¶œ ìµœì†Œí™”)"""
        cache_key = f"user_comprehensive:{user_id}"

        # ìºì‹œ í™•ì¸
        if self._is_cache_valid(cache_key, 'user_data'):
            self.stats['cache_hits'] += 1
            return self.user_data_cache[cache_key]

        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ DBì—ì„œ ì¡°íšŒ
        comprehensive_data = await self._get_comprehensive_user_data(user_id)
        self._update_cache(cache_key, comprehensive_data, 'user_data')
        return comprehensive_data

    async def _get_comprehensive_user_data(self, user_id: str) -> Dict[str, Any]:
        """
        í†µí•© ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ (DB í˜¸ì¶œ ìµœì†Œí™”)
        - í–‰ë™ ì ìˆ˜
        - ì„ í˜¸ë„ ì •ë³´
        - ë¶ë§ˆí¬ ë°ì´í„°
        - ë²¡í„° ë°ì´í„°
        ëª¨ë“  ê²ƒì„ í•œ ë²ˆì— ì¡°íšŒ
        """
        try:
            # ë‹¨ì¼ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ëª¨ë“  ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ
            async with self.db_manager.get_connection() as conn:
                # 1. í–‰ë™ ì ìˆ˜ ì¡°íšŒ (user_behavior_vectors í…Œì´ë¸” êµ¬ì¡°ì— ë§ì¶¤)
                behavior_query = """
                    SELECT COALESCE(
                        total_bookmarks + total_likes + total_clicks, 0
                    ) as behavior_score
                    FROM user_behavior_vectors
                    WHERE user_id = $1
                """

                # 2. ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ (user_preferences í…Œì´ë¸”ì—ì„œ)
                preferences_query = """
                    SELECT priority, accommodation, exploration, persona
                    FROM user_preferences
                    WHERE user_id = $1
                """

                # 3. ì„ í˜¸ë„ íƒœê·¸ ì¡°íšŒ
                tags_query = """
                    SELECT tag, weight
                    FROM user_preference_tags
                    WHERE user_id = $1
                    ORDER BY weight DESC
                """

                # 4. í–‰ë™ ë²¡í„° ì¡°íšŒ
                vector_query = """
                    SELECT behavior_vector
                    FROM user_behavior_vectors
                    WHERE user_id = $1 AND behavior_vector IS NOT NULL
                """

                # 5. ë¶ë§ˆí¬ ë°ì´í„° ì¡°íšŒ
                bookmarks_query = """
                    SELECT places
                    FROM saved_locations
                    WHERE user_id = $1
                """

                # ìˆœì°¨ ì¿¼ë¦¬ ì‹¤í–‰ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    behavior_score = await conn.fetchval(behavior_query, user_id) or 0
                except Exception as e:
                    logger.error(f"Behavior query failed: {e}")
                    behavior_score = 0

                try:
                    preferences_data = await conn.fetchrow(preferences_query, user_id)
                except Exception as e:
                    logger.error(f"Preferences query failed: {e}")
                    preferences_data = None

                try:
                    tags_data = await conn.fetch(tags_query, user_id)
                except Exception as e:
                    logger.error(f"Tags query failed: {e}")
                    tags_data = []

                try:
                    vector_data = await conn.fetchval(vector_query, user_id)
                except Exception as e:
                    logger.error(f"Vector query failed: {e}")
                    vector_data = None

                try:
                    bookmarks_data = await conn.fetch(bookmarks_query, user_id)
                except Exception as e:
                    logger.error(f"Bookmarks query failed: {e}")
                    bookmarks_data = []

                # ê²°ê³¼ í†µí•©
                result = {
                    'behavior_score': behavior_score,
                    'user_preferences': {
                        'priority': preferences_data['priority'] if preferences_data else None,
                        'accommodation': preferences_data['accommodation'] if preferences_data else None,
                        'exploration': preferences_data['exploration'] if preferences_data else None,
                        'persona': preferences_data['persona'] if preferences_data else None
                    },
                    'preference_tags': {
                        row['tag']: row['weight'] for row in tags_data
                    },
                    'behavior_vector': validate_vector_data(vector_data),
                    'bookmarks': [dict(row) for row in bookmarks_data]
                }

                return result

        except Exception as e:
            logger.error(f"âŒ Failed to get comprehensive user data for {user_id}: {e}")
            return {
                'behavior_score': 0,
                'preferences': {'preference_tags': {}},
                'behavior_vector': None,
                'bookmarks': []
            }

    async def _get_user_behavior_score(self, user_id: str) -> int:
        """ì‚¬ìš©ì í–‰ë™ ì ìˆ˜ (ìºì‹œëœ ë°ì´í„° í™œìš©)"""
        cache_key = f"user_comprehensive:{user_id}"
        if self._is_cache_valid(cache_key):
            cached_data = self.vector_cache.get(cache_key)
            return cached_data.get('behavior_score', 0)

        # ìºì‹œê°€ ì—†ìœ¼ë©´ í†µí•© ë°ì´í„° ì¡°íšŒ
        comprehensive_data = await self._get_comprehensive_user_data(user_id)
        self._update_cache(cache_key, comprehensive_data)
        return comprehensive_data.get('behavior_score', 0)

    async def _get_user_behavior_vector_cached(self, user_id: str) -> Optional[np.ndarray]:
        """ìºì‹œë¥¼ í™œìš©í•œ ì‚¬ìš©ì ë²¡í„° ì¡°íšŒ (PostgreSQL ARRAY íƒ€ì… ì§€ì›)"""
        cache_key = f"user_vector:{user_id}"

        # ìºì‹œ í™•ì¸
        if self._is_cache_valid(cache_key):
            self.stats['cache_hits'] += 1
            cached_data = self.vector_cache[cache_key]
            if cached_data is not None:
                return np.array(cached_data, dtype=np.float32)
            return None

        # DBì—ì„œ ì¡°íšŒ (PostgreSQL ARRAY íƒ€ì…)
        try:
            query = """
                SELECT behavior_vector
                FROM user_behavior_vectors
                WHERE user_id = $1 AND behavior_vector IS NOT NULL
            """
            vector_data = await self.db_manager.execute_single_query(query, user_id)
            logger.info(f"ğŸ” [Vector] User {user_id} raw vector data from DB: {vector_data is not None}")
            if vector_data is not None:
                logger.info(f"ğŸ” [Vector] User {user_id} vector type: {type(vector_data)}, length: {len(vector_data) if hasattr(vector_data, '__len__') else 'N/A'}")

            validated_vector = validate_vector_data(vector_data)
            logger.info(f"ğŸ” [Vector] User {user_id} validated vector: {validated_vector is not None}")

            if validated_vector is not None:
                logger.info(f"ğŸ” [Vector] User {user_id} validated vector shape: {validated_vector.shape}, non-zero: {np.count_nonzero(validated_vector)}")

            # ìºì‹œì— ì €ì¥ (Noneë„ ìºì‹œí•˜ì—¬ ë°˜ë³µ ì¿¼ë¦¬ ë°©ì§€)
            if validated_vector is not None:
                self._update_cache(cache_key, validated_vector.tolist())
                return validated_vector
            else:
                self._update_cache(cache_key, None)
                return None

        except Exception as e:
            logger.error(f"âŒ Failed to get user vector for {user_id}: {e}")
            return None

    async def _get_user_bookmark_preferences(self, user_id: str) -> Dict[str, float]:
        """ì‚¬ìš©ì ë¶ë§ˆí¬ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„ ê³„ì‚°"""
        try:
            query = """
                SELECT places
                FROM saved_locations
                WHERE user_id = $1
            """

            bookmarks = await self.db_manager.execute_query(query, user_id)

            if not bookmarks:
                return {}

            # ì¹´í…Œê³ ë¦¬ë³„ ë¶ë§ˆí¬ ìˆ˜ ê³„ì‚°
            category_counts = {}
            total_bookmarks = 0

            for bookmark in bookmarks:
                places_text = bookmark.get('places', '')
                if ':' in places_text:
                    table_name = places_text.split(':', 1)[0]
                    category_counts[table_name] = category_counts.get(table_name, 0) + 1
                    total_bookmarks += 1

            # ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚° (ë¹„ìœ¨ ê¸°ë°˜)
            preferences = {}
            for category, count in category_counts.items():
                preferences[category] = count / total_bookmarks if total_bookmarks > 0 else 0

            logger.info(f"ğŸ“Š User {user_id} bookmark preferences: {preferences}")
            return preferences

        except Exception as e:
            logger.error(f"âŒ Failed to get bookmark preferences for {user_id}: {e}")
            return {}

    def _apply_category_quotas(
        self,
        recommendations: List[Dict],
        bookmark_preferences: Dict[str, float],
        limit: int
    ) -> List[Dict]:
        """ë¶ë§ˆí¬ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ í• ë‹¹ëŸ‰ì„ ì ìš©í•œ ê· í˜•ì¡íŒ ì¶”ì²œ"""
        try:
            if not bookmark_preferences or not recommendations:
                return recommendations[:limit]

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¶”ì²œ ë¶„ë¥˜
            category_recommendations = {}
            for rec in recommendations:
                category = rec.get('table_name', 'unknown')
                if category not in category_recommendations:
                    category_recommendations[category] = []
                category_recommendations[category].append(rec)

            # ì„ í˜¸ë„ ê¸°ë°˜ í• ë‹¹ëŸ‰ ê³„ì‚°
            result = []
            remaining_slots = limit

            # ë‹¤ì–‘ì„± ë³´ì¥: ìµœì†Œ 3ê°œ ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ í¬í•¨
            min_categories = min(3, len(category_recommendations))
            max_per_category = max(1, limit // min_categories)

            # 1ë‹¨ê³„: ì£¼ìš” ì„ í˜¸ ì¹´í…Œê³ ë¦¬ë¶€í„° í• ë‹¹
            sorted_preferences = sorted(bookmark_preferences.items(), key=lambda x: x[1], reverse=True)

            for category, preference_rate in sorted_preferences:
                if category not in category_recommendations:
                    continue

                # ì´ˆê°•ë ¥ ì„ í˜¸ë„ í¸í–¥ ì‹œìŠ¤í…œ (ë” ê³µê²©ì  í• ë‹¹)
                if preference_rate > 0.6:  # 60% ì´ìƒ ê°•í•œ ì„ í˜¸ (ê¸°ì¤€ ë‚®ì¶¤)
                    quota = min(max(int(limit * 0.5), 4), max_per_category + 3)  # 50% ë˜ëŠ” ê¸°ë³¸+3 (ë” ê³µê²©ì )
                elif preference_rate > 0.3:  # 30% ì´ìƒ ì„ í˜¸ ì¹´í…Œê³ ë¦¬ (ê¸°ì¤€ ë‚®ì¶¤)
                    quota = min(max(int(limit * 0.35), 3), max_per_category + 2)  # 35% ë˜ëŠ” ê¸°ë³¸+2
                elif preference_rate > 0.1:  # 10% ì´ìƒ ì„ í˜¸ ì¹´í…Œê³ ë¦¬ (ê¸°ì¤€ ë‚®ì¶¤)
                    quota = min(max(int(limit * 0.2), 2), max_per_category + 1)  # 20% ë˜ëŠ” ê¸°ë³¸+1
                else:  # ë‚®ì€ ì„ í˜¸ë„ ì¹´í…Œê³ ë¦¬
                    quota = max(1, int(limit * 0.05))  # ìµœì†Œ 5% í• ë‹¹

                # ì‹¤ì œ í• ë‹¹ ê°€ëŠ¥í•œ ìˆ˜ë§Œí¼ ì¶”ê°€
                available = min(quota, len(category_recommendations[category]), remaining_slots)
                result.extend(category_recommendations[category][:available])
                remaining_slots -= available

                logger.info(f"ğŸ“Š {category}: {preference_rate:.3f} -> {available}ê°œ í• ë‹¹")

                if remaining_slots <= 0:
                    break

            # 2ë‹¨ê³„: ë‚¨ì€ ìŠ¬ë¡¯ì„ ì ìˆ˜ìˆœìœ¼ë¡œ ì±„ì›€
            if remaining_slots > 0:
                used_ids = {rec.get('place_id') for rec in result}
                remaining_recs = [rec for rec in recommendations if rec.get('place_id') not in used_ids]
                result.extend(remaining_recs[:remaining_slots])

            return result[:limit]

        except Exception as e:
            logger.error(f"âŒ Category quota application failed: {e}")
            return recommendations[:limit]

    def _apply_category_shuffling(self, recommendations: List[Dict]) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ê°€ ì ì ˆíˆ ì„ì´ë„ë¡ ì¸í„°ë¦¬ë¹™ ë°©ì‹ìœ¼ë¡œ ì¬ë°°ì¹˜"""
        try:
            if len(recommendations) <= 3:
                return recommendations

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
            category_groups = {}
            for rec in recommendations:
                category = rec.get('table_name', 'unknown')
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(rec)

            # ì¹´í…Œê³ ë¦¬ê°€ 2ê°œ ì´í•˜ë©´ ì…”í”Œë§ ë¶ˆí•„ìš”
            if len(category_groups) <= 2:
                return recommendations

            logger.info(f"ğŸ”„ Shuffling {len(category_groups)} categories for better distribution")

            # ì¸í„°ë¦¬ë¹™ ë°©ì‹ìœ¼ë¡œ ì¬ë°°ì¹˜
            result = []
            category_indices = {cat: 0 for cat in category_groups.keys()}
            categories = list(category_groups.keys())

            # ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœí™˜í•˜ë©° ë°°ì¹˜
            for position in range(len(recommendations)):
                # í˜„ì¬ ë¼ìš´ë“œì—ì„œ ì‚¬ìš©í•  ì¹´í…Œê³ ë¦¬ ì„ íƒ
                category_idx = position % len(categories)
                current_category = categories[category_idx]

                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ ì•„ì§ ë°°ì¹˜ë˜ì§€ ì•Šì€ ì•„ì´í…œì´ ìˆëŠ”ì§€ í™•ì¸
                attempts = 0
                while attempts < len(categories):
                    cat_idx = category_indices[current_category]
                    if cat_idx < len(category_groups[current_category]):
                        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ ì•„ì´í…œ ì¶”ê°€
                        result.append(category_groups[current_category][cat_idx])
                        category_indices[current_category] += 1
                        break
                    else:
                        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ ì†Œì§„ë˜ë©´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ
                        category_idx = (category_idx + 1) % len(categories)
                        current_category = categories[category_idx]
                        attempts += 1

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ ì†Œì§„ë˜ë©´ ì¢…ë£Œ
                if attempts >= len(categories):
                    break

            # í˜¹ì‹œ ë‚¨ì€ ì•„ì´í…œë“¤ ì¶”ê°€
            for category, items in category_groups.items():
                start_idx = category_indices[category]
                result.extend(items[start_idx:])

            logger.info(f"âœ… Category shuffling completed: {len(result)} items redistributed")
            return result[:len(recommendations)]

        except Exception as e:
            logger.error(f"âŒ Category shuffling failed: {e}")
            return recommendations

    async def _get_place_candidates(
        self,
        region: Optional[str],
        category: Optional[str]
    ) -> List[Dict]:
        """ì¶”ì²œ í›„ë³´ ì¥ì†Œ ì¡°íšŒ (ìµœì í™”ëœ ì¿¼ë¦¬)"""
        try:
            # place_recommendations í…Œì´ë¸”ì—ì„œ í…ìŠ¤íŠ¸ ë²¡í„° ì¡°íšŒ (PostgreSQL vector íƒ€ì…)
            query = """
                SELECT
                    pr.place_id::text as place_id,
                    pr.table_name,
                    pr.vector as vector,
                    COALESCE(pr.bookmark_cnt, 0) as total_likes,
                    COALESCE(pr.bookmark_cnt, 0) as total_bookmarks,
                    COALESCE(pr.bookmark_cnt, 0) as total_clicks,
                    1 as unique_users,
                    COALESCE(pr.bookmark_cnt, 0)::float as popularity_score,
                    COALESCE(pr.bookmark_cnt, 0)::float as engagement_score,
                    pr.name,
                    pr.region,
                    pr.city,
                    pr.latitude,
                    pr.longitude,
                    pr.overview as description,
                    pr.image_urls,
                    pr.bookmark_cnt
                FROM place_recommendations pr
                WHERE
                    pr.vector IS NOT NULL
                    AND pr.name IS NOT NULL
                    AND pr.bookmark_cnt IS NOT NULL
            """

            params = []
            param_count = 0

            # ì§€ì—­ í•„í„°
            if region:
                param_count += 1
                query += f" AND pr.region = ${param_count}"
                params.append(region)

            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if category:
                param_count += 1
                query += f" AND pr.table_name = ${param_count}::text"
                params.append(category)

            # ì„±ëŠ¥ì„ ìœ„í•œ ì œí•œ ë° ì •ë ¬ (ë¶ë§ˆí¬ ì¹´ìš´íŠ¸ ê¸°ì¤€)
            query += " ORDER BY COALESCE(pr.bookmark_cnt, 0) DESC"
            param_count += 1
            query += f" LIMIT ${param_count}"
            params.append(CONFIG.candidate_limit)

            places = await self.db_manager.execute_query(query, *params)

            # ë²¡í„° ë°ì´í„° ê²€ì¦
            valid_places = []
            for place in places:
                if validate_vector_data(place['vector']) is not None:
                    # S3 ì´ë¯¸ì§€ URLì„ HTTPSë¡œ ë³€í™˜
                    place = self._convert_s3_urls_to_https(place)
                    valid_places.append(place)

            logger.info(f"ğŸ“‹ Retrieved {len(valid_places)} valid place candidates")
            return valid_places

        except Exception as e:
            logger.error(f"âŒ Failed to get place candidates: {e}")
            return []

    async def _get_popular_recommendations(
        self,
        region: Optional[str],
        category: Optional[str],
        limit: int,
        fast_mode: bool = False
    ) -> List[Dict]:
        """ì¸ê¸° ê¸°ë°˜ ì¶”ì²œ (ë‹¨ìˆœ ë¶ë§ˆí¬ ì¹´ìš´íŠ¸ ì •ë ¬)"""
        # fast_modeì— ë”°ë¼ ë‹¤ë¥¸ í›„ë³´ ì¡°íšŒ
        if fast_mode:
            places = await self._get_fast_place_candidates(region, category, limit * 2)
        else:
            places = await self._get_place_candidates(region, category)

        if not places:
            return []

        # ë‹¨ìˆœ ë¶ë§ˆí¬ ì¹´ìš´íŠ¸ ê¸°ë°˜ ì •ë ¬
        for place in places:
            try:
                bookmark_count = place.get('bookmark_cnt', 0)
                place['final_score'] = bookmark_count
                place['recommendation_type'] = 'popular_fast' if fast_mode else 'popular'
                place['source'] = 'popular'  # ì†ŒìŠ¤ íƒœê·¸ ì¶”ê°€
                place['similarity_score'] = 0.8  # ì¸ê¸° ì¶”ì²œìš© ê¸°ë³¸ê°’
            except Exception as e:
                logger.error(f"âŒ Popular score calculation failed for place {place.get('place_id')}: {e}")
                place['final_score'] = 0

        # ë¶ë§ˆí¬ ì¹´ìš´íŠ¸ìˆœ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        places.sort(key=lambda x: x.get('bookmark_cnt', 0), reverse=True)
        
        # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
        for place in places[:limit]:
            if 'vector' in place and isinstance(place['vector'], np.ndarray):
                place['vector'] = place['vector'].tolist()
            if 'text_vector' in place and isinstance(place['text_vector'], np.ndarray):
                place['text_vector'] = place['text_vector'].tolist()
            if 'image_vector' in place and isinstance(place['image_vector'], np.ndarray):
                place['image_vector'] = place['image_vector'].tolist()
        
        return places[:limit]

    async def _get_multi_vector_recommendations(
        self,
        user_id: str,
        user_vector: np.ndarray,
        bookmark_preferences: Dict[str, float],
        region: Optional[str],
        category: Optional[str],
        limit: int
    ) -> List[Dict]:
        """
        ë‹¤ì¤‘ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ (ë™ì  ê°€ì¤‘ì¹˜ ì ìš©)
        - ê¸°ì¡´ ì‚¬ìš©ì: ìš°ì„ ìˆœìœ„ ì„ í˜¸ë„ 70% + í–‰ë™ ë°ì´í„° 30%
        - í…ìŠ¤íŠ¸-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (ê¸°ì¡´ overview ë²¡í„°)
        - ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìœ ì‚¬ë„ (ìƒˆë¡œìš´ image ë²¡í„°)
        - í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœ ì‚¬ë„ (í…ìŠ¤íŠ¸-ì´ë¯¸ì§€, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸)
        - ë¶ë§ˆí¬ ê¸°ë°˜ ì„ í˜¸ë„ ë°˜ì˜
        """
        try:
            # ì¥ì†Œ í›„ë³´êµ° ì¡°íšŒ (ì´ë¯¸ì§€ ë²¡í„° í¬í•¨)
            places = await self._get_place_candidates_with_images(region, category)

            if not places:
                return []

            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”: ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë™ì‹œì— ì¡°íšŒ
            user_data_tasks = [
                self._get_user_preferences(user_id),
                self._get_user_image_preferences(user_id)
            ]

            # ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰
            user_preferences, user_image_preferences = await asyncio.gather(
                *user_data_tasks, return_exceptions=True
            )

            # ì˜ˆì™¸ ì²˜ë¦¬
            if isinstance(user_preferences, Exception):
                user_preferences = {}
                logger.error(f"Failed to get user preferences: {user_preferences}")

            if isinstance(user_image_preferences, Exception):
                user_image_preferences = {}
                logger.error(f"Failed to get user image preferences: {user_image_preferences}")

            # ì´ë¯¸ì§€ ì„ í˜¸ë„ê°€ ì¤€ë¹„ë˜ë©´ ë‹¤ì¤‘ ë²¡í„° ìœ ì‚¬ë„ ì¬ê³„ì‚°
            multi_scores = await self._calculate_independent_similarities(
                user_id, user_vector, user_image_preferences, places
            )

            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¸ê¸°ë„ ì ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°
            popularity_scores = {}
            engagement_scores = {}
            for place in places:
                place_data = {
                    'total_clicks': place.get('total_clicks', 0),
                    'total_likes': place.get('total_likes', 0),
                    'total_bookmarks': place.get('total_bookmarks', 0)
                }
                place_id = place.get('place_id')
                popularity_scores[place_id] = calculate_weighted_popularity_score(place_data)
                engagement_scores[place_id] = calculate_engagement_score(place_data)

            # ë°°ì¹˜ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚°
            preference_scores = {}
            for place in places:
                place_id = place.get('place_id')
                preference_scores[place_id] = self._calculate_place_preference_score(place, user_preferences)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ìƒì„±
            results = []
            for i, place in enumerate(places):
                place_id = place.get('place_id')

                if i >= len(multi_scores):
                    logger.warning(f"Missing score for place {i}, using defaults")
                    scores = {
                        'behavior_text_similarity': 0.0,
                        'upload_image_similarity': 0.0,
                        'bookmark_text_similarity': 0.0,
                        'bookmark_image_similarity': 0.0,
                        'liked_post_similarity': 0.0,
                        'combined_score': 0.0
                    }
                else:
                    scores = multi_scores[i]

                try:
                    # ë¯¸ë¦¬ ê³„ì‚°ëœ ì ìˆ˜ë“¤ ì‚¬ìš©
                    popularity_score = popularity_scores.get(place_id, 0.0)
                    engagement_score = engagement_scores.get(place_id, 0.0)
                    preference_score = preference_scores.get(place_id, 0.0)

                    # ë¶ë§ˆí¬ ì„ í˜¸ë„ ë³´ë„ˆìŠ¤
                    category_preference = bookmark_preferences.get(place['table_name'], 0)
                    bookmark_bonus = category_preference * 0.5

                    # ë‹¤ì¤‘ ë²¡í„° ì¢…í•© ì ìˆ˜ (í–‰ë™ ë°ì´í„° ê¸°ë°˜)
                    behavior_score = scores.get('combined_score', 0.0)

                    # ë™ì  ê°€ì¤‘ì¹˜ ì ìš© (ê¸°ì¡´ ì‚¬ìš©ì: ì„ í˜¸ë„ 70% + í–‰ë™ ë°ì´í„° 30%)
                    weighted_preference_score = preference_score * CONFIG.experienced_user_preference_weight
                    weighted_behavior_score = behavior_score * CONFIG.experienced_user_behavior_weight

                    # ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ë™ì  ê°€ì¤‘ì¹˜ ë°˜ì˜)
                    final_score = (
                        weighted_preference_score +
                        weighted_behavior_score +
                        (popularity_score / 100.0) * 0.1 +  # ì¸ê¸°ë„ ì•½ê°„ ë°˜ì˜
                        bookmark_bonus * 0.1  # ë¶ë§ˆí¬ ë³´ë„ˆìŠ¤ ì•½ê°„ ë°˜ì˜
                    )

                    # ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
                    if behavior_score >= CONFIG.min_similarity_threshold:
                        place['behavior_text_similarity'] = round(scores.get('behavior_text_similarity', 0.0), 4)
                        place['upload_image_similarity'] = round(scores.get('upload_image_similarity', 0.0), 4)
                        place['bookmark_text_similarity'] = round(scores.get('bookmark_text_similarity', 0.0), 4)
                        place['bookmark_image_similarity'] = round(scores.get('bookmark_image_similarity', 0.0), 4)
                        place['liked_post_similarity'] = round(scores.get('liked_post_similarity', 0.0), 4)
                        place['combined_score'] = round(scores.get('combined_score', 0.0), 4)
                        place['multi_vector_score'] = round(behavior_score, 4)
                        place['preference_score'] = round(preference_score, 4)
                        place['weighted_preference_score'] = round(weighted_preference_score, 4)
                        place['weighted_behavior_score'] = round(weighted_behavior_score, 4)
                        place['popularity_score'] = popularity_score
                        place['engagement_score'] = engagement_score
                        place['bookmark_bonus'] = round(bookmark_bonus, 4)
                        place['final_score'] = round(final_score, 4)
                        place['recommendation_type'] = 'five_channel_system'

                        results.append(place)

                except Exception as e:
                    logger.error(f"âŒ Multi-vector score calculation failed for place {i}: {e}")
                    continue

            # ì ìˆ˜ìˆœ ì •ë ¬
            results.sort(key=lambda x: x['final_score'], reverse=True)

            logger.info(f"ğŸ¯ Multi-vector recommendations: {len(results)} candidates")

            # ì¹´í…Œê³ ë¦¬ ê· í˜• ì¡°ì • ì ìš©
            balanced_results = self._apply_category_quotas(results, bookmark_preferences, limit)
            final_results = self._apply_category_shuffling(balanced_results)
            
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            for result in final_results:
                if 'vector' in result and isinstance(result['vector'], np.ndarray):
                    result['vector'] = result['vector'].tolist()
                if 'text_vector' in result and isinstance(result['text_vector'], np.ndarray):
                    result['text_vector'] = result['text_vector'].tolist()
                if 'image_vector' in result and isinstance(result['image_vector'], np.ndarray):
                    result['image_vector'] = result['image_vector'].tolist()

            return final_results

        except Exception as e:
            logger.error(f"âŒ Multi-vector recommendation failed: {e}")
            # Fallback to enhanced vector recommendations
            return await self._get_enhanced_vector_recommendations(
                user_vector, bookmark_preferences, region, category, limit
            )

    async def _get_enhanced_vector_recommendations(
        self,
        user_vector: np.ndarray,
        bookmark_preferences: Dict[str, float],
        region: Optional[str],
        category: Optional[str],
        limit: int
    ) -> List[Dict]:
        """ë¶ë§ˆí¬ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•œ ê°œì„ ëœ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        places = await self._get_place_candidates(region, category)

        if not places:
            return []

        try:
            # ë²¡í„° ë°°ì¹˜ ì²˜ë¦¬
            place_vectors = []
            valid_places = []

            for place in places:
                vector = validate_vector_data(place['vector'])
                if vector is not None:
                    place_vectors.append(vector)
                    valid_places.append(place)

            if not valid_places:
                logger.warning("âš ï¸ No valid place vectors found, falling back to popular")
                return await self._get_popular_recommendations(region, category, limit)

            # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
            place_vectors_array = np.array(place_vectors, dtype=np.float32)
            similarities = safe_cosine_similarity(
                user_vector,
                place_vectors_array,
                use_ann=self.ann_enabled,
                faiss_manager=self.faiss_manager
            )

            # í†µê³„ ì—…ë°ì´íŠ¸
            if self.ann_enabled:
                self.stats['ann_searches'] += 1
            else:
                self.stats['cosine_searches'] += 1

            # ë¶ë§ˆí¬ ì„ í˜¸ë„ ê°•í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            results = []
            for i, place in enumerate(valid_places):
                try:
                    similarity = float(similarities[i])

                    # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
                    if similarity < CONFIG.min_similarity_threshold:
                        continue

                    # ê¸°ë³¸ ì¸ê¸°ë„ ì ìˆ˜
                    place_data = {
                        'total_clicks': place.get('total_clicks', 0),
                        'total_likes': place.get('total_likes', 0),
                        'total_bookmarks': place.get('total_bookmarks', 0)
                    }
                    popularity_score = calculate_weighted_popularity_score(place_data)
                    engagement_score = calculate_engagement_score(place_data)

                    # ğŸ”¥ ë¶ë§ˆí¬ ì„ í˜¸ë„ ë³´ë„ˆìŠ¤ ì¶”ê°€ (ì ì ˆí•œ ê°•í™”)
                    category_preference = bookmark_preferences.get(place['table_name'], 0)
                    bookmark_bonus = category_preference * 0.5  # ìµœëŒ€ 50% ë³´ë„ˆìŠ¤ (ê· í˜•ì¡íŒ ì¡°ì •)

                    # ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
                    hybrid_score = (
                        similarity * CONFIG.similarity_weight +
                        (popularity_score / 100.0) * CONFIG.popularity_weight * 0.7 +
                        (engagement_score / 100.0) * CONFIG.popularity_weight * 0.3 +
                        bookmark_bonus  # ë¶ë§ˆí¬ ì„ í˜¸ë„ ë³´ë„ˆìŠ¤
                    )

                    place['similarity_score'] = round(similarity, 4)
                    place['popularity_score'] = popularity_score
                    place['engagement_score'] = engagement_score
                    place['bookmark_bonus'] = round(bookmark_bonus, 4)
                    place['final_score'] = round(hybrid_score, 4)
                    place['recommendation_type'] = 'personalized_enhanced'

                    results.append(place)

                except Exception as e:
                    logger.error(f"âŒ Score calculation failed for place {i}: {e}")
                    continue

            # ì ìˆ˜ìˆœ ì •ë ¬
            results.sort(key=lambda x: x['final_score'], reverse=True)

            # ğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ë³´ì¥ ì¶”ì²œ ì‹œìŠ¤í…œ ì ìš©
            balanced_results = self._apply_category_quotas(results, bookmark_preferences, limit)

            # ğŸ”„ ì¹´í…Œê³ ë¦¬ ë¶„ì‚°ì„ ìœ„í•œ ì…”í”Œë§ ì ìš©
            final_results = self._apply_category_shuffling(balanced_results)

            return final_results

        except Exception as e:
            logger.error(f"âŒ Enhanced vector-based recommendation failed: {e}")
            # Fallback to popular recommendations
            return await self._get_popular_recommendations(region, category, limit)

    async def _get_vector_based_recommendations(
        self,
        user_vector: np.ndarray,
        region: Optional[str],
        category: Optional[str],
        limit: int
    ) -> List[Dict]:
        """ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ (ìµœì í™”ëœ ë²„ì „)"""
        places = await self._get_place_candidates(region, category)

        if not places:
            return []

        try:
            # ë²¡í„° ë°°ì¹˜ ì²˜ë¦¬
            place_vectors = []
            valid_places = []

            for place in places:
                vector = validate_vector_data(place['vector'])
                if vector is not None:
                    place_vectors.append(vector)
                    valid_places.append(place)

            if not valid_places:
                logger.warning("âš ï¸ No valid place vectors found, falling back to popular")
                return await self._get_popular_recommendations(region, category, limit)

            # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
            place_vectors_array = np.array(place_vectors, dtype=np.float32)
            similarities = safe_cosine_similarity(
                user_vector,
                place_vectors_array,
                use_ann=self.ann_enabled,
                faiss_manager=self.faiss_manager
            )

            # í†µê³„ ì—…ë°ì´íŠ¸
            if self.ann_enabled:
                self.stats['ann_searches'] += 1
            else:
                self.stats['cosine_searches'] += 1

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            results = []
            for i, place in enumerate(valid_places):
                try:
                    similarity = float(similarities[i])

                    # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
                    if similarity < CONFIG.min_similarity_threshold:
                        continue

                    # ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°
                    place_data = {
                        'total_clicks': place.get('total_clicks', 0),
                        'total_likes': place.get('total_likes', 0),
                        'total_bookmarks': place.get('total_bookmarks', 0)
                    }
                    popularity_score = calculate_weighted_popularity_score(place_data)
                    engagement_score = calculate_engagement_score(place_data)

                    # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ê°œì¸í™” + ì¸ê¸°ë„ + ì°¸ì—¬ë„)
                    hybrid_score = (
                        similarity * CONFIG.similarity_weight +
                        (popularity_score / 100.0) * CONFIG.popularity_weight * 0.7 +
                        (engagement_score / 100.0) * CONFIG.popularity_weight * 0.3
                    )

                    place['similarity_score'] = round(similarity, 4)
                    place['popularity_score'] = popularity_score
                    place['engagement_score'] = engagement_score
                    place['final_score'] = round(hybrid_score, 4)
                    place['recommendation_type'] = 'personalized'

                    results.append(place)

                except Exception as e:
                    logger.error(f"âŒ Score calculation failed for place {i}: {e}")
                    continue

            # ì ìˆ˜ìˆœ ì •ë ¬
            results.sort(key=lambda x: x['final_score'], reverse=True)

            logger.info(f"ğŸ¯ Generated {len(results)} personalized recommendations")
            
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            for result in results[:limit]:
                if 'vector' in result and isinstance(result['vector'], np.ndarray):
                    result['vector'] = result['vector'].tolist()
                if 'text_vector' in result and isinstance(result['text_vector'], np.ndarray):
                    result['text_vector'] = result['text_vector'].tolist()
                if 'image_vector' in result and isinstance(result['image_vector'], np.ndarray):
                    result['image_vector'] = result['image_vector'].tolist()
            
            return results[:limit]

        except Exception as e:
            logger.error(f"âŒ Vector-based recommendation failed: {e}")
            # Fallback to popular recommendations
            return await self._get_popular_recommendations(region, category, limit)

    def _update_response_time(self, response_time: float):
        """ì‘ë‹µ ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸ (ì´ë™í‰ê· )"""
        alpha = 0.1  # ì´ë™í‰ê·  ê°€ì¤‘ì¹˜
        if self.stats['avg_response_time'] == 0.0:
            self.stats['avg_response_time'] = response_time
        else:
            self.stats['avg_response_time'] = (
                alpha * response_time +
                (1 - alpha) * self.stats['avg_response_time']
            )

    def get_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        cache_hit_rate = (
            self.stats['cache_hits'] / max(self.stats['total_requests'], 1)
        ) * 100

        return {
            **self.stats,
            'cache_hit_rate': round(cache_hit_rate, 2),
            'cache_size': len(self.vector_cache),
            'personalization_rate': round(
                (self.stats['personalized_requests'] / max(self.stats['total_requests'], 1)) * 100, 2
            )
        }

    async def get_popular_regions_and_categories(self) -> Dict[str, List[str]]:
        """ì¸ê¸°ë„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì§€ì—­/ì¹´í…Œê³ ë¦¬ ìˆœì„œ ê²°ì •"""
        try:
            # ì§€ì—­ë³„ ë¶ë§ˆí¬ ì´í•© ì¡°íšŒ
            region_query = """
                SELECT region, SUM(COALESCE(bookmark_cnt, 0)) as total_bookmarks
                FROM place_recommendations
                WHERE region IS NOT NULL
                GROUP BY region
                ORDER BY total_bookmarks DESC
                LIMIT 10
            """

            # ì¹´í…Œê³ ë¦¬ë³„ ë¶ë§ˆí¬ ì´í•© ì¡°íšŒ
            category_query = """
                SELECT table_name as category, SUM(COALESCE(bookmark_cnt, 0)) as total_bookmarks
                FROM place_recommendations
                WHERE table_name IS NOT NULL
                GROUP BY table_name
                ORDER BY total_bookmarks DESC
                LIMIT 10
            """

            regions_data = await self.db_manager.execute_query(region_query)
            categories_data = await self.db_manager.execute_query(category_query)

            regions = [row['region'] for row in regions_data if row['region']]
            categories = [row['category'] for row in categories_data if row['category']]

            logger.info(f"ğŸ“Š Dynamic ordering: {len(regions)} regions, {len(categories)} categories")

            return {
                'regions': regions,
                'categories': categories
            }

        except Exception as e:
            logger.error(f"âŒ Failed to get dynamic regions/categories: {e}")
            # Fallback to config defaults
            return {
                'regions': CONFIG.explore_regions or [],
                'categories': CONFIG.explore_categories or []
            }

    async def health_check(self) -> Dict[str, Any]:
        """ì—”ì§„ í—¬ìŠ¤ì²´í¬"""
        try:
            # ê°„ë‹¨í•œ DB ì—°ê²° í…ŒìŠ¤íŠ¸
            test_result = await self.db_manager.execute_single_query("SELECT 1")
            db_healthy = test_result == 1

            # ìºì‹œ ìƒíƒœ í™•ì¸
            cache_healthy = len(self.vector_cache) < CONFIG.vector_cache_size

            # ì „ì²´ ìƒíƒœ íŒë‹¨
            overall_healthy = db_healthy and cache_healthy

            return {
                'status': 'healthy' if overall_healthy else 'degraded',
                'database': 'connected' if db_healthy else 'disconnected',
                'cache': 'normal' if cache_healthy else 'full',
                'stats': self.get_stats(),
                'config': {
                    'candidate_limit': CONFIG.candidate_limit,
                    'similarity_weight': CONFIG.similarity_weight,
                    'popularity_weight': CONFIG.popularity_weight
                }
            }

        except Exception as e:
            logger.error(f"âŒ Health check failed: {e}")
            return {
                'status': 'unhealthy',
                'error': str(e),
                'stats': self.get_stats()
            }

    async def _get_preference_based_recommendations(
        self,
        user_id: str,
        region: Optional[str] = None,
        category: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """
        ìš°ì„ ìˆœìœ„ íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ (behavior_vector í†µí•©)
        user_preferences, user_preference_tags í…Œì´ë¸”ê³¼ user_behavior_vectorsì˜ behavior_vectorë¥¼ í™œìš©
        """
        try:
            # 1. ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ ì¡°íšŒ
            logger.info(f"ğŸ” Getting user preferences for user {user_id}")
            user_preferences = await self._get_user_preferences(user_id)
            if not user_preferences:
                logger.info(f"âŒ No preferences found for user {user_id}")
                return []

            logger.info(f"âœ… Found user preferences for user {user_id}: {list(user_preferences.keys())}")

            # 2. ì‚¬ìš©ì í–‰ë™ ë²¡í„° ì¡°íšŒ (ë¶ë§ˆí¬ íŒ¨í„´ ë¶„ì„ìš©)
            logger.info(f"ğŸ§  Getting user behavior vector for user {user_id}")
            user_behavior_vector = await self._get_user_behavior_vector_cached(user_id)
            if user_behavior_vector is not None:
                logger.info(f"âœ… Found behavior vector for user {user_id}: shape {user_behavior_vector.shape}")
            else:
                logger.info(f"âŒ No behavior vector found for user {user_id}")

            # 3. ìš°ì„ ìˆœìœ„ íƒœê·¸ ë‚´ì—ì„œ behavior_vector í†µí•©ëœ ì ìˆ˜ ê³„ì‚°
            recommendations = await self._calculate_priority_enhanced_scores(
                user_preferences, user_behavior_vector, region, category, limit
            )

            logger.info(f"âœ… Generated {len(recommendations)} priority-enhanced recommendations for user {user_id}")
            return recommendations

        except Exception as e:
            logger.error(f"âŒ Priority-enhanced recommendation failed for user {user_id}: {e}")
            return []

    async def _get_regional_diverse_recommendations(
        self,
        user_id: str,
        region: str,
        category: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ ë‚´ì—ì„œ ìš°ì„ ìˆœìœ„ + ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í†µí•© ì¶”ì²œ
        - ìš°ì„ ìˆœìœ„ íƒœê·¸: 50%
        - ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬: 30%
        - ìœ ì‚¬í•œ ì¥ì†Œ: 20%
        """
        try:
            # 1. ì‚¬ìš©ì ì„ í˜¸ë„ ë° í–‰ë™ ë²¡í„° ì¡°íšŒ
            logger.info(f"ğŸ” [Regional Diverse] Getting user data for {user_id} in {region}")
            user_preferences = await self._get_user_preferences(user_id)
            if not user_preferences:
                logger.info(f"âŒ [Regional Diverse] No preferences found for user {user_id}")
                return []

            user_behavior_vector = await self._get_user_behavior_vector_cached(user_id)
            logger.info(f"ğŸ§  [Regional Diverse] Behavior vector: {'Found' if user_behavior_vector is not None else 'Not found'}")

            # 2. íŠ¹ì • ì§€ì—­ ì¶”ì²œ: ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì¡°í•© (ì‚¬ìš©ìë³„ ì¶”ì²œê³¼ êµ¬ë³„)
            recommendations = []

            if user_behavior_vector is not None:
                logger.info(f"ğŸ§  [Regional Diverse] Mixed recommendations for region {region} (behavior + priority)")

                # í–‰ë™ ë²¡í„° ê¸°ë°˜ + ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¡°í•© (50:50)
                behavior_limit = max(1, int(limit * 0.5))
                priority_limit = limit - behavior_limit

                # í–‰ë™ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ (50%)
                behavior_recommendations = await self._get_similar_places_in_region(
                    user_behavior_vector, region, behavior_limit
                )

                # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¶”ì²œ (50%)
                priority_recommendations = await self._calculate_preference_scores(
                    user_preferences, region, category, priority_limit
                )

                # ë‘ ì¶”ì²œ ê²°ê³¼ ë³‘í•©
                recommendations.extend(behavior_recommendations)
                recommendations.extend(priority_recommendations)

                # ì¤‘ë³µ ì œê±°
                seen_places = set()
                unique_recommendations = []
                for rec in recommendations:
                    place_id = rec.get('place_id')
                    if place_id not in seen_places:
                        seen_places.add(place_id)
                        unique_recommendations.append(rec)

                recommendations = unique_recommendations[:limit]
                logger.info(f"âœ… [Regional Diverse] Mixed completed: {len(recommendations)} items (behavior+priority)")
                return recommendations
            else:
                logger.info(f"âŒ [Regional Diverse] No behavior vector, using priority-based only")
                # í–‰ë™ ë²¡í„°ê°€ ì—†ìœ¼ë©´ ì„ í˜¸ë„ ê¸°ë°˜ìœ¼ë¡œë§Œ
                recommendations = await self._calculate_preference_scores(
                    user_preferences, region, category, limit
                )
                return recommendations

            # 3. ì¹´í…Œê³ ë¦¬ í•„í„°ê°€ ì—†ëŠ” ê²½ìš° ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì¡°í•©
            recommendations = []

            # ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ (80% - ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°)
            priority_limit = max(10, int(limit * 0.8))  # ìµœì†Œ 10ê°œ, 80%ë¡œ ì¦ê°€
            logger.info(f"ğŸ¯ [Regional Diverse] Getting {priority_limit} priority recommendations")
            priority_recommendations = await self._calculate_priority_enhanced_scores(
                user_preferences, user_behavior_vector, region, None, priority_limit
            )

            # ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ (15%)
            diverse_limit = max(3, int(limit * 0.15))  # ìµœì†Œ 3ê°œ, 15%ë¡œ ê°ì†Œ
            logger.info(f"ğŸŒˆ [Regional Diverse] Getting {diverse_limit} diverse category recommendations")
            user_priority = user_preferences.get('priority')
            diverse_recommendations = await self._get_diverse_category_recommendations(
                user_behavior_vector, region, diverse_limit, exclude_priority=user_priority
            )

            # ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œ (5%)
            similar_limit = max(1, limit - priority_limit - diverse_limit)
            logger.info(f"ğŸ” [Regional Diverse] Getting {similar_limit} similar place recommendations")
            similar_recommendations = []
            if user_behavior_vector is not None:
                similar_recommendations = await self._get_similar_places_in_region(
                    user_behavior_vector, region, similar_limit
                )

            # 4. ì¶”ì²œ ë³‘í•© (ì¤‘ë³µ ì œê±°)
            final_recommendations = self._merge_diverse_recommendations(
                priority_recommendations, diverse_recommendations, similar_recommendations, limit
            )

            # 5. ì†ŒìŠ¤ íƒœê·¸ ì¶”ê°€
            for rec in final_recommendations:
                if 'source' not in rec:
                    rec['source'] = 'diverse_regional'

            logger.info(f"ğŸ‰ [Regional Diverse] Generated {len(final_recommendations)} diverse recommendations "
                       f"(priority: {len(priority_recommendations)}, diverse: {len(diverse_recommendations)}, "
                       f"similar: {len(similar_recommendations)})")

            return final_recommendations

        except Exception as e:
            logger.error(f"âŒ Regional diverse recommendation failed for user {user_id}: {e}")
            return []

    async def _get_user_preferences(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ ì¡°íšŒ (user_preferences í…Œì´ë¸”ì—ì„œ ì¡°íšŒ)"""
        try:
            # user_preferences í…Œì´ë¸”ì—ì„œ ì„ í˜¸ë„ ì •ë³´ ì¡°íšŒ (ì˜¬ë°”ë¥¸ ìŠ¤í‚¤ë§ˆ)
            preferences_query = """
                SELECT
                    priority,
                    accommodation,
                    exploration,
                    persona
                FROM user_preferences
                WHERE user_id = $1
            """

            # user_preference_tags í…Œì´ë¸”ì—ì„œ íƒœê·¸ ì •ë³´ ì¡°íšŒ (í˜„ì¬ ìŠ¤í‚¤ë§ˆ)
            tags_query = """
                SELECT
                    tag,
                    weight
                FROM user_preference_tags
                WHERE user_id = $1
                ORDER BY weight DESC
            """

            # ë³‘ë ¬ë¡œ ë‘ ì¿¼ë¦¬ ì‹¤í–‰
            async with self.db_manager.get_connection() as conn:
                preferences_data = await conn.fetchrow(preferences_query, user_id)
                tags_data = await conn.fetch(tags_query, user_id)

            if not preferences_data and not tags_data:
                return {}

            # ê²°ê³¼ êµ¬ì„± (í˜„ì¬ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
            result = {
                'user_id': user_id,  # user_id ì¶”ê°€
                'priority': preferences_data.get('priority') if preferences_data else None,
                'accommodation': preferences_data.get('accommodation') if preferences_data else None,
                'exploration': preferences_data.get('exploration') if preferences_data else None,
                'persona': preferences_data.get('persona') if preferences_data else None,
                'preference_tags': {
                    row['tag']: row['weight']
                    for row in tags_data
                }
            }

            return result

        except Exception as e:
            logger.error(f"âŒ Failed to get user preferences for {user_id}: {e}")
            return {}

    async def _calculate_priority_enhanced_scores(
        self,
        user_preferences: Dict[str, Any],
        user_behavior_vector: Optional[np.ndarray],
        region: Optional[str] = None,
        category: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """
        ğŸ¯ ìš°ì„ ìˆœìœ„ íƒœê·¸ ë‚´ì—ì„œ behavior_vectorë¥¼ í™œìš©í•œ í–¥ìƒëœ ì ìˆ˜ ê³„ì‚°
        1. ìš°ì„ ìˆœìœ„ íƒœê·¸ë¡œ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        2. í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ behavior_vectorë¡œ ê°œì¸í™”
        3. ì„ í˜¸ë„ íƒœê·¸ì™€ í–‰ë™ íŒ¨í„´ì„ ì¢…í•©í•œ ì ìˆ˜ ì‚°ì¶œ
        """
        try:
            priority = user_preferences.get('priority')

            # ì²´í—˜ ìš°ì„ ìˆœìœ„ëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
            if priority == 'experience':
                logger.info(f"ğŸ¯ Experience priority: collecting from nature, humanities, leisure_sports")

                # ì²´í—˜ ìš°ì„ ìˆœìœ„ëŠ” 3ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ëª¨ë‘ ìˆ˜ì§‘
                experience_categories = ['nature', 'humanities', 'leisure_sports']
                all_places_data = []

                async with self.db_manager.get_connection() as conn:
                    for exp_category in experience_categories:
                        places_query = """
                            SELECT
                                place_id, table_name, region, name,
                                latitude, longitude, overview, image_urls, bookmark_cnt,
                                vector as text_vector,
                                COALESCE(bookmark_cnt, 0) as popularity_score
                            FROM place_recommendations
                            WHERE name IS NOT NULL
                                AND ($1::text IS NULL OR region = $1)
                                AND table_name = $2
                            ORDER BY bookmark_cnt DESC
                            LIMIT $3
                        """

                        category_places = await conn.fetch(
                            places_query,
                            region,
                            exp_category,
                            CONFIG.candidate_limit // 3  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 1/3ì”©
                        )
                        all_places_data.extend(category_places)
                        logger.info(f"ğŸ¯ Found {len(category_places)} places for experience->{exp_category}")

                places_data = all_places_data
            else:
                # ë‹¤ë¥¸ ìš°ì„ ìˆœìœ„ëŠ” ê¸°ì¡´ ë°©ì‹
                target_category = category if category else priority
                logger.info(f"ğŸ¯ Priority filtering: {priority} â†’ target_category: {target_category}")

                places_query = """
                    SELECT
                        place_id, table_name, region, name,
                        latitude, longitude, overview, image_urls, bookmark_cnt,
                        vector as text_vector,
                        COALESCE(bookmark_cnt, 0) as popularity_score
                    FROM place_recommendations
                    WHERE name IS NOT NULL
                        AND ($1::text IS NULL OR region = $1)
                        AND ($2::text IS NULL OR table_name = $2)
                    ORDER BY bookmark_cnt DESC
                    LIMIT $3
                """

                async with self.db_manager.get_connection() as conn:
                    places_data = await conn.fetch(
                        places_query,
                        region,
                        target_category,
                        CONFIG.candidate_limit
                    )

            if not places_data:
                if priority == 'experience':
                    logger.warning(f"No places found for experience priority in region: {region}")
                else:
                    logger.warning(f"No places found for priority: {priority}, region: {region}, category: {target_category}")
                return []

            # ìš°ì„ ìˆœìœ„ íƒœê·¸ ë‚´ì—ì„œ behavior_vector ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            scored_places = []
            popularity_normalizer = CONFIG.preference_weights['popularity_normalizer']

            for place in places_data:
                # 1. ê¸°ë³¸ ì„ í˜¸ë„ ì ìˆ˜ (ìš°ì„ ìˆœìœ„ íƒœê·¸ ë§¤ì¹­)
                preference_score = self._calculate_place_preference_score(place, user_preferences)

                # 2. behavior_vector ê¸°ë°˜ ê°œì¸í™” ì ìˆ˜ (ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ)
                behavior_score = 0.0
                if user_behavior_vector is not None and place.get('text_vector'):
                    place_text_vector = validate_vector_data(place['text_vector'])
                    if place_text_vector is not None:
                        similarity = safe_cosine_similarity(user_behavior_vector, place_text_vector, use_ann=False)
                        behavior_score = float(similarity[0]) if len(similarity) > 0 else 0.0
                        logger.info(f"ğŸ§  {place['name']}: behavior_score={behavior_score:.4f}")
                    else:
                        logger.info(f"âš ï¸ {place['name']}: invalid text_vector")
                else:
                    if user_behavior_vector is None:
                        logger.info(f"âŒ {place['name']}: no behavior_vector for user")
                    else:
                        logger.info(f"âŒ {place['name']}: no text_vector for place")

                # 3. ìš°ì„ ìˆœìœ„ íƒœê·¸ ë‚´ ì¢…í•© ì ìˆ˜ ê³„ì‚°
                if preference_score > 0 or behavior_score > 0:
                    place_dict = dict(place)
                    popularity_normalized = min(place['popularity_score'] / popularity_normalizer, 1.0)

                    # ğŸ¯ í–‰ë™ ë°ì´í„° ìœ ë¬´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì°¨ë³„í™”
                    if user_behavior_vector is not None:
                        # í–‰ë™ ë°ì´í„° ìˆëŠ” ì‚¬ìš©ì: ê°œì¸í™” ì¤‘ì‹¬ (ì¸ê¸°ë„ ì œì™¸)
                        priority_weight = 0.7   # ìš°ì„ ìˆœìœ„ íƒœê·¸ ê¸°ë°˜ ì„ í˜¸ë„
                        behavior_weight = 0.3   # ë¶ë§ˆí¬ í–‰ë™ íŒ¨í„´ (ê°•í™”)
                        popularity_weight = 0.0 # ì¸ê¸°ë„ ì œì™¸
                    else:
                        # í–‰ë™ ë°ì´í„° ì—†ëŠ” ì‚¬ìš©ì: ì¸ê¸°ë„ ì°¸ê³ 
                        priority_weight = 0.8   # ìš°ì„ ìˆœìœ„ íƒœê·¸ ê¸°ë°˜ ì„ í˜¸ë„ (ê°•í™”)
                        behavior_weight = 0.0   # í–‰ë™ íŒ¨í„´ ì—†ìŒ
                        popularity_weight = 0.2 # ì¸ê¸°ë„ ì°¸ê³ 

                    final_score = (
                        preference_score * priority_weight +
                        behavior_score * behavior_weight +
                        popularity_normalized * popularity_weight
                    )

                    # ìƒì„¸ ì ìˆ˜ ë¡œê¹… (behavior_score ë†’ì€ ì¥ì†Œë§Œ)
                    if behavior_score > 0.1:
                        logger.info(f"ğŸ¯ HIGH BEHAVIOR: {place['name']}: pref={preference_score:.3f}, behav={behavior_score:.4f}, pop={popularity_normalized:.3f} â†’ final={final_score:.4f}")

                    place_dict['preference_score'] = preference_score
                    place_dict['behavior_score'] = behavior_score
                    place_dict['final_score'] = final_score
                    place_dict['source'] = 'priority_enhanced'
                    scored_places.append(place_dict)

            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            logger.info(f"ğŸ”„ [Priority] Sorting {len(scored_places)} scored places...")
            scored_places.sort(key=lambda x: x['final_score'], reverse=True)
            logger.info(f"âœ… [Priority] Sorting completed")

            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            logger.info(f"ğŸ”„ [Priority] Converting numpy arrays for {len(scored_places[:limit])} places...")
            result_places = scored_places[:limit]
            for i, place in enumerate(result_places):
                logger.info(f"ğŸ”„ [Priority] Processing place {i+1}/{len(result_places)}: {place.get('name', 'Unknown')}")
                if 'text_vector' in place and isinstance(place['text_vector'], np.ndarray):
                    place['text_vector'] = place['text_vector'].tolist()
                if 'image_vector' in place and isinstance(place['image_vector'], np.ndarray):
                    place['image_vector'] = place['image_vector'].tolist()
            logger.info(f"âœ… [Priority] Numpy conversion completed")

            behavior_used = user_behavior_vector is not None
            logger.info(f"ğŸš€ Priority-enhanced scoring completed: {len(scored_places)} total, {len(result_places)} results, behavior_vector: {'âœ…' if behavior_used else 'âŒ'}")

            return result_places

        except Exception as e:
            logger.error(f"âŒ Failed to calculate priority-enhanced scores: {e}")
            return []

    async def get_user_priority_tag(self, user_id: str) -> Optional[str]:
        """ì‚¬ìš©ìì˜ ì—¬í–‰ ìš°ì„ ìˆœìœ„ íƒœê·¸ ì¡°íšŒ"""
        try:
            async with self.db_manager.get_connection() as conn:
                query = """
                    SELECT priority
                    FROM user_preferences
                    WHERE user_id = $1
                """
                result = await conn.fetchval(query, user_id)
                return result
        except Exception as e:
            logger.error(f"âŒ Failed to get user priority tag for {user_id}: {e}")
            return None

    async def _calculate_preference_scores(
        self,
        user_preferences: Dict[str, Any],
        region: Optional[str] = None,
        category: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict]:
        """ì„ í˜¸ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ë° ì¶”ì²œ ìƒì„±"""
        try:
            # ì¥ì†Œ í›„ë³´êµ° ì¡°íšŒ
            places_query = """
                SELECT
                    place_id, table_name, region, name,
                    latitude, longitude, overview, image_urls, bookmark_cnt,
                    COALESCE(bookmark_cnt, 0) as popularity_score
                FROM place_recommendations
                WHERE name IS NOT NULL
                    AND ($1::text IS NULL OR region = $1)
                    AND ($2::text IS NULL OR table_name = $2)
                ORDER BY bookmark_cnt DESC
                LIMIT $3
            """

            async with self.db_manager.get_connection() as conn:
                places_data = await conn.fetch(
                    places_query,
                    region,
                    category,
                    CONFIG.candidate_limit
                )

            if not places_data:
                return []

            # ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
            scored_places = []
            popularity_normalizer = CONFIG.preference_weights['popularity_normalizer']

            for place in places_data:
                preference_score = self._calculate_place_preference_score(place, user_preferences)

                if preference_score > 0:
                    place_dict = dict(place)
                    popularity_normalized = min(place['popularity_score'] / popularity_normalizer, 1.0)
                    final_score = (preference_score * CONFIG.similarity_weight) + (popularity_normalized * CONFIG.popularity_weight)

                    place_dict['preference_score'] = preference_score
                    place_dict['final_score'] = final_score
                    scored_places.append(place_dict)

            scored_places.sort(key=lambda x: x['final_score'], reverse=True)
            
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            for place in scored_places[:limit]:
                if 'vector' in place and isinstance(place['vector'], np.ndarray):
                    place['vector'] = place['vector'].tolist()
                if 'text_vector' in place and isinstance(place['text_vector'], np.ndarray):
                    place['text_vector'] = place['text_vector'].tolist()
                if 'image_vector' in place and isinstance(place['image_vector'], np.ndarray):
                    place['image_vector'] = place['image_vector'].tolist()
            
            return scored_places[:limit]

        except Exception as e:
            logger.error(f"âŒ Failed to calculate preference scores: {e}")
            return []

    def _calculate_place_preference_score(
        self,
        place: Dict[str, Any],
        user_preferences: Dict[str, Any]
    ) -> float:
        """ê°œë³„ ì¥ì†Œì— ëŒ€í•œ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚° (í˜„ì¬ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìˆ˜ì •)"""
        score = 0.0

        try:
            weights = CONFIG.preference_weights

            # í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„
            persona = user_preferences.get('persona')
            if persona and place['table_name']:
                # í˜ë¥´ì†Œë‚˜ë³„ ì¹´í…Œê³ ë¦¬ ë³´ë„ˆìŠ¤
                persona_bonuses = {
                    'culture_lover': {'humanities': 0.3, 'culture': 0.3},
                    'nature_lover': {'nature': 0.3, 'leisure_sports': 0.2},
                    'foodie': {'restaurants': 0.4},
                    'shopper': {'shopping': 0.3},
                    'luxury_traveler': {'accommodation': 0.2, 'restaurants': 0.2}
                }

                if persona in persona_bonuses:
                    category_bonus = persona_bonuses[persona].get(place['table_name'], 0)
                    score += category_bonus

            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ (ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ì§€ì›)
            priority = user_preferences.get('priority')
            if priority:
                # ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ì²˜ë¦¬ (ì´ˆê°•ë ¥ í¸í–¥)
                if priority == place['table_name']:
                    score += 10.0  # ì¹´í…Œê³ ë¦¬ ì •í™•íˆ ì¼ì¹˜ ì‹œ ë§¤ìš° ê°•ë ¥í•œ ë³´ë„ˆìŠ¤
                    logger.info(f"ğŸ¯ CATEGORY MATCH BOOST: {place.get('name', 'Unknown')} gets +10.0 for matching {priority} priority")

                # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ì„ í†µí•œ ì¶”ê°€ ë§¤ì¹­
                priority_category_map = {
                    'accommodation': ['accommodation'],
                    'restaurants': ['restaurants'],
                    'shopping': ['shopping'],
                    'nature': ['nature'],
                    'culture': ['humanities'],
                    'leisure': ['leisure_sports']
                }

                if priority in priority_category_map:
                    if place['table_name'] in priority_category_map[priority]:
                        score += 10.0  # ë§¤í•‘ëœ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ì‹œì—ë„ ê°•ë ¥í•œ ë³´ë„ˆìŠ¤
                        logger.info(f"ğŸ¯ MAPPED CATEGORY BOOST: {place.get('name', 'Unknown')} gets +10.0 for {priority} -> {place['table_name']} mapping")

                # ê¸°ì¡´ popular/unique ì²˜ë¦¬ (ê¸°ë³¸ ë³´ë„ˆìŠ¤)
                if priority == 'popular' and place.get('bookmark_cnt', 0) > 1000:
                    score += 0.5
                elif priority == 'unique' and place.get('bookmark_cnt', 0) < 500:
                    score += 0.5

            # ì´ˆê°•ë ¥ íƒœê·¸ ë§¤ì¹­ ì‹œìŠ¤í…œ (ê³µê²©ì  í¸í–¥)
            preference_tags = user_preferences.get('preference_tags', {})
            if preference_tags:
                place_description = (place.get('description', '') or '').lower()
                place_name = (place.get('name', '') or '').lower()
                combined_text = place_description + ' ' + place_name

                tag_score, tag_count, max_weight = 0.0, 0, 0

                for tag_name, tag_weight in preference_tags.items():
                    tag_lower = tag_name.lower()

                    # ë‹¤ì–‘í•œ ë§¤ì¹­ ì „ëµ (ë” ê³µê²©ì )
                    match_found = False
                    match_strength = 0.0

                    # 1. ì™„ì „ ì¼ì¹˜ (100% ë§¤ì¹­)
                    if tag_lower in combined_text:
                        match_strength = 1.0
                        match_found = True

                    # 2. ë¶€ë¶„ ë§¤ì¹­ (ì–´ê·¼ ë§¤ì¹­ 70%)
                    elif any(word in combined_text for word in tag_lower.split()):
                        match_strength = 0.7
                        match_found = True

                    # 3. ìœ ì‚¬ ë‹¨ì–´ ë§¤ì¹­ (ë°”ì´ì–´ìŠ¤ 60%)
                    else:
                        # ê¸°ë³¸ ìœ ì‚¬ ì—°ê²° ì‚¬ì „
                        similar_words = {
                            'ìì—°': ['ì‚°', 'ë°”ë‹¤', 'í˜¸ìˆ˜', 'ê³µì›', 'ìˆ²', 'í•˜ì´í‚¹', 'íŠ¸ë ˆí‚¹'],
                            'ë¬¸í™”': ['ë°•ë¬¼ê´€', 'ë¯¸ìˆ ê´€', 'ì ˆ', 'ê¶ê¶', 'ì „í†µ', 'ì—­ì‚¬', 'í™”ê°€'],
                            'ë§›ì§‘': ['ìŒì‹', 'ë ˆìŠ¤í† ë‘', 'ì¹´í˜', 'ë°¥ì§‘', 'í•œì‹', 'ì–‘ì‹'],
                            'ì‡¼í•‘': ['ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì•„ìš¸ë ›', 'ì‹œì¥', 'ìƒê°€'],
                            'ì²´í—˜': ['ì•¡í‹°ë¹„í‹°', 'ë ˆì €', 'ë†€ì´', 'ì¶•ì œ', 'ê³µì—°']
                        }

                        for similar_key, similar_list in similar_words.items():
                            if tag_lower == similar_key and any(word in combined_text for word in similar_list):
                                match_strength = 0.6
                                match_found = True
                                break

                    if match_found:
                        # ê°€ì¤‘ì¹˜ ë°˜ì˜: 1-10 ìŠ¤ì¼€ì¼ì„ ë” ê³µê²©ì ìœ¼ë¡œ í™œìš©
                        raw_weight = tag_weight / 10.0  # 0.1 ~ 1.0

                        # ì´ˆê°•ë ¥ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ
                        if tag_weight >= 8:  # ìµœê³  ìš°ì„ ìˆœìœ„ (80% ì´ìƒ)
                            boosted_weight = raw_weight * 3.0 * match_strength  # 3ë°° ë¶€ìŠ¤íŠ¸
                        elif tag_weight >= 6:  # ë†’ì€ ìš°ì„ ìˆœìœ„ (60% ì´ìƒ)
                            boosted_weight = raw_weight * 2.5 * match_strength  # 2.5ë°° ë¶€ìŠ¤íŠ¸
                        elif tag_weight >= 4:  # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (40% ì´ìƒ)
                            boosted_weight = raw_weight * 2.0 * match_strength  # 2ë°° ë¶€ìŠ¤íŠ¸
                        else:
                            boosted_weight = raw_weight * match_strength  # ê¸°ë³¸ ê°€ì¤‘ì¹˜

                        tag_score += boosted_weight
                        tag_count += 1
                        max_weight = max(max_weight, boosted_weight)

                if tag_count > 0:
                    # ì´ˆê°•ë ¥ íƒœê·¸ ì ìˆ˜ ì ìš©
                    # 1. ê¸°ë³¸ ì ìˆ˜: í‰ê·  ëŒ€ì‹  ìµœëŒ€ê°’ ì‚¬ìš© (ë” ê³µê²©ì )
                    primary_score = min(max_weight, 2.0)  # ìµœëŒ€ 2.0ì 

                    # 2. ë‹¤ì¤‘ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì—¬ëŸ¬ íƒœê·¸ ë§¤ì¹­ ì‹œ ì¶”ê°€ ì ìˆ˜)
                    multi_match_bonus = min((tag_count - 1) * 0.3, 1.0)  # ìµœëŒ€ 1.0ì  ë³´ë„ˆìŠ¤

                    # 3. ìµœì¢… íƒœê·¸ ì ìˆ˜
                    final_tag_score = (primary_score + multi_match_bonus) * weights['tag']

                    # 4. ì¶”ê°€ ë¶€ìŠ¤íŠ¸ (CONFIGì—ì„œ ì„¤ì •í•œ ë§¤ê°œë³€ìˆ˜)
                    if 'tag_boost_multiplier' in weights:
                        final_tag_score *= weights['tag_boost_multiplier']

                    score += final_tag_score

                    # ë””ë²„ê¹… ë¡œê·¸
                    logger.debug(f"íƒœê·¸ ë§¤ì¹­ - ì¥ì†Œ: {place.get('name')}, ë§¤ì¹­ìˆ˜: {tag_count}, ìµœëŒ€ê°€ì¤‘ì¹˜: {max_weight:.3f}, ìµœì¢…ì ìˆ˜: {final_tag_score:.3f}")

            # íƒí—˜ ì„±í–¥ ë°˜ì˜
            exploration = user_preferences.get('exploration')
            if exploration == 'adventurous' and place['table_name'] in ['nature', 'leisure_sports']:
                score += 0.1
            elif exploration == 'comfort' and place['table_name'] in ['accommodation', 'restaurants']:
                score += 0.1

            return min(score, 1.0)

        except Exception as e:
            logger.error(f"âŒ Failed to calculate preference score for place {place.get('place_id')}: {e}")
            return 0.0

    async def _get_place_candidates_with_images(
        self,
        region: Optional[str],
        category: Optional[str]
    ) -> List[Dict]:
        """ì´ë¯¸ì§€ ë²¡í„°ë¥¼ í¬í•¨í•œ ì¥ì†Œ í›„ë³´êµ° ì¡°íšŒ"""
        try:
            query = """
                SELECT
                    pr.place_id::text as place_id,
                    pr.table_name,
                    pr.vector as text_vector,
                    pr.image_vector,
                    COALESCE(pr.bookmark_cnt, 0) as total_likes,
                    COALESCE(pr.bookmark_cnt, 0) as total_bookmarks,
                    COALESCE(pr.bookmark_cnt, 0) as total_clicks,
                    1 as unique_users,
                    COALESCE(pr.bookmark_cnt, 0)::float as popularity_score,
                    COALESCE(pr.bookmark_cnt, 0)::float as engagement_score,
                    pr.name,
                    pr.region,
                    pr.city,
                    pr.latitude,
                    pr.longitude,
                    pr.overview as description,
                    pr.image_urls,
                    pr.bookmark_cnt
                FROM place_recommendations pr
                WHERE
                    pr.vector IS NOT NULL
                    AND pr.name IS NOT NULL
                    AND pr.bookmark_cnt IS NOT NULL
            """

            params = []
            param_count = 0

            if region:
                param_count += 1
                query += f" AND pr.region = ${param_count}"
                params.append(region)

            if category:
                param_count += 1
                query += f" AND pr.table_name = ${param_count}::text"
                params.append(category)

            query += " ORDER BY COALESCE(pr.bookmark_cnt, 0) DESC"
            param_count += 1
            query += f" LIMIT ${param_count}"
            params.append(CONFIG.candidate_limit)

            places = await self.db_manager.execute_query(query, *params)

            # í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” í•„ìˆ˜, ì´ë¯¸ì§€ ë²¡í„°ëŠ” ì„ íƒì 
            valid_places = []
            for place in places:
                text_vector = validate_vector_data(place['text_vector'])
                if text_vector is not None:
                    place['text_vector'] = text_vector

                    # ì´ë¯¸ì§€ ë²¡í„°ëŠ” ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ None
                    image_vector = validate_vector_data(place.get('image_vector'))
                    place['image_vector'] = image_vector

                    # S3 ì´ë¯¸ì§€ URLì„ HTTPSë¡œ ë³€í™˜
                    place = self._convert_s3_urls_to_https(place)
                    valid_places.append(place)

            logger.info(f"ğŸ“‹ Retrieved {len(valid_places)} places with text vectors ({sum(1 for p in valid_places if p['image_vector'] is not None)} with image vectors)")
            return valid_places

        except Exception as e:
            logger.error(f"âŒ Failed to get place candidates with images: {e}")
            return []

    async def _get_user_image_preferences(self, user_id: str) -> Dict[str, np.ndarray]:
        """ì‚¬ìš©ìì˜ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë²¡í„° ìˆ˜ì§‘ (ë¶ë§ˆí¬, ì¢‹ì•„ìš” ê¸°ë°˜)"""
        try:

            # 1. ë¶ë§ˆí¬í•œ ì¥ì†Œë“¤ì˜ ì´ë¯¸ì§€ ë²¡í„° ìˆ˜ì§‘
            bookmark_query = """
                SELECT pr.image_vector
                FROM saved_locations sl
                JOIN place_recommendations pr ON pr.place_id = CAST(SPLIT_PART(sl.places, ':', 2) AS INTEGER)
                    AND pr.table_name = SPLIT_PART(sl.places, ':', 1)
                WHERE sl.user_id = $1
                    AND pr.image_vector IS NOT NULL
                LIMIT 20
            """

            # 2. ì¢‹ì•„ìš”í•œ í¬ìŠ¤íŠ¸ë“¤ì˜ ì´ë¯¸ì§€ ë²¡í„° ìˆ˜ì§‘ (posts.image_vectorëŠ” PostgreSQL vector íƒ€ì…)
            liked_posts_query = """
                SELECT p.image_vector
                FROM user_actions ua
                JOIN posts p ON p.id = CAST(ua.place_id AS INTEGER)
                WHERE ua.user_id = $1
                    AND ua.action_type = 'like'
                    AND ua.place_category = 'posts'
                    AND p.image_vector IS NOT NULL
                LIMIT 20
            """

            # 3. ì‚¬ìš©ìê°€ ì§ì ‘ ì—…ë¡œë“œí•œ í¬ìŠ¤íŠ¸ë“¤ì˜ ì´ë¯¸ì§€ ë²¡í„° ìˆ˜ì§‘ (ìì‹ ì˜ ì„ í˜¸ë„ ë°˜ì˜)
            user_posts_query = """
                SELECT image_vector
                FROM posts
                WHERE user_id = $1
                    AND image_vector IS NOT NULL
                LIMIT 30
            """

            bookmark_vectors = await self.db_manager.execute_query(bookmark_query, user_id)
            liked_post_vectors = await self.db_manager.execute_query(liked_posts_query, user_id)
            user_post_vectors = await self.db_manager.execute_query(user_posts_query, user_id)

            # ë²¡í„° ìˆ˜ì§‘ ë° ê²€ì¦ (ë¶„ë¦¬ëœ ë¦¬ìŠ¤íŠ¸ë¡œ)
            bookmark_image_vectors = []
            liked_post_image_vectors = []
            user_upload_image_vectors = []

            # 1. ë¶ë§ˆí¬í•œ ì¥ì†Œ ì´ë¯¸ì§€ ë²¡í„°
            for row in bookmark_vectors:
                vector = validate_vector_data(row['image_vector'])
                if vector is not None:
                    bookmark_image_vectors.append(vector)

            # 2. ì¢‹ì•„ìš”í•œ í¬ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë²¡í„° (ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì§‘)
            for row in liked_post_vectors:
                vector = validate_vector_data(row['image_vector'])
                if vector is not None:
                    liked_post_image_vectors.append(vector)

            # 3. ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ í¬ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë²¡í„° (ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì§‘)
            for row in user_post_vectors:
                vector = validate_vector_data(row['image_vector'])
                if vector is not None:
                    user_upload_image_vectors.append(vector)

            total_vectors = len(bookmark_image_vectors) + len(liked_post_image_vectors) + len(user_upload_image_vectors)

            if total_vectors == 0:
                logger.info(f"No image preferences found for user {user_id}")
                return {}

            logger.info(f"ğŸ“¸ User {user_id} image preferences: {total_vectors} total vectors (ë¶ë§ˆí¬: {len(bookmark_image_vectors)}, ì¢‹ì•„ìš”: {len(liked_post_image_vectors)}, ì—…ë¡œë“œ: {len(user_upload_image_vectors)})")

            return {
                'bookmarks': bookmark_image_vectors,       # ë¶ë§ˆí¬ ì¥ì†Œ ì´ë¯¸ì§€ (ì±„ë„4 ì‚¬ìš©)
                'liked_posts': liked_post_image_vectors,   # ì¢‹ì•„ìš” í¬ìŠ¤íŠ¸ ì´ë¯¸ì§€ (ì±„ë„5 ì‚¬ìš©)
                'user_uploads': user_upload_image_vectors, # ì—…ë¡œë“œ í¬ìŠ¤íŠ¸ ì´ë¯¸ì§€ (ì±„ë„2 ì‚¬ìš©)
                'source_breakdown': {
                    'bookmarks': len(bookmark_image_vectors),
                    'liked_posts': len(liked_post_image_vectors),
                    'user_posts': len(user_upload_image_vectors)
                }
            }

        except Exception as e:
            logger.error(f"âŒ Failed to get user image preferences for {user_id}: {e}")
            return {}

    async def _calculate_independent_similarities(
        self,
        user_id: str,
        user_behavior_vector: np.ndarray,
        user_image_preferences: Dict[str, Any],
        places: List[Dict]
    ) -> List[Dict[str, float]]:
        """
        ë…ë¦½ì ì¸ ê²€ìƒ‰ ì±„ë„ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (5ê°œ ì±„ë„)
        1. í–‰ë™ë²¡í„°(í´ë¦­/ë¶ë§ˆí¬) â†’ ì¥ì†Œ í…ìŠ¤íŠ¸
        2. ì‚¬ìš©ì ì—…ë¡œë“œ í¬ìŠ¤íŒ… ì´ë¯¸ì§€ â†’ ì¥ì†Œ ì´ë¯¸ì§€
        3. ë¶ë§ˆí¬ ì¥ì†Œ í…ìŠ¤íŠ¸ â†’ ë‹¤ë¥¸ ì¥ì†Œ í…ìŠ¤íŠ¸
        4. ë¶ë§ˆí¬ ì¥ì†Œ ì´ë¯¸ì§€ â†’ ë‹¤ë¥¸ ì¥ì†Œ ì´ë¯¸ì§€
        5. ì¢‹ì•„ìš”í•œ í¬ìŠ¤íŒ… ì´ë¯¸ì§€ â†’ ì¥ì†Œ ì´ë¯¸ì§€
        """
        results = []

        try:
            # ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ë¦¬ (ì—…ë¡œë“œ vs ì¢‹ì•„ìš”)
            user_upload_images = user_image_preferences.get('user_uploads', [])
            liked_post_images = user_image_preferences.get('liked_posts', [])

            # í‰ê·  ë²¡í„° ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)
            user_upload_vector = None
            if user_upload_images and len(user_upload_images) > 0:
                try:
                    user_upload_vector = np.mean(user_upload_images, axis=0)
                except Exception as e:
                    logger.warning(f"Failed to calculate user upload vector mean: {e}")

            liked_posts_vector = None
            if liked_post_images and len(liked_post_images) > 0:
                try:
                    liked_posts_vector = np.mean(liked_post_images, axis=0)
                except Exception as e:
                    logger.warning(f"Failed to calculate liked posts vector mean: {e}")

            # ì‚¬ìš©ì ë¶ë§ˆí¬ ì¥ì†Œ ê¸°ë°˜ ì„ í˜¸ë„ ì¶”ì¶œ
            bookmark_preferences = await self._get_detailed_bookmark_preferences(user_id)

            for place in places:
                scores = {
                    'behavior_text_similarity': 0.0,     # í–‰ë™ë²¡í„°(í´ë¦­/ë¶ë§ˆí¬) â†’ ì¥ì†Œí…ìŠ¤íŠ¸
                    'upload_image_similarity': 0.0,      # ì—…ë¡œë“œ í¬ìŠ¤íŒ…ì´ë¯¸ì§€ â†’ ì¥ì†Œì´ë¯¸ì§€
                    'bookmark_text_similarity': 0.0,     # ë¶ë§ˆí¬ì¥ì†Œí…ìŠ¤íŠ¸ â†’ ì¥ì†Œí…ìŠ¤íŠ¸
                    'bookmark_image_similarity': 0.0,    # ë¶ë§ˆí¬ì¥ì†Œì´ë¯¸ì§€ â†’ ì¥ì†Œì´ë¯¸ì§€
                    'liked_post_similarity': 0.0,        # ì¢‹ì•„ìš” í¬ìŠ¤íŒ…ì´ë¯¸ì§€ â†’ ì¥ì†Œì´ë¯¸ì§€
                    'combined_score': 0.0
                }

                # 1. í–‰ë™ ë²¡í„°(í´ë¦­/ë¶ë§ˆí¬) â†’ ì¥ì†Œ í…ìŠ¤íŠ¸ (384ì°¨ì›)
                place_text_vector = place.get('text_vector')
                if place_text_vector is not None and user_behavior_vector is not None:
                    text_sim = safe_cosine_similarity(user_behavior_vector, place_text_vector, use_ann=False)
                    scores['behavior_text_similarity'] = float(text_sim[0]) if len(text_sim) > 0 else 0.0

                # 2. ì—…ë¡œë“œ í¬ìŠ¤íŒ… ì´ë¯¸ì§€ â†’ ì¥ì†Œ ì´ë¯¸ì§€ (512ì°¨ì›)
                place_image_vector = place.get('image_vector')
                if user_upload_vector is not None and place_image_vector is not None:
                    upload_sim = safe_cosine_similarity(user_upload_vector, place_image_vector, use_ann=False)
                    scores['upload_image_similarity'] = float(upload_sim[0]) if len(upload_sim) > 0 else 0.0

                # 3. ë¶ë§ˆí¬ ì¥ì†Œ í…ìŠ¤íŠ¸ â†’ ì¥ì†Œ í…ìŠ¤íŠ¸ (384ì°¨ì›)
                if bookmark_preferences.get('avg_text_vector') is not None and place_text_vector is not None:
                    bookmark_text_sim = safe_cosine_similarity(
                        bookmark_preferences['avg_text_vector'], place_text_vector, use_ann=False
                    )
                    scores['bookmark_text_similarity'] = float(bookmark_text_sim[0]) if len(bookmark_text_sim) > 0 else 0.0

                # 4. ë¶ë§ˆí¬ ì¥ì†Œ ì´ë¯¸ì§€ â†’ ì¥ì†Œ ì´ë¯¸ì§€ (512ì°¨ì›)
                if bookmark_preferences.get('avg_image_vector') is not None and place_image_vector is not None:
                    bookmark_image_sim = safe_cosine_similarity(
                        bookmark_preferences['avg_image_vector'], place_image_vector, use_ann=False
                    )
                    scores['bookmark_image_similarity'] = float(bookmark_image_sim[0]) if len(bookmark_image_sim) > 0 else 0.0

                # 5. ì¢‹ì•„ìš”í•œ í¬ìŠ¤íŒ… ì´ë¯¸ì§€ â†’ ì¥ì†Œ ì´ë¯¸ì§€ (512ì°¨ì›)
                if liked_posts_vector is not None and place_image_vector is not None:
                    liked_sim = safe_cosine_similarity(liked_posts_vector, place_image_vector, use_ann=False)
                    scores['liked_post_similarity'] = float(liked_sim[0]) if len(liked_sim) > 0 else 0.0

                # 6. 5ê°œ ë…ë¦½ì  ì±„ë„ë“¤ì˜ ì¡°í•© ì ìˆ˜ ê³„ì‚°
                channel_scores = [
                    scores['behavior_text_similarity'] * 0.25,     # í–‰ë™ê¸°ë°˜ í…ìŠ¤íŠ¸
                    scores['upload_image_similarity'] * 0.25,      # ì—…ë¡œë“œ í¬ìŠ¤íŒ… ì´ë¯¸ì§€
                    scores['bookmark_text_similarity'] * 0.2,      # ë¶ë§ˆí¬ í…ìŠ¤íŠ¸
                    scores['bookmark_image_similarity'] * 0.15,    # ë¶ë§ˆí¬ ì´ë¯¸ì§€
                    scores['liked_post_similarity'] * 0.15         # ì¢‹ì•„ìš” í¬ìŠ¤íŒ… ì´ë¯¸ì§€
                ]

                # ìœ íš¨í•œ ì±„ë„ë“¤ë§Œ ì¡°í•©
                valid_scores = [score for score in channel_scores if score > 0]
                if valid_scores:
                    scores['combined_score'] = sum(valid_scores) / len(valid_scores)
                    # ë‹¤ì¤‘ ì±„ë„ ë³´ë„ˆìŠ¤
                    if len(valid_scores) > 1:
                        scores['combined_score'] += 0.1 * (len(valid_scores) - 1)
                else:
                    scores['combined_score'] = 0.0

                results.append(scores)

            logger.info(f"ğŸ”„ Calculated independent channel similarities for {len(results)} places")
            return results

        except Exception as e:
            logger.error(f"âŒ Independent similarity calculation failed: {e}")
            # ë¹ˆ ì ìˆ˜ ë°˜í™˜
            return [{
                'behavior_text_similarity': 0.0,
                'upload_image_similarity': 0.0,
                'bookmark_text_similarity': 0.0,
                'bookmark_image_similarity': 0.0,
                'liked_post_similarity': 0.0,
                'combined_score': 0.0
            } for _ in places]

    async def _get_detailed_bookmark_preferences(self, user_id: str) -> Dict[str, np.ndarray]:
        """ë¶ë§ˆí¬í•œ ì¥ì†Œë“¤ì˜ ìƒì„¸ ë²¡í„° ì„ í˜¸ë„ ì¶”ì¶œ"""
        try:
            # ë¶ë§ˆí¬í•œ ì¥ì†Œë“¤ì˜ í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ë²¡í„° ì¡°íšŒ
            query = """
                SELECT pr.vector as text_vector, pr.image_vector
                FROM saved_locations sl
                JOIN place_recommendations pr ON pr.place_id = CAST(SPLIT_PART(sl.places, ':', 2) AS INTEGER)
                    AND pr.table_name = SPLIT_PART(sl.places, ':', 1)
                WHERE sl.user_id = $1
                    AND (pr.vector IS NOT NULL OR pr.image_vector IS NOT NULL)
                LIMIT 30
            """

            bookmark_data = await self.db_manager.execute_query(query, user_id)

            text_vectors = []
            image_vectors = []

            for row in bookmark_data:
                # í…ìŠ¤íŠ¸ ë²¡í„° ìˆ˜ì§‘
                if row['text_vector']:
                    text_vector = validate_vector_data(row['text_vector'])
                    if text_vector is not None:
                        text_vectors.append(text_vector)

                # ì´ë¯¸ì§€ ë²¡í„° ìˆ˜ì§‘
                if row['image_vector']:
                    image_vector = validate_vector_data(row['image_vector'])
                    if image_vector is not None:
                        image_vectors.append(image_vector)

            result = {}

            # í‰ê·  í…ìŠ¤íŠ¸ ë²¡í„° ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)
            if text_vectors and len(text_vectors) > 0:
                try:
                    result['avg_text_vector'] = np.mean(text_vectors, axis=0)
                except Exception as e:
                    logger.warning(f"Failed to calculate avg text vector: {e}")

            # í‰ê·  ì´ë¯¸ì§€ ë²¡í„° ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)
            if image_vectors and len(image_vectors) > 0:
                try:
                    result['avg_image_vector'] = np.mean(image_vectors, axis=0)
                except Exception as e:
                    logger.warning(f"Failed to calculate avg image vector: {e}")

            logger.info(f"ğŸ“š User {user_id} bookmark preferences: {len(text_vectors)} text, {len(image_vectors)} image vectors")
            return result

        except Exception as e:
            logger.error(f"âŒ Failed to get detailed bookmark preferences for {user_id}: {e}")
            return {}

    async def _get_fast_place_candidates(
        self,
        region: Optional[str],
        category: Optional[str],
        limit: int = 50
    ) -> List[Dict]:
        """ë©”ì¸ í˜ì´ì§€ìš© ê³ ì† ì¥ì†Œ í›„ë³´ ì¡°íšŒ (ìµœì†Œ ì»¬ëŸ¼ë§Œ)"""
        try:
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"fast_places:{region or 'all'}:{category or 'all'}:{limit}"

            # ìºì‹œ í™•ì¸
            if self._is_cache_valid(cache_key, 'place_batch'):
                self.stats['cache_hits'] += 1
                return self.place_batch_cache[cache_key]

            # DBì—ì„œ í•µì‹¬ ì»¬ëŸ¼ ì¡°íšŒ (ì´ë¯¸ì§€ ë²¡í„°ëŠ” ì œì™¸í•˜ë˜ ì´ë¯¸ì§€ URLì€ í¬í•¨)
            query = """
                SELECT
                    pr.place_id::text as place_id,
                    pr.table_name,
                    pr.vector as vector,
                    pr.name,
                    pr.region,
                    pr.city,
                    pr.latitude,
                    pr.longitude,
                    pr.bookmark_cnt,
                    pr.image_urls,  -- ë©”ì¸ í˜ì´ì§€ ì´ë¯¸ì§€ í‘œì‹œìš©
                    pr.overview,    -- ê°„ë‹¨í•œ ì„¤ëª…
                    COALESCE(pr.bookmark_cnt, 0) as total_likes,
                    COALESCE(pr.bookmark_cnt, 0) as total_bookmarks,
                    COALESCE(pr.bookmark_cnt, 0) as total_clicks,
                    COALESCE(pr.bookmark_cnt, 0)::float as popularity_score,
                    COALESCE(pr.bookmark_cnt, 0)::float as engagement_score
                FROM place_recommendations pr
                WHERE
                    pr.vector IS NOT NULL
                    AND pr.name IS NOT NULL
                    AND pr.bookmark_cnt IS NOT NULL
                    AND pr.bookmark_cnt > 0
            """

            params = []
            param_count = 0

            # ì§€ì—­ í•„í„°
            if region:
                param_count += 1
                query += f" AND pr.region = ${param_count}"
                params.append(region)

            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if category:
                param_count += 1
                query += f" AND pr.table_name = ${param_count}::text"
                params.append(category)

            # ì„±ëŠ¥ ìµœì í™”: ë¶ë§ˆí¬ ê¸°ì¤€ ì •ë ¬ë¡œ ìƒìœ„ë§Œ ì¡°íšŒ
            query += " ORDER BY COALESCE(pr.bookmark_cnt, 0) DESC"
            param_count += 1
            query += f" LIMIT ${param_count}"
            params.append(limit)

            places = await self.db_manager.execute_query(query, *params)

            # ë²¡í„° ê²€ì¦ ë° ì´ë¯¸ì§€ URL ë³€í™˜
            valid_places = []
            for place in places:
                if place['vector'] is not None:  # ê°„ë‹¨í•œ null ì²´í¬ë§Œ
                    # S3 ì´ë¯¸ì§€ URLì„ HTTPSë¡œ ë³€í™˜ (ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©)
                    place = self._convert_s3_urls_to_https(place)
                    valid_places.append(place)

            # ìºì‹œì— ì €ì¥
            self._update_cache(cache_key, valid_places, 'place_batch')

            logger.info(f"ğŸ“‹ Fast retrieval: {len(valid_places)} places for {region or 'all'}/{category or 'all'}")
            return valid_places

        except Exception as e:
            logger.error(f"âŒ Failed to get fast place candidates: {e}")
            return []

    async def _get_fast_vector_recommendations(
        self,
        user_id: str,
        user_vector: np.ndarray,
        region: Optional[str],
        category: Optional[str],
        limit: int
    ) -> List[Dict]:
        """ë©”ì¸ í˜ì´ì§€ìš© ê³ ì† ë²¡í„° ê¸°ë°˜ ì¶”ì²œ (ë‹¨ì¼ ì±„ë„ë§Œ)"""
        try:
            # ìºì‹œëœ ìœ ì‚¬ë„ ê²°ê³¼ í™•ì¸
            similarity_cache_key = f"similarity:{user_id}:{region or 'all'}:{category or 'all'}:{limit}"
            if self._is_cache_valid(similarity_cache_key, 'similarity'):
                logger.info(f"âš¡ Using cached similarities for user {user_id}")
                cached_results = self.similarity_cache[similarity_cache_key]
                return cached_results[:limit]

            # ê³ ì† ì¥ì†Œ í›„ë³´ ì¡°íšŒ (ì œí•œëœ ìˆ˜)
            places = await self._get_fast_place_candidates(region, category, min(limit * 3, 150))

            if not places:
                logger.warning("âš ï¸ No fast candidates found, falling back to popular")
                return await self._get_popular_recommendations(region, category, limit, fast_mode=True)

            # ë²¡í„° ë°°ì¹˜ ì²˜ë¦¬ (ìµœì í™”)
            place_vectors = []
            valid_places = []

            for place in places:
                vector = validate_vector_data(place['vector'])
                if vector is not None:
                    place_vectors.append(vector)
                    # S3 ì´ë¯¸ì§€ URLì„ HTTPSë¡œ ë³€í™˜
                    place = self._convert_s3_urls_to_https(place)
                    valid_places.append(place)

            if not valid_places:
                return await self._get_popular_recommendations(region, category, limit, fast_mode=True)

            # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ì¼ ì±„ë„)
            place_vectors_array = np.array(place_vectors, dtype=np.float32)
            similarities = safe_cosine_similarity(
                user_vector,
                place_vectors_array,
                use_ann=self.ann_enabled,
                faiss_manager=self.faiss_manager
            )

            # í†µê³„ ì—…ë°ì´íŠ¸
            if self.ann_enabled:
                self.stats['ann_searches'] += 1
            else:
                self.stats['cosine_searches'] += 1

            # ê°„ì†Œí™”ëœ ì ìˆ˜ ê³„ì‚° (ë³µì¡í•œ ê°€ì¤‘ì¹˜ ì—†ìŒ)
            results = []
            for i, place in enumerate(valid_places):
                try:
                    similarity = float(similarities[i])

                    # ì„ê³„ê°’ ì ìš©
                    if similarity < CONFIG.min_similarity_threshold:
                        continue

                    # ê°„ë‹¨í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ë¹ ë¥¸ ê³„ì‚°)
                    bookmark_count = place.get('bookmark_cnt', 0)
                    popularity_factor = min(bookmark_count / 100.0, 1.0)  # ê°„ë‹¨í•œ ì •ê·œí™”

                    final_score = similarity * 0.7 + popularity_factor * 0.3

                    place['similarity_score'] = round(similarity, 4)
                    place['final_score'] = round(final_score, 4)
                    place['recommendation_type'] = 'fast_personalized'

                    results.append(place)

                except Exception as e:
                    logger.error(f"âŒ Fast score calculation failed for place {i}: {e}")
                    continue

            # ì ìˆ˜ìˆœ ì •ë ¬
            results.sort(key=lambda x: x['final_score'], reverse=True)
            final_results = results[:limit]

            # ìœ ì‚¬ë„ ê²°ê³¼ ìºì‹œ
            self._update_cache(similarity_cache_key, final_results, 'similarity')

            logger.info(f"âš¡ Fast recommendations: {len(final_results)} results for user {user_id}")

            # ì´ë¯¸ì§€ URL í¬í•¨ ì—¬ë¶€ ë””ë²„ê¹…
            image_count = sum(1 for place in final_results if place.get('image_urls'))
            logger.info(f"ğŸ“¸ ì´ë¯¸ì§€ í¬í•¨ ì¥ì†Œ: {image_count}/{len(final_results)}ê°œ")

            # ì²« ë²ˆì§¸ ê²°ê³¼ì˜ ì´ë¯¸ì§€ URL ë¡œê¹…
            if final_results and final_results[0].get('image_urls'):
                first_place = final_results[0]
                logger.info(f"ğŸ–¼ï¸ ì²« ë²ˆì§¸ ì¥ì†Œ '{first_place.get('name')}' ì´ë¯¸ì§€: {first_place.get('image_urls')}")

            return final_results

        except Exception as e:
            logger.error(f"âŒ Fast vector recommendation failed: {e}")
            return await self._get_popular_recommendations(region, category, limit, fast_mode=True)


    async def _get_hybrid_fast_recommendations(
        self,
        user_id: str,
        user_priority: str,
        region: Optional[str],
        category: Optional[str],
        limit: int
    ) -> List[Dict]:
        """Fast modeìš© í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (95% ì„ í˜¸ë„ + 5% í–‰ë™ ë°ì´í„°)"""
        try:
            # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í˜¼í•©
            extended_limit = min(limit * 4, 100)

            # 1. ì„ í˜¸ë„ ê¸°ë°˜ ì¶”ì²œ (95%)
            preference_results = await self._get_preference_based_recommendations(
                user_id, region, category, extended_limit
            )

            # 2. í–‰ë™ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ (5%)
            user_vector = await self._get_user_behavior_vector_cached(user_id)
            behavior_results = []
            if user_vector is not None:
                behavior_results = await self._get_fast_vector_recommendations(
                    user_id, user_vector, region, category, extended_limit
                )

            # 3. ê²°ê³¼ í˜¼í•© (95:5 ë¹„ìœ¨) - í•˜ë“œì½”ë”©ìœ¼ë¡œ ì„ì‹œ í•´ê²°
            preference_weight = 0.95  # experienced_user_preference_weight
            behavior_weight = 0.05    # experienced_user_behavior_weight

            # ì ìˆ˜ ì¬ê³„ì‚° ë° í˜¼í•©
            combined_results = {}

            # ì„ í˜¸ë„ ê²°ê³¼ ì¶”ê°€ (95% ê°€ì¤‘ì¹˜)
            for place in preference_results:
                place_id = place.get('place_id') or place.get('id')
                if place_id:
                    adjusted_score = place.get('final_score', 0) * preference_weight
                    place_copy = place.copy()
                    place_copy['final_score'] = adjusted_score
                    place_copy['source'] = 'preference'
                    combined_results[place_id] = place_copy

            # í–‰ë™ ë°ì´í„° ê²°ê³¼ ì¶”ê°€ (5% ê°€ì¤‘ì¹˜, ì¤‘ë³µ ì‹œ ì ìˆ˜ í•©ì‚°)
            for place in behavior_results:
                place_id = place.get('place_id') or place.get('id')
                if place_id:
                    adjusted_score = place.get('final_score', 0) * behavior_weight

                    if place_id in combined_results:
                        # ê¸°ì¡´ ì„ í˜¸ë„ ì ìˆ˜ì— í–‰ë™ ì ìˆ˜ ì¶”ê°€
                        combined_results[place_id]['final_score'] += adjusted_score
                        combined_results[place_id]['source'] = 'hybrid'
                    else:
                        # ìƒˆë¡œìš´ í–‰ë™ ê¸°ë°˜ ê²°ê³¼
                        place_copy = place.copy()
                        place_copy['final_score'] = adjusted_score
                        place_copy['source'] = 'behavior'
                        combined_results[place_id] = place_copy

            # ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° ì œí•œ
            final_results = list(combined_results.values())
            final_results.sort(key=lambda x: x['final_score'], reverse=True)
            final_results = final_results[:limit]

            # í†µê³„ ë¡œê¹…
            preference_count = sum(1 for r in final_results if r.get('source') in ['preference', 'hybrid'])
            behavior_count = sum(1 for r in final_results if r.get('source') == 'behavior')
            hybrid_count = sum(1 for r in final_results if r.get('source') == 'hybrid')

            logger.info(f"ğŸ”„ Hybrid results: {len(final_results)} total "
                      f"(preference: {preference_count-hybrid_count}, behavior: {behavior_count}, hybrid: {hybrid_count})")

            return final_results

        except Exception as e:
            logger.error(f"âŒ Hybrid fast recommendation failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì„ í˜¸ë„ ê¸°ë°˜ìœ¼ë¡œ í´ë°±
            return await self._get_preference_based_recommendations(user_id, region, category, limit)

    async def _get_user_personalized_recommendations(
        self,
        user_id: str,
        limit: int = 50
    ) -> List[Dict]:
        """
        ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ (ì „ì²´ ì§€ì—­ ëŒ€ìƒ)
        - í–‰ë™ ë²¡í„°ê°€ ìˆìœ¼ë©´: ëª¨ë“  ì§€ì—­ì—ì„œ í–‰ë™ ë²¡í„° ê¸°ë°˜ ìœ ì‚¬ë„ ìˆœ ì¶”ì²œ
        - í–‰ë™ ë²¡í„°ê°€ ì—†ìœ¼ë©´: ëª¨ë“  ì§€ì—­ì—ì„œ ì„ í˜¸ë„ ê¸°ë°˜ ì ìˆ˜ ìˆœ ì¶”ì²œ
        - ì¤‘ë³µ ì œê±° í›„ ìœ ì‚¬ë„/ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        """
        try:
            # 1. ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ ì¡°íšŒ
            logger.info(f"ğŸ” [Personalized] Getting user preferences for user {user_id}")
            user_preferences = await self._get_user_preferences(user_id)
            if not user_preferences:
                logger.info(f"âŒ [Personalized] No preferences found for user {user_id}")
                return []

            user_priority = user_preferences.get('priority')
            if not user_priority:
                logger.info(f"âŒ [Personalized] No priority found for user {user_id}")
                return []

            logger.info(f"âœ… [Personalized] Found user preferences for user {user_id}: priority={user_priority}")

            # 2. ì‚¬ìš©ì í–‰ë™ ë²¡í„° ì¡°íšŒ (ë¶ë§ˆí¬ íŒ¨í„´ ë¶„ì„ìš©)
            logger.info(f"ğŸ§  [Personalized] Getting user behavior vector for user {user_id}")
            user_behavior_vector = await self._get_user_behavior_vector_cached(user_id)
            if user_behavior_vector is not None:
                logger.info(f"âœ… [Personalized] Found behavior vector for user {user_id}: shape {user_behavior_vector.shape}")
                logger.info(f"ğŸ” [Personalized] Behavior vector type: {type(user_behavior_vector)}, non-zero elements: {np.count_nonzero(user_behavior_vector)}")
            else:
                logger.info(f"âŒ [Personalized] No behavior vector found for user {user_id}")
                # ì¶”ê°€ ë””ë²„ê¹…: user_behavior_vectors í…Œì´ë¸” ì§ì ‘ í™•ì¸
                try:
                    async with self.db_manager.get_connection() as conn:
                        debug_result = await conn.fetchrow(
                            "SELECT user_id, behavior_vector IS NOT NULL as has_vector FROM user_behavior_vectors WHERE user_id = $1",
                            user_id
                        )
                        if debug_result:
                            logger.info(f"ğŸ” [Personalized] DEBUG: user_behavior_vectors table has user {user_id}, has_vector: {debug_result['has_vector']}")
                        else:
                            logger.info(f"ğŸ” [Personalized] DEBUG: user {user_id} not found in user_behavior_vectors table")
                except Exception as debug_e:
                    logger.warning(f"âš ï¸ [Personalized] DEBUG query failed: {debug_e}")

            # 2. ëª¨ë“  ì§€ì—­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì ìˆ˜ ê³„ì‚° ì—†ì´)
            regions_query = """
                SELECT DISTINCT region
                FROM place_recommendations
                WHERE region IS NOT NULL
                ORDER BY region
            """

            all_regions = []
            async with self.db_manager.get_connection() as conn:
                regions_data = await conn.fetch(regions_query)
                all_regions = [row['region'] for row in regions_data]

            logger.info(f"ğŸ“ Found {len(all_regions)} regions: {all_regions}")

            # 3. ëª¨ë“  ì§€ì—­ì˜ ì¶”ì²œì„ ìˆ˜ì§‘í•˜ê³  ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            all_recommendations = []
            logger.info(f"ğŸ”¥ [DEBUG] Starting to collect recommendations from {len(all_regions)} regions")

            for region in all_regions:
                # í•´ë‹¹ ì§€ì—­ì˜ ì¶”ì²œ ìƒì„±
                logger.info(f"ğŸ¯ [Personalized] Generating recommendations for region {region}")

                # behavior_vectorê°€ ìˆëŠ” ì‚¬ìš©ìëŠ” í–‰ë™ ë²¡í„° ê¸°ë°˜ ìœ ì‚¬í•œ ì¥ì†Œë§Œ ì¶”ì²œ (ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ë¬´ê´€)
                if user_behavior_vector is not None:
                    logger.info(f"ğŸ§  [Personalized] Using behavior vector based recommendations for region {region}")

                    # í–‰ë™ ë²¡í„° ê¸°ë°˜ ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œë§Œ ì‚¬ìš© (ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ìƒê´€ì—†ì´)
                    try:
                        region_recommendations = await self._get_similar_places_in_region(
                            user_behavior_vector, region, 5  # ê° ì§€ì—­ì—ì„œ 5ê°œì”©ë§Œ ê°€ì ¸ì˜¤ê¸°
                        )
                        logger.info(f"ğŸ” [Personalized] Found {len(region_recommendations)} behavior-based recommendations for {region}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Similar places search failed for {region}, using fallback: {e}")
                        # í´ë°±ìœ¼ë¡œ ê°„ë‹¨í•œ ì§€ì—­ ê¸°ë°˜ ì¶”ì²œ ì‚¬ìš©
                        region_recommendations = await self._get_simple_regional_recommendations(region, 5)
                else:
                    logger.info(f"ğŸ¯ [Personalized] Using priority-based recommendations for region {region}")

                    # ê¸°ì¡´ ë°©ì‹: ìš°ì„ ìˆœìœ„ íƒœê·¸ ê¸°ë°˜ ì¶”ì²œë§Œ - ë” ë§ì€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    region_recommendations = await self._calculate_priority_enhanced_scores(
                        user_preferences, user_behavior_vector, region, None, 15  # ê° ì§€ì—­ì—ì„œ 15ê°œì”© ê°€ì ¸ì˜¤ê¸° (5->15ë¡œ ì¦ê°€)
                    )

                if region_recommendations:
                    # ì§€ì—­ ì •ë³´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for rec in region_recommendations:
                        rec['source_region'] = region

                    all_recommendations.extend(region_recommendations)
                    logger.info(f"ğŸ”¥ [DEBUG] Added {len(region_recommendations)} recommendations from {region}")
                else:
                    logger.info(f"ğŸ”¥ [DEBUG] No recommendations from {region}")

            # ë””ë²„ê¹…: ì§€ì—­ë³„ ì¶”ì²œ ë¶„í¬ í™•ì¸
            region_distribution = {}
            for rec in all_recommendations:
                region = rec.get('source_region', 'unknown')
                region_distribution[region] = region_distribution.get(region, 0) + 1

            logger.info(f"ğŸ—ºï¸ [DEBUG] Regional distribution BEFORE sorting: {region_distribution}")
            logger.info(f"ğŸ”¢ [DEBUG] Total recommendations collected: {len(all_recommendations)}")
            logger.info(f"ğŸ”¥ [DEBUG] About to sort recommendations")

            # 5. ëª¨ë“  ì¶”ì²œì„ ìœ ì‚¬ë„/ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            if user_behavior_vector is not None:
                # í–‰ë™ ë²¡í„°ê°€ ìˆëŠ” ê²½ìš°: similarity_score ë˜ëŠ” final_scoreë¡œ ì •ë ¬
                all_recommendations.sort(key=lambda x: x.get('similarity_score', x.get('final_score', 0)), reverse=True)
                logger.info(f"ğŸ¯ [Personalized] Sorted all recommendations by similarity_score for behavior vector user")
            else:
                # í–‰ë™ ë²¡í„°ê°€ ì—†ëŠ” ê²½ìš°: final_scoreë¡œ ì •ë ¬
                all_recommendations.sort(key=lambda x: x.get('final_score', 0), reverse=True)
                logger.info(f"ğŸ¯ [Personalized] Sorted all recommendations by final_score for preference-based user")

            # ì¤‘ë³µ ì œê±° (place_id ê¸°ì¤€)
            seen_places = set()
            final_recommendations = []
            for rec in all_recommendations:
                place_id = rec.get('place_id')
                if place_id not in seen_places:
                    seen_places.add(place_id)
                    final_recommendations.append(rec)

                if len(final_recommendations) >= limit:
                    break

            # ë””ë²„ê¹…: ìµœì¢… ê²°ê³¼ì˜ ì§€ì—­ë³„ ë¶„í¬ í™•ì¸
            final_region_distribution = {}
            for rec in final_recommendations:
                region = rec.get('source_region', 'unknown')
                final_region_distribution[region] = final_region_distribution.get(region, 0) + 1

            logger.info(f"ğŸ [DEBUG] Final regional distribution: {final_region_distribution}")
            logger.info(f"ğŸ“‹ [DEBUG] Sample recommendations (first 3): {[{'name': r.get('name'), 'region': r.get('source_region'), 'score': r.get('similarity_score', r.get('final_score', 0))} for r in final_recommendations[:3]]}")

            logger.info(f"âœ… Generated {len(final_recommendations)} user personalized recommendations for user {user_id}")
            logger.info(f"ğŸ”¥ [DEBUG] Returning {min(len(final_recommendations), limit)} recommendations")
            return final_recommendations[:limit]

        except Exception as e:
            logger.error(f"âŒ User personalized recommendation failed for user {user_id}: {e}")
            logger.error(f"ğŸ”¥ [DEBUG] Exception traceback: ", exc_info=True)
            return []

    async def _calculate_regional_recommendation_scores(
        self,
        user_preferences: Dict[str, Any],
        user_behavior_vector: Optional[np.ndarray] = None
    ) -> Dict[str, float]:
        """
        ê° ì§€ì—­ë³„ ì‚¬ìš©ì ì„ í˜¸ íƒœê·¸ + í–‰ë™ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ ìˆ˜ëŸ‰ ì ìˆ˜ ê³„ì‚°
        """
        try:
            regional_scores = {}

            # ì§€ì—­ë³„ ì¥ì†Œ ë°ì´í„°ì™€ ì‚¬ìš©ì ì„ í˜¸ë„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            regions_query = """
                SELECT DISTINCT region, COUNT(*) as place_count
                FROM place_recommendations
                WHERE region IS NOT NULL
                GROUP BY region
                ORDER BY place_count DESC
            """

            async with self.db_manager.pool.acquire() as conn:
                regions_data = await conn.fetch(regions_query)

                for region_row in regions_data:
                    region = region_row['region']
                    place_count = region_row['place_count']

                    # í•´ë‹¹ ì§€ì—­ì˜ ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                    region_score = await self._calculate_region_preference_score(
                        user_preferences, region, conn
                    )

                    # í–‰ë™ ë²¡í„° ê¸°ë°˜ ì§€ì—­ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ fallback)
                    behavior_score = 0.0
                    if user_behavior_vector is not None:
                        try:
                            behavior_score = await self._calculate_region_behavior_score(
                                user_behavior_vector, region, conn
                            )
                            logger.info(f"ğŸ§  [Regional] Region {region} behavior score: {behavior_score:.4f}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ Behavior score calculation failed for {region}: {e}")
                            behavior_score = 0.0

                    # íƒœê·¸ ì„ í˜¸ë„ + í–‰ë™ ë²¡í„° + ì¥ì†Œ ìˆ˜ë¥¼ ê²°í•©í•œ ìµœì¢… ì ìˆ˜
                    combined_score = (region_score * 0.6) + (behavior_score * 0.4)  # íƒœê·¸:í–‰ë™ = 6:4 ë¹„ìœ¨
                    final_score = combined_score * (1 + place_count / 1000)  # ì¥ì†Œ ìˆ˜ ê°€ì¤‘ì¹˜ ì ìš©
                    regional_scores[region] = final_score

            return regional_scores

        except Exception as e:
            logger.error(f"âŒ Regional scores calculation failed: {e}")
            return {}

    async def _calculate_region_preference_score(
        self,
        user_preferences: Dict[str, Any],
        region: str,
        conn
    ) -> float:
        """
        íŠ¹ì • ì§€ì—­ì— ëŒ€í•œ ì‚¬ìš©ì ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        ì˜¤ì§ ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ì˜ ì¥ì†Œ ìˆ˜ë§Œ ê³„ì‚°
        """
        try:
            preference_score = 0.0
            user_priority = user_preferences.get('priority')

            # ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ì˜ ì¥ì†Œ ìˆ˜ë§Œ ê³„ì‚°
            if user_priority:
                # experience íƒœê·¸ì¸ ê²½ìš° nature, humanities, leisure_sports í¬í•¨
                if user_priority == 'experience':
                    experience_categories = ['nature', 'humanities', 'leisure_sports']
                    priority_query = """
                        SELECT COUNT(*) as count
                        FROM place_recommendations
                        WHERE region = $1 AND table_name = ANY($2)
                    """
                    priority_result = await conn.fetchrow(priority_query, region, experience_categories)
                else:
                    # ì¼ë°˜ ì¹´í…Œê³ ë¦¬ (accommodation, restaurants, shopping)
                    priority_query = """
                        SELECT COUNT(*) as count
                        FROM place_recommendations
                        WHERE region = $1 AND table_name = $2
                    """
                    priority_result = await conn.fetchrow(priority_query, region, user_priority)

                if priority_result:
                    preference_score = float(priority_result['count'])

            return preference_score

        except Exception as e:
            logger.error(f"âŒ Region preference score calculation failed for {region}: {e}")
            return 0.0

    async def _calculate_region_behavior_score(
        self,
        user_behavior_vector: np.ndarray,
        region: str,
        conn
    ) -> float:
        """
        íŠ¹ì • ì§€ì—­ì— ëŒ€í•œ ì‚¬ìš©ì í–‰ë™ ë²¡í„° ê¸°ë°˜ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        """
        try:
            # í•´ë‹¹ ì§€ì—­ì˜ ì¥ì†Œë“¤ì˜ ë²¡í„°ë¥¼ ê°€ì ¸ì™€ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            region_vectors_query = """
                SELECT pr.place_id, l.embedding_vector
                FROM place_recommendations pr
                JOIN locations l ON pr.place_id = l.id
                WHERE pr.region = $1
                AND l.embedding_vector IS NOT NULL
                AND array_length(l.embedding_vector, 1) > 0
                LIMIT 100
            """

            region_places = await conn.fetch(region_vectors_query, region)

            if not region_places:
                return 0.0

            # ë²¡í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ìˆ˜ì§‘
            valid_vectors = []
            for place in region_places:
                vector = validate_vector_data(place['embedding_vector'])
                if vector is not None:
                    valid_vectors.append(vector)

            if not valid_vectors:
                return 0.0

            # ì§€ì—­ ì¥ì†Œë“¤ì˜ í‰ê·  ë²¡í„° ê³„ì‚°
            region_vectors_array = np.array(valid_vectors, dtype=np.float32)
            region_avg_vector = np.mean(region_vectors_array, axis=0)

            # ì‚¬ìš©ì í–‰ë™ ë²¡í„°ì™€ ì§€ì—­ í‰ê·  ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ANN ì§€ì›)
            similarities = safe_cosine_similarity(
                user_behavior_vector,
                region_avg_vector.reshape(1, -1),
                use_ann=self.ann_enabled,
                faiss_manager=self.faiss_manager
            )

            # í†µê³„ ì—…ë°ì´íŠ¸
            if self.ann_enabled:
                self.stats['ann_searches'] += 1
            else:
                self.stats['cosine_searches'] += 1

            similarity_score = float(similarities[0]) if len(similarities) > 0 else 0.0

            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ê³  ê°€ì¤‘ì¹˜ ì ìš©
            normalized_score = max(0.0, similarity_score)

            logger.info(f"ğŸ§  [Regional] Region {region}: {len(valid_vectors)} places, similarity={similarity_score:.4f}")

            return normalized_score

        except Exception as e:
            logger.error(f"âŒ Region behavior score calculation failed for {region}: {e}")
            return 0.0

    async def _get_similar_places_in_region(
        self,
        user_behavior_vector: np.ndarray,
        region: str,
        limit: int = 10,
        category_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì—ì„œ ì‚¬ìš©ì í–‰ë™ ë²¡í„°ì™€ ìœ ì‚¬í•œ ì¥ì†Œë“¤ì„ ANNìœ¼ë¡œ ì°¾ê¸°
        """
        try:
            logger.info(f"ğŸ” [Similar] Finding similar places in region {region} using ANN")

            # ANNì´ í™œì„±í™”ëœ ê²½ìš°, ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            if self.ann_enabled and self.faiss_manager.index is not None:
                # ANNìœ¼ë¡œ ìœ ì‚¬í•œ ì¥ì†Œ IDë“¤ ì°¾ê¸°
                similar_place_ids, scores = self.faiss_manager.search(user_behavior_vector, k=limit * 3)

                if not similar_place_ids:
                    return []

                # í•´ë‹¹ ì§€ì—­ì˜ ì¥ì†Œë“¤ë§Œ í•„í„°ë§
                if category_filter:
                    region_query = """
                        SELECT
                            pr.place_id, pr.table_name, pr.region, pr.name,
                            pr.latitude, pr.longitude, pr.overview, pr.image_urls,
                            pr.bookmark_cnt, pr.vector as text_vector,
                            pr.vector as embedding_vector
                        FROM place_recommendations pr
                        WHERE pr.region = $1
                        AND pr.place_id = ANY($2)
                        AND pr.table_name = $3
                        AND pr.name IS NOT NULL
                        ORDER BY pr.bookmark_cnt DESC
                    """
                else:
                    region_query = """
                        SELECT
                            pr.place_id, pr.table_name, pr.region, pr.name,
                            pr.latitude, pr.longitude, pr.overview, pr.image_urls,
                            pr.bookmark_cnt, pr.vector as text_vector,
                            pr.vector as embedding_vector
                        FROM place_recommendations pr
                        WHERE pr.region = $1
                        AND pr.place_id = ANY($2)
                        AND pr.name IS NOT NULL
                        ORDER BY pr.bookmark_cnt DESC
                    """

                async with self.db_manager.get_connection() as conn:
                    if category_filter:
                        places_data = await conn.fetch(region_query, region, similar_place_ids, category_filter)
                    else:
                        places_data = await conn.fetch(region_query, region, similar_place_ids)

                # ì‹¤ì œ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
                scored_places = []
                place_id_to_score = {pid: score for pid, score in zip(similar_place_ids, scores)}

                for place in places_data:
                    place_dict = dict(place)
                    ann_score = place_id_to_score.get(place['place_id'], 0.0)

                    # ìœ ì‚¬ë„ ì ìˆ˜ì™€ ì¸ê¸°ë„ë¥¼ ê²°í•©í•œ ìµœì¢… ì ìˆ˜
                    popularity_score = min(place['bookmark_cnt'] / 100.0, 1.0) if place['bookmark_cnt'] else 0.0
                    final_score = (ann_score * 0.8) + (popularity_score * 0.2)

                    place_dict['similarity_score'] = ann_score
                    place_dict['final_score'] = final_score
                    place_dict['recommendation_type'] = 'similar_places_ann'
                    place_dict['source'] = 'behavior_based'

                    scored_places.append(place_dict)

                # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
                scored_places.sort(key=lambda x: x['final_score'], reverse=True)

                self.stats['ann_searches'] += 1
                logger.info(f"âœ… [Similar] Found {len(scored_places)} similar places in {region} using ANN")

                return scored_places[:limit]

            else:
                # ANNì´ ë¹„í™œì„±í™”ëœ ê²½ìš°, ì „í†µì ì¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
                return await self._get_similar_places_fallback(user_behavior_vector, region, limit, category_filter)

        except Exception as e:
            logger.error(f"âŒ Similar places search failed for region {region}: {e}")
            return []

    async def _get_similar_places_fallback(
        self,
        user_behavior_vector: np.ndarray,
        region: str,
        limit: int = 10,
        category_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        ì „í†µì ì¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬í•œ ì¥ì†Œ ê²€ìƒ‰ (Fallback)
        """
        try:
            # í•´ë‹¹ ì§€ì—­ì˜ ëª¨ë“  ì¥ì†Œ ë²¡í„° ì¡°íšŒ
            if category_filter:
                region_query = """
                    SELECT
                        pr.place_id, pr.table_name, pr.region, pr.name,
                        pr.latitude, pr.longitude, pr.overview, pr.image_urls,
                        pr.bookmark_cnt, pr.vector as text_vector,
                        pr.vector as embedding_vector
                    FROM place_recommendations pr
                    WHERE pr.region = $1
                    AND pr.table_name = $2
                    AND pr.vector IS NOT NULL
                    AND pr.name IS NOT NULL
                    LIMIT 200
                """
            else:
                region_query = """
                    SELECT
                        pr.place_id, pr.table_name, pr.region, pr.name,
                        pr.latitude, pr.longitude, pr.overview, pr.image_urls,
                        pr.bookmark_cnt, pr.vector as text_vector,
                        pr.vector as embedding_vector
                    FROM place_recommendations pr
                    WHERE pr.region = $1
                    AND pr.vector IS NOT NULL
                    AND pr.name IS NOT NULL
                    LIMIT 200
                """

            async with self.db_manager.get_connection() as conn:
                if category_filter:
                    places_data = await conn.fetch(region_query, region, category_filter)
                else:
                    places_data = await conn.fetch(region_query, region)

            if not places_data:
                return []

            # ë²¡í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ìœ ì‚¬ë„ ê³„ì‚°
            valid_places = []
            place_vectors = []

            for place in places_data:
                vector = validate_vector_data(place['embedding_vector'])
                if vector is not None:
                    valid_places.append(dict(place))
                    place_vectors.append(vector)

            if not valid_places:
                return []

            # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
            place_vectors_array = np.array(place_vectors, dtype=np.float32)
            similarities = safe_cosine_similarity(user_behavior_vector, place_vectors_array, use_ann=False)

            # ì ìˆ˜í™” ë° ì •ë ¬
            scored_places = []
            for i, place in enumerate(valid_places):
                similarity_score = float(similarities[i])
                popularity_score = min(place['bookmark_cnt'] / 100.0, 1.0) if place['bookmark_cnt'] else 0.0
                final_score = (similarity_score * 0.8) + (popularity_score * 0.2)

                place['similarity_score'] = similarity_score
                place['final_score'] = final_score
                place['recommendation_type'] = 'similar_places_cosine'
                place['source'] = 'behavior_based'

                scored_places.append(place)

            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
            scored_places.sort(key=lambda x: x['final_score'], reverse=True)

            self.stats['cosine_searches'] += 1
            logger.info(f"âœ… [Similar] Found {len(scored_places)} similar places in {region} using cosine similarity")

            return scored_places[:limit]

        except Exception as e:
            logger.error(f"âŒ Similar places fallback search failed for region {region}: {e}")
            return []

    def _merge_recommendations(
        self,
        priority_recommendations: List[Dict],
        similar_recommendations: List[Dict],
        target_limit: int
    ) -> List[Dict]:
        """
        ìš°ì„ ìˆœìœ„ ì¶”ì²œê³¼ ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œì„ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        """
        try:
            # place_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            seen_place_ids = set()
            merged_recommendations = []

            # 1. ìš°ì„ ìˆœìœ„ ì¶”ì²œì„ ë¨¼ì € ì¶”ê°€
            for rec in priority_recommendations:
                place_id = rec.get('place_id')
                if place_id and place_id not in seen_place_ids:
                    rec['merge_source'] = 'priority'
                    merged_recommendations.append(rec)
                    seen_place_ids.add(place_id)

            # 2. ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œ ì¶”ê°€ (ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
            for rec in similar_recommendations:
                place_id = rec.get('place_id')
                if place_id and place_id not in seen_place_ids:
                    rec['merge_source'] = 'similar'
                    merged_recommendations.append(rec)
                    seen_place_ids.add(place_id)

            # 3. ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ìë¥´ê¸°
            final_recommendations = merged_recommendations[:target_limit]

            logger.info(f"ğŸ”„ [Merge] Combined {len(priority_recommendations)} priority + {len(similar_recommendations)} similar â†’ {len(final_recommendations)} final")

            return final_recommendations

        except Exception as e:
            logger.error(f"âŒ Recommendation merge failed: {e}")
            return priority_recommendations[:target_limit]  # Fallback to priority only

    async def _get_diverse_category_recommendations(
        self,
        user_behavior_vector: np.ndarray,
        region: str,
        limit: int,
        exclude_priority: Optional[str] = None
    ) -> List[Dict]:
        """
        í–‰ë™ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¶”ì²œ ìƒì„±
        ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë“¤ì—ì„œ ì¶”ì²œ
        """
        try:
            logger.info(f"ğŸŒˆ [Diverse] STARTED - Getting diverse category recommendations for region {region}, limit={limit}, excluding {exclude_priority}")
            logger.info(f"ğŸŒˆ [Diverse] Input validation - user_behavior_vector: {user_behavior_vector is not None}, region: {region}, limit: {limit}")

            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ì œì™¸)
            all_categories = ['nature', 'humanities', 'leisure_sports', 'accommodation', 'dining', 'shopping']

            # experienceëŠ” íŠ¹ë³„ ì²˜ë¦¬ - nature, humanities, leisure_sports í¬í•¨í•˜ë¯€ë¡œ ì´ë“¤ì€ ì œì™¸
            if exclude_priority == 'experience':
                diverse_categories = ['accommodation', 'dining', 'shopping']
            elif exclude_priority in all_categories:
                diverse_categories = [cat for cat in all_categories if cat != exclude_priority]
            else:
                diverse_categories = all_categories

            logger.info(f"ğŸ¯ [Diverse] Target categories: {diverse_categories}")

            diverse_recommendations = []
            items_per_category = max(1, limit // len(diverse_categories)) if diverse_categories else 0

            for category in diverse_categories:
                try:
                    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ í–‰ë™ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ ìƒì„±
                    category_places = await self._get_category_places_with_vectors(region, category)

                    if not category_places:
                        logger.info(f"âš ï¸ [Diverse] No places found for category {category} in region {region}")
                        continue

                    # í–‰ë™ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                    category_recommendations = []
                    for place in category_places:
                        try:
                            place_vector = validate_vector_data(place.get('text_vector'))
                            if place_vector is not None:
                                similarity = safe_cosine_similarity(
                                    user_behavior_vector.reshape(1, -1),
                                    place_vector.reshape(1, -1)
                                )[0]

                                place_dict = dict(place)
                                place_dict['similarity_score'] = float(similarity)
                                place_dict['recommendation_category'] = category
                                place_dict['recommendation_source'] = 'diverse_category'
                                category_recommendations.append(place_dict)
                        except Exception as place_e:
                            logger.warning(f"âš ï¸ [Diverse] Failed to process place {place.get('place_id', 'unknown')}: {place_e}")
                            continue

                    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                    category_recommendations.sort(key=lambda x: x['similarity_score'], reverse=True)
                    selected_recommendations = category_recommendations[:items_per_category]

                    if selected_recommendations:
                        logger.info(f"âœ… [Diverse] Added {len(selected_recommendations)} recommendations from {category}")
                        diverse_recommendations.extend(selected_recommendations)

                except Exception as cat_e:
                    logger.warning(f"âš ï¸ [Diverse] Failed to get recommendations for category {category}: {cat_e}")
                    continue

            logger.info(f"ğŸŒˆ [Diverse] Generated {len(diverse_recommendations)} diverse category recommendations")
            return diverse_recommendations[:limit]

        except Exception as e:
            logger.error(f"âŒ [Diverse] Diverse category recommendations failed: {e}")
            return []

    async def _get_category_places_with_vectors(self, region: str, category: str) -> List[Dict]:
        """íŠ¹ì • ì§€ì—­ê³¼ ì¹´í…Œê³ ë¦¬ì˜ ì¥ì†Œë“¤ê³¼ ë²¡í„° ë°ì´í„° ì¡°íšŒ"""
        try:
            places_query = """
                SELECT
                    place_id, table_name, region, name,
                    latitude, longitude, overview, image_urls, bookmark_cnt,
                    vector as text_vector
                FROM place_recommendations
                WHERE name IS NOT NULL
                    AND region = $1
                    AND table_name = $2
                    AND vector IS NOT NULL
                ORDER BY bookmark_cnt DESC
                LIMIT 100
            """

            async with self.db_manager.get_connection() as conn:
                places_data = await conn.fetch(places_query, region, category)
                return [dict(row) for row in places_data]

        except Exception as e:
            logger.error(f"âŒ Failed to get places for region {region}, category {category}: {e}")
            return []

    def _merge_diverse_recommendations(
        self,
        priority_recommendations: List[Dict],
        diverse_recommendations: List[Dict],
        similar_recommendations: List[Dict],
        target_limit: int
    ) -> List[Dict]:
        """
        ìš°ì„ ìˆœìœ„, ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬, ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œì„ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        """
        try:
            seen_place_ids = set()
            merged_recommendations = []

            # 1. ìš°ì„ ìˆœìœ„ ì¶”ì²œì„ ë¨¼ì € ì¶”ê°€
            for rec in priority_recommendations:
                place_id = rec.get('place_id')
                if place_id and place_id not in seen_place_ids:
                    rec['merge_source'] = 'priority'
                    merged_recommendations.append(rec)
                    seen_place_ids.add(place_id)

            # 2. ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì¶”ê°€
            for rec in diverse_recommendations:
                place_id = rec.get('place_id')
                if place_id and place_id not in seen_place_ids:
                    rec['merge_source'] = 'diverse_category'
                    merged_recommendations.append(rec)
                    seen_place_ids.add(place_id)

            # 3. ìœ ì‚¬í•œ ì¥ì†Œ ì¶”ì²œ ì¶”ê°€ (ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
            for rec in similar_recommendations:
                place_id = rec.get('place_id')
                if place_id and place_id not in seen_place_ids:
                    rec['merge_source'] = 'similar'
                    merged_recommendations.append(rec)
                    seen_place_ids.add(place_id)

            # 4. ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ìë¥´ê¸°
            final_recommendations = merged_recommendations[:target_limit]

            logger.info(f"ğŸ”„ [Diverse Merge] Combined {len(priority_recommendations)} priority + {len(diverse_recommendations)} diverse + {len(similar_recommendations)} similar â†’ {len(final_recommendations)} final")

            return final_recommendations

        except Exception as e:
            logger.error(f"âŒ Diverse recommendation merge failed: {e}")
            return priority_recommendations[:target_limit]  # Fallback to priority only

    async def _get_priority_ordered_recommendations(
        self,
        user_preferences: Dict[str, Any],
        region: str,
        user_priority: str,
        limit: int
    ) -> List[Dict]:
        """
        ì§€ì—­ ë‚´ì—ì„œ ì‚¬ìš©ì ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì²œ ìƒì„±
        """
        try:
            recommendations = []

            # experience íƒœê·¸ì¸ ê²½ìš° nature, humanities, leisure_sportsë§Œ ì¶”ì²œ
            if user_priority == 'experience':
                experience_categories = ['nature', 'humanities', 'leisure_sports']
                for category in experience_categories:
                    category_recommendations = await self._calculate_preference_scores(
                        user_preferences, region, category, limit // len(experience_categories) + 1
                    )

                    if category_recommendations:
                        for rec in category_recommendations:
                            rec['category_priority'] = 'high'
                            rec['recommendation_reason'] = f'ì²´í—˜ ìš°ì„ ìˆœìœ„: {category}'
                        recommendations.extend(category_recommendations)
            else:
                # ì¼ë°˜ ì¹´í…Œê³ ë¦¬ëŠ” í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì²œ
                priority_recommendations = await self._calculate_preference_scores(
                    user_preferences, region, user_priority, limit
                )

                if priority_recommendations:
                    for rec in priority_recommendations:
                        rec['category_priority'] = 'high'
                        rec['recommendation_reason'] = f'ì‚¬ìš©ì ìš°ì„ ìˆœìœ„: {user_priority}'
                    recommendations.extend(priority_recommendations)

            logger.info(f"ğŸ¯ Generated {len(recommendations)} priority-only recommendations for {region} ({user_priority})")
            return recommendations[:limit]

        except Exception as e:
            logger.error(f"âŒ Priority ordered recommendations failed for {region}: {e}")
            return []

    async def _get_simple_regional_recommendations(
        self,
        region: str,
        limit: int
    ) -> List[Dict]:
        """
        ê°„ë‹¨í•œ ì§€ì—­ ê¸°ë°˜ ì¶”ì²œ (í´ë°±ìš©)
        """
        try:
            query = """
                SELECT
                    pr.place_id, pr.table_name, pr.region, pr.name,
                    pr.latitude, pr.longitude, pr.overview, pr.image_urls,
                    pr.bookmark_cnt, pr.vector as text_vector,
                    pr.vector as embedding_vector
                FROM place_recommendations pr
                WHERE pr.region = $1
                AND pr.name IS NOT NULL
                ORDER BY pr.bookmark_cnt DESC
                LIMIT $2
            """

            async with self.db_manager.get_connection() as conn:
                places_data = await conn.fetch(query, region, limit)

            recommendations = []
            for place in places_data:
                place_dict = dict(place)
                place_dict['recommendation_type'] = 'simple_regional'
                place_dict['source'] = 'regional_fallback'
                recommendations.append(place_dict)

            logger.info(f"ğŸ“ Generated {len(recommendations)} simple regional recommendations for {region}")
            return recommendations

        except Exception as e:
            logger.error(f"âŒ Simple regional recommendations failed for {region}: {e}")
            return []


# ============================================================================
# ğŸš€ ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
# ============================================================================

_engine_instance: Optional[UnifiedRecommendationEngine] = None

async def get_engine() -> UnifiedRecommendationEngine:
    """ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ì´ˆê¸°í™”)"""
    global _engine_instance

    if _engine_instance is None:
        try:
            _engine_instance = UnifiedRecommendationEngine()
            await _engine_instance.initialize()
            logger.info("âœ… Global recommendation engine initialized successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize recommendation engine: {e}")
            # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì—”ì§„ì€ ë°˜í™˜ (ë¶€ë¶„ ê¸°ëŠ¥ ì‚¬ìš©)
            if _engine_instance is None:
                _engine_instance = UnifiedRecommendationEngine()
                # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ DB ì—°ê²°ë§Œì´ë¼ë„ ì‹œë„
                try:
                    await _engine_instance.db_manager.initialize()
                except Exception as db_e:
                    logger.error(f"âŒ DB connection also failed: {db_e}")

    return _engine_instance

async def close_engine():
    """ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬"""
    global _engine_instance

    if _engine_instance:
        await _engine_instance.close()
        _engine_instance = None


