"""
S3ì—ì„œ PostgreSQLë¡œ ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
EC2 Collection Serverê°€ S3ì— ì €ì¥í•œ JSON ë°ì´í„°ë¥¼ PostgreSQLë¡œ ì´ê´€
"""

import asyncio
import asyncpg
import boto3
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
import os
import uuid

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class S3ToDBPipeline:
    def __init__(self):
        # AWS S3 ì„¤ì •
        self.s3_client = boto3.client(
            's3',
            region_name='ap-northeast-2',
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
        )
        
        # S3 ë²„í‚· ì„¤ì • (EC2 Collection Serverê°€ ì‚¬ìš©í•˜ëŠ” ë²„í‚·)
        self.source_bucket = 'user-actions-data'
        
        # PostgreSQL ì—°ê²° ì„¤ì •
        self.database_url = os.getenv('DATABASE_URL')
        
    async def get_db_connection(self):
        """PostgreSQL ì—°ê²°"""
        return await asyncpg.connect(self.database_url)
        
    def list_s3_files(self, hours_back: int = 1) -> List[Dict]:
        """ì§€ì •ëœ ì‹œê°„ ì´í›„ì˜ S3 íŒŒì¼ë“¤ì„ ì¡°íšŒ"""
        try:
            # í˜„ì¬ ì‹œê°„ì—ì„œ hours_back ì‹œê°„ ì „ê¹Œì§€ì˜ íŒŒì¼ë“¤ì„ ê°€ì ¸ì˜´
            now = datetime.now()
            start_time = now - timedelta(hours=hours_back)
            
            # S3 ê°ì²´ ëª©ë¡ ì¡°íšŒ
            paginator = self.s3_client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(
                Bucket=self.source_bucket,
                Prefix='user-actions/'
            )
            
            files = []
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ì´ ì§€ì •ëœ ì‹œê°„ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
                        if obj['LastModified'].replace(tzinfo=None) >= start_time:
                            files.append({
                                'key': obj['Key'],
                                'size': obj['Size'],
                                'last_modified': obj['LastModified']
                            })
            
            logger.info(f"Found {len(files)} files in S3 from last {hours_back} hours")
            return files
            
        except Exception as e:
            logger.error(f"Error listing S3 files: {e}")
            return []
    
    def download_s3_file(self, key: str) -> Dict[str, Any]:
        """S3ì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  íŒŒì‹±"""
        try:
            response = self.s3_client.get_object(Bucket=self.source_bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            
            # JSON íŒŒì‹±
            data = json.loads(content)
            logger.debug(f"Downloaded and parsed: {key}")
            return data
            
        except Exception as e:
            logger.error(f"Error downloading {key}: {e}")
            return None
    
    async def insert_action_to_db(self, conn: asyncpg.Connection, action_data: Dict[str, Any]) -> bool:
        """ê°œë³„ ì•¡ì…˜ì„ PostgreSQLì— ì‚½ì…"""
        try:
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['user_id', 'place_category', 'place_id', 'action_type']
            for field in required_fields:
                if field not in action_data:
                    logger.warning(f"Missing required field {field} in action data")
                    return False
            
            # UUID ìƒì„± (S3ì—ì„œ collection_idê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
            action_id = action_data.get('collection_id', str(uuid.uuid4()))
            
            # PostgreSQLì— ì‚½ì…
            await conn.execute("""
                INSERT INTO user_action_logs 
                (id, user_id, place_category, place_id, action_type, action_value, action_detail, created_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT (id) DO NOTHING
            """,
            action_id,
            action_data['user_id'],
            action_data['place_category'],
            action_data['place_id'],
            action_data['action_type'],
            action_data.get('action_value'),
            action_data.get('action_detail'),
            # timestampê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„
            datetime.fromisoformat(action_data.get('timestamp', datetime.utcnow().isoformat()).replace('Z', '+00:00'))
            )
            
            return True
            
        except Exception as e:
            logger.error(f"Error inserting action to DB: {e}")
            logger.error(f"Action data: {action_data}")
            return False
    
    async def process_batch(self, hours_back: int = 1) -> Dict[str, int]:
        """S3ì—ì„œ PostgreSQLë¡œ ë°°ì¹˜ ì²˜ë¦¬"""
        stats = {
            'files_processed': 0,
            'actions_processed': 0,
            'actions_inserted': 0,
            'errors': 0
        }
        
        try:
            # S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            files = self.list_s3_files(hours_back)
            if not files:
                logger.info("No files to process")
                return stats
            
            # PostgreSQL ì—°ê²°
            conn = await self.get_db_connection()
            
            try:
                for file_info in files:
                    try:
                        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        action_data = self.download_s3_file(file_info['key'])
                        if not action_data:
                            stats['errors'] += 1
                            continue
                        
                        # ë‹¨ì¼ ì•¡ì…˜ ì²˜ë¦¬
                        stats['actions_processed'] += 1
                        success = await self.insert_action_to_db(conn, action_data)
                        
                        if success:
                            stats['actions_inserted'] += 1
                        else:
                            stats['errors'] += 1
                        
                        stats['files_processed'] += 1
                        
                    except Exception as e:
                        logger.error(f"Error processing file {file_info['key']}: {e}")
                        stats['errors'] += 1
                        
            finally:
                await conn.close()
                
        except Exception as e:
            logger.error(f"Error in batch processing: {e}")
            stats['errors'] += 1
        
        logger.info(f"Batch processing completed: {stats}")
        return stats
    
    async def process_and_calculate_recommendations(self, hours_back: int = 1):
        """ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰"""
        
        # 1. S3 â†’ PostgreSQL ë°°ì¹˜ ì²˜ë¦¬
        logger.info("ğŸ”„ Starting S3 to PostgreSQL batch processing...")
        batch_stats = await self.process_batch(hours_back)
        
        if batch_stats['actions_inserted'] == 0:
            logger.info("No new actions to process recommendations")
            return
        
        # 2. ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ê³„ì‚°
        logger.info("ğŸ§® Starting recommendation algorithm calculation...")
        await self.calculate_user_recommendations()
        
        logger.info("âœ… Pipeline completed successfully")
    
    async def get_dynamic_weight(self, conn: asyncpg.Connection, action_type: str, action_value: float) -> float:
        """action_valueì— ë”°ë¼ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            if action_type == 'dwell_time':
                if action_value < 5:
                    value_range = '0-5s'
                elif action_value <= 20:
                    value_range = '5-20s'
                else:
                    value_range = '20s+'
            elif action_type == 'scroll_depth':
                if action_value < 30:
                    value_range = '0-30%'
                elif action_value <= 70:
                    value_range = '30-70%'
                else:
                    value_range = '70-100%'
            else:
                # click, search, like, bookmarkëŠ” value_rangeê°€ NULL
                value_range = None
            
            # DBì—ì„œ ê°€ì¤‘ì¹˜ ì¡°íšŒ
            weight_query = """
                SELECT weight FROM action_weights 
                WHERE action_type = $1 AND (value_range = $2 OR (value_range IS NULL AND $2 IS NULL))
            """
            
            result = await conn.fetchval(weight_query, action_type, value_range)
            return float(result) if result else 1.0
            
        except Exception as e:
            logger.error(f"Error getting dynamic weight: {e}")
            return 1.0

    async def calculate_user_recommendations(self):
        """ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •êµí•œ ì¶”ì²œ ì ìˆ˜ ê³„ì‚°"""
        try:
            conn = await self.get_db_connection()
            
            try:
                # ìµœê·¼ 7ì¼ê°„ì˜ ì‚¬ìš©ì í–‰ë™ ë¶„ì„ (ê°œë³„ action_value í¬í•¨)
                analysis_query = """
                SELECT 
                    user_id,
                    place_category,
                    action_type,
                    action_value,
                    COUNT(*) as action_count
                FROM user_action_logs 
                WHERE created_at >= NOW() - INTERVAL '7 days'
                GROUP BY user_id, place_category, action_type, action_value
                ORDER BY user_id, place_category, action_type
                """
                
                results = await conn.fetch(analysis_query)
                
                # ì‚¬ìš©ìë³„ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚°
                user_preferences = {}
                
                for row in results:
                    user_id = row['user_id']
                    category = row['place_category']
                    action_type = row['action_type']
                    action_value = float(row['action_value']) if row['action_value'] else 1.0
                    count = row['action_count']
                    
                    if user_id not in user_preferences:
                        user_preferences[user_id] = {}
                    
                    if category not in user_preferences[user_id]:
                        user_preferences[user_id][category] = 0
                    
                    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
                    weight = await self.get_dynamic_weight(conn, action_type, action_value)
                    
                    # ì ìˆ˜ ê³„ì‚°: í–‰ë™ íšŸìˆ˜ Ã— ë™ì  ê°€ì¤‘ì¹˜
                    score = count * weight
                    
                    user_preferences[user_id][category] += score
                    
                    logger.debug(f"User {user_id[:8]}, {category}, {action_type}, value={action_value}, weight={weight}, score={score}")
                
                logger.info(f"Calculated preferences for {len(user_preferences)} users")
                
                # ì‚¬ìš©ìë³„ ìƒìœ„ ì„ í˜¸ë„ ì¶œë ¥
                for user_id, preferences in user_preferences.items():
                    sorted_prefs = sorted(preferences.items(), key=lambda x: x[1], reverse=True)
                    logger.info(f"User {user_id[:8]}... preferences: {sorted_prefs[:3]}")
                
                # ì¶”ì²œ ê²°ê³¼ë¥¼ user_category_preferences í…Œì´ë¸”ì— ì €ì¥
                await self.save_user_preferences(conn, user_preferences)
                
            finally:
                await conn.close()
                
        except Exception as e:
            logger.error(f"Error calculating recommendations: {e}")
    
    async def save_user_preferences(self, conn: asyncpg.Connection, user_preferences: dict):
        """ê³„ì‚°ëœ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ DBì— ì €ì¥"""
        try:
            # user_category_preferences í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS user_category_preferences (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    user_id VARCHAR NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
                    place_category VARCHAR(50) NOT NULL,
                    preference_score NUMERIC(10,2) NOT NULL DEFAULT 0,
                    last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    UNIQUE(user_id, place_category)
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_user_category_preferences_user_score 
                ON user_category_preferences(user_id, preference_score DESC)
            """)
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…
            for user_id, preferences in user_preferences.items():
                # í•´ë‹¹ ì‚¬ìš©ìì˜ ê¸°ì¡´ ì„ í˜¸ë„ ì‚­ì œ
                await conn.execute("DELETE FROM user_category_preferences WHERE user_id = $1", user_id)
                
                # ìƒˆ ì„ í˜¸ë„ ì‚½ì…
                for category, score in preferences.items():
                    await conn.execute("""
                        INSERT INTO user_category_preferences 
                        (user_id, place_category, preference_score, last_updated) 
                        VALUES ($1, $2, $3, NOW())
                    """, user_id, category, round(score, 2))
            
            logger.info(f"Saved preferences for {len(user_preferences)} users to database")
            
        except Exception as e:
            logger.error(f"Error saving user preferences: {e}")

# CLI ì¸í„°í˜ì´ìŠ¤
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    pipeline = S3ToDBPipeline()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        hours = int(sys.argv[2]) if len(sys.argv) > 2 else 1
        
        if command == 'batch':
            # ë°°ì¹˜ ì²˜ë¦¬ë§Œ ì‹¤í–‰
            await pipeline.process_batch(hours)
        elif command == 'recommend':
            # ì¶”ì²œ ê³„ì‚°ë§Œ ì‹¤í–‰  
            await pipeline.calculate_user_recommendations()
        elif command == 'full':
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            await pipeline.process_and_calculate_recommendations(hours)
        else:
            print("Usage: python s3_to_db_pipeline.py [batch|recommend|full] [hours_back]")
    else:
        # ê¸°ë³¸: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        await pipeline.process_and_calculate_recommendations()

if __name__ == "__main__":
    asyncio.run(main())