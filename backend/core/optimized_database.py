"""
ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ì‹œìŠ¤í…œ
"""
import asyncio
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass
from contextlib import asynccontextmanager
import time
import hashlib
from cachetools import TTLCache, LRUCache
from sqlalchemy import text, create_engine
from sqlalchemy.pool import QueuePool
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from langchain_core.documents import Document
import json


@dataclass
class QueryMetrics:
    """ì¿¼ë¦¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    query_count: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_response_time: float = 0.0
    total_response_time: float = 0.0
    slow_queries: List[Dict] = None

    def __post_init__(self):
        if self.slow_queries is None:
            self.slow_queries = []


class OptimizedDatabaseManager:
    """ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ìž"""

    def __init__(self, database_url: str, **kwargs):
        # ì—°ê²° í’€ ì„¤ì •
        pool_config = {
            'poolclass': QueuePool,
            'pool_size': kwargs.get('pool_size', 20),
            'max_overflow': kwargs.get('max_overflow', 30),
            'pool_pre_ping': True,
            'pool_recycle': kwargs.get('pool_recycle', 3600),  # 1ì‹œê°„
            'pool_timeout': kwargs.get('pool_timeout', 30)
        }

        # ë™ê¸° ì—”ì§„ (ê¸°ì¡´ í˜¸í™˜ì„±)
        self.sync_engine = create_engine(database_url, **pool_config)

        # ë¹„ë™ê¸° ì—”ì§„ (ì„±ëŠ¥ ìµœì í™”ìš©)
        async_url = database_url.replace('postgresql://', 'postgresql+asyncpg://')
        self.async_engine = create_async_engine(async_url, **pool_config)

        # ìºì‹œ ì„¤ì •
        self.query_cache = TTLCache(
            maxsize=kwargs.get('cache_size', 1000),
            ttl=kwargs.get('cache_ttl', 3600)  # 1ì‹œê°„
        )

        self.metadata_cache = LRUCache(maxsize=500)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = QueryMetrics()

        # ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì¿¼ë¦¬ë“¤
        self._compiled_queries = {}
        self._prepare_compiled_queries()

        print("âœ… ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ìž ì´ˆê¸°í™” ì™„ë£Œ")

    def _prepare_compiled_queries(self):
        """ìžì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ ë¯¸ë¦¬ ì»´íŒŒì¼"""
        self._compiled_queries = {
            'regions_catalog': text("""
                SELECT DISTINCT cmetadata->>'region' as region
                FROM langchain_pg_embedding
                WHERE cmetadata->>'region' IS NOT NULL
                AND cmetadata->>'region' != ''
                ORDER BY region
            """),

            'cities_catalog': text("""
                SELECT DISTINCT cmetadata->>'city' as city
                FROM langchain_pg_embedding
                WHERE cmetadata->>'city' IS NOT NULL
                AND cmetadata->>'city' != ''
                ORDER BY city
            """),

            'categories_catalog': text("""
                SELECT DISTINCT cmetadata->>'category' as category
                FROM langchain_pg_embedding
                WHERE cmetadata->>'category' IS NOT NULL
                AND cmetadata->>'category' != ''
                ORDER BY category
            """),

            'collection_uuid': text("""
                SELECT uuid FROM langchain_pg_collection
                WHERE name = :collection_name
            """),

            'place_by_regions': text("""
                SELECT document, cmetadata, embedding
                FROM langchain_pg_embedding
                WHERE collection_id = :collection_id
                AND (cmetadata->>'region' ILIKE ANY(:regions))
                AND NOT (cmetadata->>'category' ILIKE ANY(:exclude_categories))
                ORDER BY cmetadata->>'similarity_score' DESC NULLS LAST
                LIMIT :limit_count
            """),

            'place_by_cities': text("""
                SELECT document, cmetadata, embedding
                FROM langchain_pg_embedding
                WHERE collection_id = :collection_id
                AND (
                    cmetadata->>'city' ILIKE ANY(:cities) OR
                    cmetadata->>'region' ILIKE ANY(:cities)
                )
                AND NOT (cmetadata->>'category' ILIKE ANY(:exclude_categories))
                ORDER BY cmetadata->>'similarity_score' DESC NULLS LAST
                LIMIT :limit_count
            """),

            'place_by_categories': text("""
                SELECT document, cmetadata, embedding
                FROM langchain_pg_embedding
                WHERE collection_id = :collection_id
                AND (cmetadata->>'category' ILIKE ANY(:categories))
                ORDER BY cmetadata->>'similarity_score' DESC NULLS LAST
                LIMIT :limit_count
            """),

            'place_random_sample': text("""
                SELECT document, cmetadata, embedding
                FROM langchain_pg_embedding
                WHERE collection_id = :collection_id
                AND NOT (cmetadata->>'category' ILIKE ANY(:exclude_categories))
                ORDER BY RANDOM()
                LIMIT :limit_count
            """)
        }

    def _generate_cache_key(self, query_type: str, **params) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # íŒŒë¼ë¯¸í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
        sorted_params = sorted(params.items())
        params_str = json.dumps(sorted_params, sort_keys=True)
        key_data = f"{query_type}:{params_str}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def get_db_catalogs_optimized(self) -> Dict[str, List[str]]:
        """ìµœì í™”ëœ DB ì¹´íƒˆë¡œê·¸ ë¡œë“œ"""
        cache_key = self._generate_cache_key("catalogs")

        # ìºì‹œì—ì„œ ì¡°íšŒ
        if cache_key in self.metadata_cache:
            print("ðŸŽ¯ ì¹´íƒˆë¡œê·¸ ìºì‹œ ížˆíŠ¸")
            return self.metadata_cache[cache_key]

        start_time = time.time()

        try:
            # ë¹„ë™ê¸°ë¡œ ëª¨ë“  ì¹´íƒˆë¡œê·¸ ë™ì‹œ ë¡œë“œ
            async with self.async_engine.connect() as conn:
                regions_task = conn.execute(self._compiled_queries['regions_catalog'])
                cities_task = conn.execute(self._compiled_queries['cities_catalog'])
                categories_task = conn.execute(self._compiled_queries['categories_catalog'])

                # ë™ì‹œ ì‹¤í–‰
                regions_result, cities_result, categories_result = await asyncio.gather(
                    regions_task, cities_task, categories_task
                )

                catalogs = {
                    "regions": [row.region for row in regions_result.fetchall() if row.region],
                    "cities": [row.city for row in cities_result.fetchall() if row.city],
                    "categories": [row.category for row in categories_result.fetchall() if row.category]
                }

            # ìºì‹œì— ì €ìž¥
            self.metadata_cache[cache_key] = catalogs

            query_time = time.time() - start_time
            print(f"âœ… DB ì¹´íƒˆë¡œê·¸ ë¡œë“œ ì™„ë£Œ ({query_time:.3f}ì´ˆ):")
            print(f"   - ì§€ì—­: {len(catalogs['regions'])}ê°œ")
            print(f"   - ë„ì‹œ: {len(catalogs['cities'])}ê°œ")
            print(f"   - ì¹´í…Œê³ ë¦¬: {len(catalogs['categories'])}ê°œ")

            return catalogs

        except Exception as e:
            print(f"âŒ DB ì¹´íƒˆë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"regions": [], "cities": [], "categories": []}

    async def search_places_optimized(
        self,
        query: str,
        regions: List[str] = None,
        cities: List[str] = None,
        categories: List[str] = None,
        limit: int = 1000,
        collection_name: str = 'place_recommendations'
    ) -> List[Document]:
        """ìµœì í™”ëœ ìž¥ì†Œ ê²€ìƒ‰"""
        start_time = time.time()
        self.metrics.query_count += 1

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(
            "search_places",
            query=query,
            regions=regions or [],
            cities=cities or [],
            categories=categories or [],
            limit=limit
        )

        # ìºì‹œ ì¡°íšŒ
        if cache_key in self.query_cache:
            self.metrics.cache_hits += 1
            print(f"ðŸŽ¯ ê²€ìƒ‰ ìºì‹œ ížˆíŠ¸: '{query}'")
            return self.query_cache[cache_key]

        self.metrics.cache_misses += 1

        try:
            # ì»¬ë ‰ì…˜ UUID ì¡°íšŒ (ìºì‹±ë¨)
            collection_id = await self._get_collection_id(collection_name)
            if not collection_id:
                return []

            # ì œì™¸í•  ì¹´í…Œê³ ë¦¬ (ìˆ™ì†Œ)
            exclude_categories = [
                '%ìˆ™ì†Œ%', '%í˜¸í…”%', '%íŽœì…˜%', '%ëª¨í…”%', '%ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤%',
                '%ë¦¬ì¡°íŠ¸%', '%í•œì˜¥%', '%ê´€ê´‘í˜¸í…”%', '%ìœ ìŠ¤í˜¸ìŠ¤í…”%'
            ]

            docs = await self._execute_optimized_search(
                collection_id, regions, cities, categories, exclude_categories, limit
            )

            # ê²°ê³¼ ìºì‹±
            self.query_cache[cache_key] = docs

            query_time = time.time() - start_time
            self._update_metrics(query_time, len(docs))

            print(f"âœ… ìµœì í™”ëœ ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ ({query_time:.3f}ì´ˆ)")

            return docs

        except Exception as e:
            query_time = time.time() - start_time
            self._update_metrics(query_time, 0, error=str(e))
            print(f"âŒ ìµœì í™”ëœ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _get_collection_id(self, collection_name: str) -> Optional[str]:
        """ì»¬ë ‰ì…˜ ID ì¡°íšŒ (ìºì‹±ë¨)"""
        cache_key = f"collection_id:{collection_name}"

        if cache_key in self.metadata_cache:
            return self.metadata_cache[cache_key]

        try:
            async with self.async_engine.connect() as conn:
                result = await conn.execute(
                    self._compiled_queries['collection_uuid'],
                    {"collection_name": collection_name}
                )
                row = result.fetchone()

                if row:
                    collection_id = str(row.uuid)
                    self.metadata_cache[cache_key] = collection_id
                    return collection_id

        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ID ì¡°íšŒ ì˜¤ë¥˜: {e}")

        return None

    async def _execute_optimized_search(
        self,
        collection_id: str,
        regions: List[str],
        cities: List[str],
        categories: List[str],
        exclude_categories: List[str],
        limit: int
    ) -> List[Document]:
        """ìµœì í™”ëœ ê²€ìƒ‰ ì‹¤í–‰"""
        async with self.async_engine.connect() as conn:
            # ìš°ì„ ìˆœìœ„: ë„ì‹œ > ì§€ì—­ > ì¹´í…Œê³ ë¦¬ > ëžœë¤
            if cities:
                # PostgreSQL ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
                cities_patterns = [f'%{city.replace("íŠ¹ë³„ì‹œ", "").replace("ê´‘ì—­ì‹œ", "")}%' for city in cities]

                result = await conn.execute(
                    self._compiled_queries['place_by_cities'],
                    {
                        "collection_id": collection_id,
                        "cities": cities_patterns,
                        "exclude_categories": exclude_categories,
                        "limit_count": limit
                    }
                )

            elif regions:
                regions_patterns = [f'%{region.replace("ë„", "").replace("íŠ¹ë³„ìžì¹˜ë„", "")}%' for region in regions]

                result = await conn.execute(
                    self._compiled_queries['place_by_regions'],
                    {
                        "collection_id": collection_id,
                        "regions": regions_patterns,
                        "exclude_categories": exclude_categories,
                        "limit_count": limit
                    }
                )

            elif categories:
                categories_patterns = [f'%{cat}%' for cat in categories]

                result = await conn.execute(
                    self._compiled_queries['place_by_categories'],
                    {
                        "collection_id": collection_id,
                        "categories": categories_patterns,
                        "limit_count": limit
                    }
                )

            else:
                # ëžœë¤ ìƒ˜í”Œë§
                result = await conn.execute(
                    self._compiled_queries['place_random_sample'],
                    {
                        "collection_id": collection_id,
                        "exclude_categories": exclude_categories,
                        "limit_count": min(limit, 500)  # ëžœë¤ì€ ì œí•œ
                    }
                )

            # Document ê°ì²´ë¡œ ë³€í™˜
            docs = []
            for row in result.fetchall():
                metadata = row.cmetadata or {}
                metadata['search_method'] = 'optimized_sql'

                if row.embedding:
                    metadata['_embedding'] = row.embedding

                docs.append(Document(
                    page_content=row.document,
                    metadata=metadata
                ))

            return docs

    def _update_metrics(self, query_time: float, result_count: int, error: str = None):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics.total_response_time += query_time
        self.metrics.avg_response_time = (
            self.metrics.total_response_time / self.metrics.query_count
        )

        # ëŠë¦° ì¿¼ë¦¬ ê¸°ë¡ (1ì´ˆ ì´ìƒ)
        if query_time > 1.0:
            self.metrics.slow_queries.append({
                'timestamp': time.time(),
                'duration': query_time,
                'result_count': result_count,
                'error': error
            })

            # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
            if len(self.metrics.slow_queries) > 50:
                self.metrics.slow_queries = self.metrics.slow_queries[-50:]

    async def batch_search_places(
        self,
        queries: List[Dict[str, Any]],
        collection_name: str = 'place_recommendations'
    ) -> Dict[str, List[Document]]:
        """ë°°ì¹˜ ìž¥ì†Œ ê²€ìƒ‰ (ë™ì‹œ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)"""
        print(f"ðŸ”„ ë°°ì¹˜ ê²€ìƒ‰ ì‹œìž‘: {len(queries)}ê°œ ì¿¼ë¦¬")

        # ì»¬ë ‰ì…˜ ID ë¯¸ë¦¬ ì¡°íšŒ
        collection_id = await self._get_collection_id(collection_name)
        if not collection_id:
            return {}

        # ëª¨ë“  ê²€ìƒ‰ì„ ë™ì‹œì— ì‹¤í–‰
        tasks = []
        for i, query_params in enumerate(queries):
            task = self._single_search_task(i, query_params, collection_id)
            tasks.append(task)

        # ë™ì‹œ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì·¨í•©
        batch_results = {}
        for i, result in enumerate(results):
            query_key = f"query_{i}"
            if isinstance(result, Exception):
                print(f"âŒ ë°°ì¹˜ ê²€ìƒ‰ {i} ì‹¤íŒ¨: {result}")
                batch_results[query_key] = []
            else:
                batch_results[query_key] = result

        print(f"âœ… ë°°ì¹˜ ê²€ìƒ‰ ì™„ë£Œ: {len(batch_results)}ê°œ ê²°ê³¼")
        return batch_results

    async def _single_search_task(
        self,
        task_id: int,
        query_params: Dict[str, Any],
        collection_id: str
    ) -> List[Document]:
        """ë‹¨ì¼ ê²€ìƒ‰ íƒœìŠ¤í¬"""
        try:
            return await self.search_places_optimized(
                query=query_params.get('query', ''),
                regions=query_params.get('regions', []),
                cities=query_params.get('cities', []),
                categories=query_params.get('categories', []),
                limit=query_params.get('limit', 1000)
            )
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ê²€ìƒ‰ íƒœìŠ¤í¬ {task_id} ì˜¤ë¥˜: {e}")
            return []

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        cache_hit_rate = (
            self.metrics.cache_hits / max(self.metrics.cache_hits + self.metrics.cache_misses, 1)
        )

        return {
            "query_count": self.metrics.query_count,
            "cache_hit_rate": cache_hit_rate,
            "avg_response_time": self.metrics.avg_response_time,
            "cache_size": len(self.query_cache),
            "metadata_cache_size": len(self.metadata_cache),
            "slow_queries_count": len(self.metrics.slow_queries),
            "pool_status": {
                "pool_size": self.sync_engine.pool.size(),
                "checked_in": self.sync_engine.pool.checkedin(),
                "checked_out": self.sync_engine.pool.checkedout(),
                "overflow": self.sync_engine.pool.overflow()
            }
        }

    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        self.metadata_cache.clear()
        print("ðŸ§¹ ë°ì´í„°ë² ì´ìŠ¤ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    async def warm_up_cache(self, common_queries: List[Dict]):
        """ìºì‹œ ì˜ˆì—´"""
        print(f"ðŸ”¥ ë°ì´í„°ë² ì´ìŠ¤ ìºì‹œ ì˜ˆì—´ ì‹œìž‘: {len(common_queries)}ê°œ ì¿¼ë¦¬")

        # ì¹´íƒˆë¡œê·¸ ë¨¼ì € ë¡œë“œ
        await self.get_db_catalogs_optimized()

        # ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ ì˜ˆì—´
        for query_params in common_queries:
            await self.search_places_optimized(**query_params)

        print(f"âœ… ìºì‹œ ì˜ˆì—´ ì™„ë£Œ: {len(self.query_cache)}ê°œ ì¿¼ë¦¬, {len(self.metadata_cache)}ê°œ ë©”íƒ€ë°ì´í„°")

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.async_engine.dispose()
        self.sync_engine.dispose()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬ ì™„ë£Œ")


class DatabaseConnectionPool:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ê´€ë¦¬ìž"""

    def __init__(self, database_url: str, **pool_config):
        self.database_url = database_url
        self.pool_config = pool_config
        self._manager: Optional[OptimizedDatabaseManager] = None

    async def get_manager(self) -> OptimizedDatabaseManager:
        """ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¡°íšŒ (ì‹±ê¸€í†¤)"""
        if self._manager is None:
            self._manager = OptimizedDatabaseManager(self.database_url, **self.pool_config)
            # ìºì‹œ ì˜ˆì—´
            common_queries = [
                {"query": "ì„œìš¸ ì—¬í–‰", "cities": ["ì„œìš¸"], "limit": 100},
                {"query": "ë¶€ì‚° ì—¬í–‰", "cities": ["ë¶€ì‚°"], "limit": 100},
                {"query": "ì œì£¼ ì—¬í–‰", "cities": ["ì œì£¼"], "limit": 100},
                {"query": "ë§›ì§‘", "categories": ["ìŒì‹", "ë§›ì§‘"], "limit": 50},
                {"query": "ê´€ê´‘ì§€", "categories": ["ê´€ê´‘", "ë¬¸í™”"], "limit": 50}
            ]
            await self._manager.warm_up_cache(common_queries)

        return self._manager

    async def close(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        if self._manager:
            await self._manager.close()
            self._manager = None


# ì „ì—­ ì—°ê²° í’€ ì¸ìŠ¤í„´ìŠ¤
_global_pool: Optional[DatabaseConnectionPool] = None


def get_database_pool() -> DatabaseConnectionPool:
    """ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ í’€ ì¡°íšŒ"""
    global _global_pool
    if _global_pool is None:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ URL ì½ê¸°
        import os
        database_url = os.getenv('DATABASE_URL', 'postgresql://user:pass@localhost/db')

        _global_pool = DatabaseConnectionPool(
            database_url=database_url,
            pool_size=20,
            max_overflow=30,
            pool_recycle=3600,
            cache_size=1000,
            cache_ttl=3600
        )

    return _global_pool


async def get_optimized_database() -> OptimizedDatabaseManager:
    """ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¡°íšŒ"""
    pool = get_database_pool()
    return await pool.get_manager()


# íŽ¸ì˜ í•¨ìˆ˜ë“¤
async def search_places_fast(
    query: str,
    regions: List[str] = None,
    cities: List[str] = None,
    categories: List[str] = None,
    limit: int = 1000
) -> List[Document]:
    """ë¹ ë¥¸ ìž¥ì†Œ ê²€ìƒ‰"""
    db = await get_optimized_database()
    return await db.search_places_optimized(
        query=query,
        regions=regions,
        cities=cities,
        categories=categories,
        limit=limit
    )


async def get_catalogs_fast() -> Dict[str, List[str]]:
    """ë¹ ë¥¸ ì¹´íƒˆë¡œê·¸ ì¡°íšŒ"""
    db = await get_optimized_database()
    return await db.get_db_catalogs_optimized()