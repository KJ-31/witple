import numpy as np
from sentence_transformers import SentenceTransformer
import logging
from typing import Dict, List, Any
import asyncpg
import json
from config import settings

def cosine_similarity(X, Y):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    X = np.array(X)
    Y = np.array(Y)
    
    if X.ndim == 1:
        X = X.reshape(1, -1)
    if Y.ndim == 1:
        Y = Y.reshape(1, -1)
    
    # L2 ì •ê·œí™”
    X_norm = X / np.linalg.norm(X, axis=1, keepdims=True)
    Y_norm = Y / np.linalg.norm(Y, axis=1, keepdims=True)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity = np.dot(X_norm, Y_norm.T)
    
    return similarity

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RecommendationEngine:
    def __init__(self):
        self.bert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        self.db_url = settings.DATABASE_URL

    async def get_user_priority_tags(self, user_id: str) -> List[Dict]:
        """ì‚¬ìš©ì ìš°ì„ ìˆœìœ„ íƒœê·¸ ê°€ì ¸ì˜¤ê¸° - ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •"""
        conn = await asyncpg.connect(self.db_url)
        try:
            # 1. user_preferencesì—ì„œ priority (ë‹¨ì¼ ìµœìš°ì„  íƒœê·¸) ê°€ì ¸ì˜¤ê¸°
            priority_data = await conn.fetchval("""
                SELECT priority FROM user_preferences 
                WHERE user_id = $1 
                ORDER BY created_at DESC LIMIT 1
            """, user_id)
            
            # 2. user_preference_tagsì—ì„œ ì¶”ê°€ íƒœê·¸ë“¤ ê°€ì ¸ì˜¤ê¸°
            additional_tags = await conn.fetch("""
                SELECT tag, weight FROM user_preference_tags 
                WHERE user_id = $1
                ORDER BY created_at ASC
            """, user_id)
            
            priority_tags = []
            
            # 3. Priority íƒœê·¸ê°€ ìˆìœ¼ë©´ ìµœê³  ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •
            if priority_data:
                # Priorityë¥¼ ì‹¤ì œ descriptionì´ë‚˜ ê´€ë ¨ í‚¤ì›Œë“œë¡œ í™•ì¥
                priority_keywords = self._expand_priority_to_keywords(priority_data)
                for keyword in priority_keywords:
                    priority_tags.append({
                        'tag': keyword,
                        'weight': 10.0  # ìµœê³  ìš°ì„ ìˆœìœ„ - ë‹¤ë¥¸ íƒœê·¸ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë†’ê²Œ
                    })
            
            # 4. ì¶”ê°€ íƒœê·¸ë“¤ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¶”ê°€ (priorityì™€ ì¶©ëŒí•˜ëŠ” íƒœê·¸ ì œì™¸)
            priority_category = self._get_category_from_priority(priority_data) if priority_data else None
            
            for idx, tag_row in enumerate(additional_tags):
                tag = tag_row['tag']
                original_weight = tag_row['weight']
                
                # Priorityì™€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ëŠ” ê°€ì¤‘ì¹˜ë¥¼ í¬ê²Œ ë‚®ì¶¤
                if priority_category and self._is_conflicting_tag(tag, priority_category):
                    # ì¶©ëŒí•˜ëŠ” íƒœê·¸ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ëŒ€í­ ë‚®ì¶¤ (1.0)
                    weight = 1.0
                    logger.info(f"Conflicting tag '{tag}' weight reduced to 1.0 (priority: {priority_data})")
                else:
                    # ì¶©ëŒí•˜ì§€ ì•ŠëŠ” íƒœê·¸ëŠ” ê¸°ì¡´ ê°€ì¤‘ì¹˜ + ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜
                    base_weight = self._calculate_tag_category_weight(tag)
                    weight = min(base_weight + (original_weight * 0.5), 5.0)  # ìµœëŒ€ 5.0 ì œí•œ
                
                priority_tags.append({
                    'tag': tag,
                    'weight': weight
                })
                
                # ë„ˆë¬´ ë§ì€ íƒœê·¸ëŠ” ì„±ëŠ¥ì— ì•…ì˜í–¥ - ìƒìœ„ 12ê°œë§Œ
                if len(priority_tags) >= 12:
                    break
            
            # 5. ê¸°ë³¸ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if not priority_tags:
                return []
                
            return priority_tags
                
        except Exception as e:
            logger.error(f"Error getting user priority tags: {e}")
            return []
        finally:
            await conn.close()
    
    def _calculate_tag_category_weight(self, tag: str) -> float:
        """íƒœê·¸ ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        # ì—¬í–‰ ìš°ì„ ìˆœìœ„ ê´€ë ¨ íƒœê·¸ë“¤ (ë†’ì€ ê°€ì¤‘ì¹˜)
        high_priority_tags = {
            'ì‡¼í•‘', 'ë©´ì„¸ì ', 'ë°±í™”ì ', 'ì‡¼í•‘ëª°', 'íŠ¹ì‚°ë¬¼', 'êµ¬ë§¤',
            'ìˆ™ë°•', 'í˜¸í…”', 'ë¦¬ì¡°íŠ¸', 'íœì…˜', 'íœ´ì–‘ì‹œì„¤', 
            'ë§›ì§‘', 'ìŒì‹', 'ë ˆìŠ¤í† ë‘', 'ì¹´í˜',
            'ì²´í—˜', 'ì•¡í‹°ë¹„í‹°', 'ë ˆí¬ì¸ '
        }
        
        # ì¥ì†Œ/ì§€ì—­ ê´€ë ¨ íƒœê·¸ë“¤ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
        medium_priority_tags = {
            'ìì—°', 'ë°”ë‹¤', 'ì‚°', 'ê³µì›',
            'ë¬¸í™”', 'ì—­ì‚¬', 'ë°•ë¬¼ê´€', 'ëª…ì†Œ',
            'í•«í”Œë ˆì´ìŠ¤', 'ìœ ëª…ê´€ê´‘ì§€', 'ì¸ê¸°ëª…ì†Œ', 'í•„ìˆ˜ì½”ìŠ¤'
        }
        
        # ì„œë¹„ìŠ¤/í’ˆì§ˆ ê´€ë ¨ íƒœê·¸ë“¤ (ë‚®ì€ ê°€ì¤‘ì¹˜)
        low_priority_tags = {
            'ëŸ­ì…”ë¦¬', 'ê³ ê¸‰', 'ìµœê³ ê¸‰', 'ì„œë¹„ìŠ¤', 'í¸ì•ˆí•¨', 'ì•„ëŠ‘í•¨',
            'íë§', 'í‰í™”', 'íœ´ì‹', 'ì—¬ìœ ', 'ëŒ€ì¤‘ì '
        }
        
        if tag in high_priority_tags:
            return 4.0
        elif tag in medium_priority_tags:
            return 3.0
        elif tag in low_priority_tags:
            return 2.0
        else:
            return 1.5  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
    
    def _expand_priority_to_keywords(self, priority: str) -> List[str]:
        """Priorityë¥¼ ê´€ë ¨ í‚¤ì›Œë“œë¡œ í™•ì¥"""
        priority_mappings = {
            'accommodation': ['ìˆ™ë°•', 'í˜¸í…”', 'ë¦¬ì¡°íŠ¸', 'íœì…˜', 'ìˆ™ì†Œ', 'íœ´ì–‘ì‹œì„¤', 'í¸ì•ˆí•¨', 'ì„œë¹„ìŠ¤'],
            'restaurants': ['ë§›ì§‘', 'ìŒì‹', 'ë ˆìŠ¤í† ë‘', 'ì¹´í˜', 'ì‹ë‹¹', 'ìš”ë¦¬', 'ë¯¸ì‹', 'ì‹ì‚¬'],
            'shopping': ['ì‡¼í•‘', 'ë©´ì„¸ì ', 'ë°±í™”ì ', 'ì‡¼í•‘ëª°', 'íŠ¹ì‚°ë¬¼', 'êµ¬ë§¤', 'ìƒì ', 'ì‹œì¥'],
            'experience': ['ì²´í—˜', 'ì•¡í‹°ë¹„í‹°', 'ë ˆí¬ì¸ ', 'ëª¨í—˜', 'í™œë™', 'ê²½í—˜', 'ì¦ê±°ì›€', 'ì¬ë¯¸']
        }
        
        return priority_mappings.get(priority, [priority])
    
    def _get_category_from_priority(self, priority: str) -> str:
        """Priorityì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        category_mapping = {
            'accommodation': 'accommodation',
            'restaurants': 'restaurants', 
            'shopping': 'shopping',
            'experience': 'experience'
        }
        return category_mapping.get(priority, priority)
    
    def _is_conflicting_tag(self, tag: str, priority_category: str) -> bool:
        """íƒœê·¸ê°€ priority ì¹´í…Œê³ ë¦¬ì™€ ì¶©ëŒí•˜ëŠ”ì§€ í™•ì¸"""
        # ê° ì¹´í…Œê³ ë¦¬ë³„ íƒœê·¸ ì •ì˜
        category_tags = {
            'restaurants': ['ì‡¼í•‘', 'ë©´ì„¸ì ', 'ë°±í™”ì ', 'ì‡¼í•‘ëª°', 'ì‹œì¥', 'ì•„ìš¸ë ›', 'êµ¬ë§¤', 'ìƒì ', 'íŠ¹ì‚°ë¬¼'],
            'shopping': ['ë§›ì§‘', 'ìŒì‹', 'ë ˆìŠ¤í† ë‘', 'ì¹´í˜', 'ì‹ë‹¹', 'ìš”ë¦¬', 'ë¯¸ì‹', 'ì‹ì‚¬'],
            'accommodation': ['ì‡¼í•‘', 'ë©´ì„¸ì ', 'ë°±í™”ì ', 'ì‡¼í•‘ëª°', 'ì‹œì¥', 'ì•„ìš¸ë ›', 'ë§›ì§‘', 'ìŒì‹', 'ë ˆìŠ¤í† ë‘'],
            'experience': ['ì‡¼í•‘', 'ë©´ì„¸ì ', 'ë°±í™”ì ', 'ì‡¼í•‘ëª°', 'ì‹œì¥', 'ì•„ìš¸ë ›', 'ë§›ì§‘', 'ìŒì‹', 'ë ˆìŠ¤í† ë‘']
        }
        
        conflicting_tags = category_tags.get(priority_category, [])
        return tag in conflicting_tags

    async def get_tag_vectors(self, tags: List[str]) -> Dict[str, np.ndarray]:
        """íƒœê·¸ ë²¡í„° ê°€ì ¸ì˜¤ê¸° - BERTë¡œ ì§ì ‘ ì¸ì½”ë”©"""
        tag_vectors = {}
        for tag in tags:
            # BERTë¡œ ì§ì ‘ ì¸ì½”ë”© (preference_tags í…Œì´ë¸” ì—†ì´)
            tag_vectors[tag] = self.bert_model.encode([tag])[0].astype(np.float32)
        
        return tag_vectors

    async def get_user_vector(self, user_id: str) -> np.ndarray:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ë²¡í„° ìƒì„±"""
        priority_tags = await self.get_user_priority_tags(user_id)
        
        if not priority_tags:
            # ê¸°ë³¸ ë²¡í„° ë°˜í™˜
            return np.random.normal(0, 0.1, 384).astype(np.float32)
        
        # íƒœê·¸ë³„ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        tag_names = [tag_info['tag'] for tag_info in priority_tags]
        tag_vectors = await self.get_tag_vectors(tag_names)
        
        # ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ì‚¬ìš©ì ë²¡í„° ìƒì„±
        weighted_vectors = []
        for tag_info in priority_tags:
            tag = tag_info['tag']
            weight = tag_info['weight']
            
            if tag in tag_vectors:
                # ê°€ì¤‘ì¹˜ë§Œí¼ ë²¡í„°ë¥¼ ë°˜ë³µ ì¶”ê°€
                repeat_count = int(weight)
                for _ in range(repeat_count):
                    weighted_vectors.append(tag_vectors[tag])
        
        if weighted_vectors:
            # í‰ê·  ë²¡í„° ê³„ì‚°
            user_vector = np.mean(weighted_vectors, axis=0)
        else:
            user_vector = np.random.normal(0, 0.1, 384).astype(np.float32)
        
        return user_vector.astype(np.float32)

    async def get_user_preferences(self, user_id: str) -> Dict:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ê°€ì ¸ì˜¤ê¸° (ì¶”ì²œ ì‹œìŠ¤í…œìš©)"""
        conn = await asyncpg.connect(self.db_url)
        try:
            # ê¸°ë³¸ ì„ í˜¸ë„
            basic_prefs = await conn.fetchrow("""
                SELECT persona, priority, accommodation, exploration 
                FROM user_preferences 
                WHERE user_id = $1
                ORDER BY created_at DESC LIMIT 1
            """, user_id)
            
            # íƒœê·¸ ì„ í˜¸ë„
            tag_prefs = await conn.fetch("""
                SELECT tag, weight
                FROM user_preference_tags
                WHERE user_id = $1
                ORDER BY created_at ASC
            """, user_id)
            
            # ìš°ì„ ìˆœìœ„ íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
            priority_tags = await self.get_user_priority_tags(user_id)
            
            return {
                'basic': dict(basic_prefs) if basic_prefs else None,
                'tags': priority_tags,  # ìš°ì„ ìˆœìœ„ê°€ ì ìš©ëœ íƒœê·¸
                'original_tags': [dict(tag) for tag in tag_prefs] if tag_prefs else []
            }
            
        finally:
            await conn.close()

    async def get_fallback_places(self, limit: int = 20) -> List[Dict]:
        """ê°œì¸í™” ì¶”ì²œ ì‹¤íŒ¨ì‹œ ì¸ê¸°ë„ ê¸°ë°˜ ì¥ì†Œ ë°˜í™˜"""
        conn = await asyncpg.connect(self.db_url)
        try:
            query = """
                SELECT place_id, table_name, name, region, city, latitude, longitude, 
                       overview as description, image_urls,
                       random() as similarity_score
                FROM place_recommendations pr
                WHERE pr.name IS NOT NULL 
                  AND pr.overview IS NOT NULL 
                  AND pr.image_urls IS NOT NULL 
                  AND pr.image_urls != 'null'::jsonb
                ORDER BY random()
                LIMIT $1
            """
            
            places = await conn.fetch(query, limit)
            
            results = []
            for place in places:
                results.append({
                    'place_id': place['place_id'],
                    'table_name': place['table_name'],
                    'name': place['name'] or 'ì´ë¦„ ì—†ìŒ',
                    'region': place['region'] or 'ì§€ì—­ ë¯¸ìƒ',
                    'city': place['city'],
                    'latitude': float(place['latitude']) if place['latitude'] else 0.0,
                    'longitude': float(place['longitude']) if place['longitude'] else 0.0,
                    'description': place['description'] or 'ì„¤ëª… ì—†ìŒ',
                    'image_urls': place['image_urls'],
                    'similarity_score': 0.7 + (place.get('popularity_score', 0) * 0.1)
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Error in fallback places: {str(e)}")
            return []
        finally:
            await conn.close()

    async def get_personalized_recommendations(
        self, 
        user_id: str, 
        region: str = None, 
        category: str = None, 
        limit: int = 20
    ) -> List[Dict]:
        """ê°œì¸í™” ì¶”ì²œ - ë²¡í„° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜"""
        conn = await asyncpg.connect(self.db_url)
        try:
            # ì‚¬ìš©ì ë²¡í„° ìƒì„±
            user_vector = await self.get_user_vector(user_id)
            
            # place_recommendationsì—ì„œ ì¥ì†Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            query = """
                SELECT place_id, table_name, name, region, city, 
                       latitude, longitude, overview as description, 
                       image_urls, vector
                FROM place_recommendations
                WHERE vector IS NOT NULL
            """
            params = []
            param_count = 0
            
            # í•„í„° ì¡°ê±´ ì¶”ê°€
            if region:
                param_count += 1
                query += f" AND region = ${param_count}"
                params.append(region)
            
            if category:
                param_count += 1
                query += f" AND table_name = ${param_count}"
                params.append(category)
            
            query += " ORDER BY random() LIMIT 2000"  # ì„±ëŠ¥ì„ ìœ„í•´ 2000ê°œë¡œ ì œí•œ
            
            places = await conn.fetch(query, *params)
            
            if not places:
                return []
            
            # ë²¡í„° ì¶”ì¶œ ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            place_vectors = []
            valid_places = []
            
            for place in places:
                try:
                    if isinstance(place['vector'], str):
                        vector_data = json.loads(place['vector'])
                    else:
                        vector_data = place['vector']
                    
                    vector_array = np.array(vector_data, dtype=np.float32)
                    if len(vector_array) == 384:  # ì˜¬ë°”ë¥¸ ì°¨ì›ì¸ì§€ í™•ì¸
                        place_vectors.append(vector_array)
                        valid_places.append(place)
                except:
                    continue
            
            if not place_vectors:
                return []
            
            # ë°°ì¹˜ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            place_vectors = np.array(place_vectors)
            similarities = cosine_similarity(user_vector.reshape(1, -1), place_vectors)
            similarity_scores = similarities.flatten()
            
            # ê²°ê³¼ ìƒì„±
            results = []
            for i, place in enumerate(valid_places):
                results.append({
                    'place_id': place['place_id'],
                    'table_name': place['table_name'],
                    'name': place['name'] or 'ì´ë¦„ ì—†ìŒ',
                    'region': place['region'] or 'ì§€ì—­ ë¯¸ìƒ',
                    'city': place['city'],
                    'latitude': float(place['latitude']) if place['latitude'] else 0.0,
                    'longitude': float(place['longitude']) if place['longitude'] else 0.0,
                    'description': place['description'] or 'ì„¤ëª… ì—†ìŒ',
                    'image_urls': place['image_urls'],
                    'similarity_score': float(similarity_scores[i])
                })
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ ë°˜í™˜
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results[:limit]
        finally:
            await conn.close()

    async def get_popular_places(self, region: str = None, category: str = None, limit: int = 20) -> List[Dict]:
        """ì¸ê¸° ì¥ì†Œ ì¶”ì²œ"""
        conn = await asyncpg.connect(self.db_url)
        try:
            query = """
                SELECT place_id, table_name, name, region, city,
                       latitude, longitude, overview as description, image_urls,
                       0.7 as similarity_score
                FROM place_recommendations
                WHERE name IS NOT NULL AND overview IS NOT NULL
            """
            params = []
            param_count = 0
            
            if region:
                param_count += 1
                query += f" AND region = ${param_count}"
                params.append(region)
            
            if category:
                param_count += 1
                query += f" AND table_name = ${param_count}"
                params.append(category)
            
            query += " ORDER BY random() LIMIT $" + str(param_count + 1)
            params.append(limit)
            
            places = await conn.fetch(query, *params)
            
            results = []
            for place in places:
                results.append({
                    'place_id': place['place_id'],
                    'table_name': place['table_name'],
                    'name': place['name'] or 'ì´ë¦„ ì—†ìŒ',
                    'region': place['region'] or 'ì§€ì—­ ë¯¸ìƒ',
                    'city': place['city'],
                    'latitude': float(place['latitude']) if place['latitude'] else 0.0,
                    'longitude': float(place['longitude']) if place['longitude'] else 0.0,
                    'description': place['description'] or 'ì„¤ëª… ì—†ìŒ',
                    'image_urls': place['image_urls'],
                    'similarity_score': place['similarity_score']
                })
            
            return results
        finally:
            await conn.close()


# ============================================================================
# ğŸš€ ENHANCED RECOMMENDATION SYSTEM (íŒ€ ê²€í†  í›„ ì ìš©)
# ============================================================================

"""
# ê°œì„ ëœ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ - Phase 1: ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥í•œ ê³ ë„í™”

import math
from datetime import datetime, timedelta
from typing import Optional

# 1. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ì‹œìŠ¤í…œ
ACTION_WEIGHTS = {
    'click': 1.0,      # ê¸°ë³¸ ê´€ì‹¬ë„ (ê°€ì¥ ë§ì€ ë°ì´í„°)
    'like': 3.0,       # 3ë°° ê°€ì¤‘ì¹˜ (ê¸ì •ì  ë°˜ì‘)
    'bookmark': 5.0    # 5ë°° ê°€ì¤‘ì¹˜ (ê°•í•œ ì„ í˜¸, ì¬ë°©ë¬¸ ì˜ë„)
}

def calculate_weighted_popularity_score(place_data: Dict) -> float:
    '''ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°'''
    weighted_score = (
        place_data.get('total_clicks', 0) * ACTION_WEIGHTS['click'] +
        place_data.get('total_likes', 0) * ACTION_WEIGHTS['like'] + 
        place_data.get('total_bookmarks', 0) * ACTION_WEIGHTS['bookmark']
    )
    
    # ì •ê·œí™” (0-100 ìŠ¤ì¼€ì¼)
    # ê¸°ì¤€: click 50ê°œ + like 10ê°œ + bookmark 5ê°œ = 100ì 
    max_reference_score = (50 * 1.0) + (10 * 3.0) + (5 * 5.0)  # = 105
    normalized_score = min((weighted_score / max_reference_score) * 100, 100)
    
    return round(normalized_score, 2)

def calculate_engagement_score(place_data: Dict) -> float:
    '''ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚° (ì•¡ì…˜ì˜ ì§ˆ ì¤‘ì‹¬)'''
    total_actions = (place_data.get('total_clicks', 0) + 
                    place_data.get('total_likes', 0) + 
                    place_data.get('total_bookmarks', 0))
    
    if total_actions == 0:
        return 0.0
    
    # ë†’ì€ ê°€ì¤‘ì¹˜ ì•¡ì…˜ ë¹„ìœ¨
    high_value_actions = place_data.get('total_likes', 0) + place_data.get('total_bookmarks', 0)
    engagement_ratio = high_value_actions / total_actions
    
    # ì°¸ì—¬ë„ ì ìˆ˜ (ë¹„ìœ¨ * 100 + ì ˆëŒ€ê°’ ë³´ì •)
    base_engagement = engagement_ratio * 100
    
    # ì ˆëŒ€ê°’ ë³´ì • (ìµœì†Œí•œì˜ like/bookmarkì´ ìˆì–´ì•¼ ë†’ì€ ì ìˆ˜)
    min_threshold_bonus = min(high_value_actions * 5, 20)  # ìµœëŒ€ 20ì  ë³´ë„ˆìŠ¤
    
    return min(base_engagement + min_threshold_bonus, 100)

# 2. ì‹œê°„ ê°ì‡„ í•¨ìˆ˜ (Temporal Decay)
def calculate_time_decay_weight(action_date: datetime, decay_rate: float = 0.02) -> float:
    '''ì‹œê°„ ê°ì‡„ ê°€ì¤‘ì¹˜ ê³„ì‚°'''
    if action_date is None:
        return 0.5  # ê¸°ë³¸ê°’
    
    days_ago = (datetime.now() - action_date).days
    
    # ì§€ìˆ˜ ê°ì‡„: e^(-decay_rate * days)
    # decay_rate = 0.02 ê¸°ì¤€ìœ¼ë¡œ ì•½ 35ì¼ í›„ 50% ê°€ì¤‘ì¹˜
    weight = math.exp(-decay_rate * days_ago)
    
    return max(weight, 0.1)  # ìµœì†Œ 10% ê°€ì¤‘ì¹˜ëŠ” ìœ ì§€

def calculate_time_weighted_popularity(actions_data: List[Dict]) -> float:
    '''ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš©ëœ ì¸ê¸°ë„ ê³„ì‚°'''
    total_weighted_score = 0
    
    for action in actions_data:
        base_weight = ACTION_WEIGHTS[action['action_type']]
        time_weight = calculate_time_decay_weight(action['created_at'])
        
        total_weighted_score += base_weight * time_weight
    
    return total_weighted_score

# 3. ë²¡í„° ê¸°ë°˜ ì¶”ì²œ ì—”ì§„ (ìƒˆë¡œìš´ í–‰ë™ ë²¡í„° í™œìš©)
class VectorBasedRecommendationEngine:
    '''AWS Batchì—ì„œ ìƒì„±ëœ ì‚¬ìš©ì/ì¥ì†Œ ë²¡í„°ë¥¼ í™œìš©í•œ ì¶”ì²œ ì—”ì§„'''
    
    def __init__(self):
        self.db_url = settings.DATABASE_URL
        logger.info("ğŸ¤– VectorBasedRecommendationEngine initialized")
    
    async def get_user_behavior_vector(self, user_id: str) -> Optional[np.ndarray]:
        '''ì‚¬ìš©ì í–‰ë™ ë²¡í„° ê°€ì ¸ì˜¤ê¸°'''
        conn = await asyncpg.connect(self.db_url)
        try:
            vector_data = await conn.fetchrow('''
                SELECT behavior_vector, like_score, bookmark_score, click_score, 
                       total_actions, vector_updated_at
                FROM user_behavior_vectors 
                WHERE user_id = $1
            ''', user_id)
            
            if not vector_data or not vector_data['behavior_vector']:
                logger.info(f"No behavior vector found for user {user_id}")
                return None
                
            # PostgreSQL ARRAYë¥¼ numpy arrayë¡œ ë³€í™˜
            vector = np.array(vector_data['behavior_vector'], dtype=np.float32)
            
            if len(vector) != 384:
                logger.warning(f"Invalid vector dimension for user {user_id}: {len(vector)}")
                return None
                
            logger.info(f"âœ… User behavior vector loaded: {user_id} (actions: {vector_data['total_actions']})")
            return vector
            
        except Exception as e:
            logger.error(f"âŒ Error loading user vector {user_id}: {str(e)}")
            return None
        finally:
            await conn.close()
    
    async def get_place_vectors_enhanced(self, category: str = None, limit: int = 1000) -> List[Dict[str, Any]]:
        '''í–¥ìƒëœ ì¥ì†Œ ë²¡í„° ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ê°€ì¤‘ì¹˜ ì ìˆ˜ í¬í•¨)'''
        conn = await asyncpg.connect(self.db_url)
        try:
            query = '''
                SELECT pv.place_id, pv.place_category, pv.combined_vector,
                       pv.popularity_score, pv.engagement_score,
                       pv.total_likes, pv.total_bookmarks, pv.total_clicks,
                       pv.vector_updated_at,
                       pr.name, pr.region, pr.city, pr.latitude, pr.longitude,
                       pr.overview as description, pr.image_urls
                FROM place_vectors pv
                LEFT JOIN place_recommendations pr ON pv.place_id = pr.place_id
                WHERE pv.combined_vector IS NOT NULL
            '''
            params = []
            
            if category:
                query += " AND pv.place_category = $1"
                params.append(category)
            
            query += f" ORDER BY pv.popularity_score DESC LIMIT {limit}"
            
            places = await conn.fetch(query, *params)
            
            results = []
            for place in places:
                if not place['combined_vector']:
                    continue
                    
                try:
                    vector = np.array(place['combined_vector'], dtype=np.float32)
                    if len(vector) != 384:
                        continue
                    
                    # ê°œì„ ëœ ì ìˆ˜ ê³„ì‚° ì ìš©
                    place_data = {
                        'total_clicks': place['total_clicks'] or 0,
                        'total_likes': place['total_likes'] or 0, 
                        'total_bookmarks': place['total_bookmarks'] or 0
                    }
                    
                    enhanced_popularity = calculate_weighted_popularity_score(place_data)
                    enhanced_engagement = calculate_engagement_score(place_data)
                    
                    results.append({
                        'place_id': place['place_id'],
                        'place_category': place['place_category'],
                        'vector': vector,
                        'popularity_score': enhanced_popularity,  # ê°œì„ ëœ ì ìˆ˜ ì‚¬ìš©
                        'engagement_score': enhanced_engagement,  # ê°œì„ ëœ ì ìˆ˜ ì‚¬ìš©
                        'legacy_popularity': float(place['popularity_score'] or 0),  # ê¸°ì¡´ ì ìˆ˜ ë³´ì¡´
                        'legacy_engagement': float(place['engagement_score'] or 0),  # ê¸°ì¡´ ì ìˆ˜ ë³´ì¡´
                        'total_interactions': place_data['total_clicks'] + place_data['total_likes'] + place_data['total_bookmarks'],
                        'name': place['name'] or 'ì´ë¦„ ì—†ìŒ',
                        'region': place['region'] or 'ì§€ì—­ ë¯¸ìƒ',
                        'city': place['city'],
                        'latitude': float(place['latitude']) if place['latitude'] else 0.0,
                        'longitude': float(place['longitude']) if place['longitude'] else 0.0,
                        'description': place['description'] or 'ì„¤ëª… ì—†ìŒ',
                        'image_urls': place['image_urls'],
                        'vector_updated_at': place['vector_updated_at']
                    })
                except Exception as e:
                    logger.warning(f"Failed to process place {place['place_id']}: {str(e)}")
                    continue
            
            logger.info(f"âœ… Loaded {len(results)} enhanced place vectors")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Error loading enhanced place vectors: {str(e)}")
            return []
        finally:
            await conn.close()
    
    async def get_vector_based_recommendations_enhanced(
        self, 
        user_id: str, 
        category: str = None, 
        region: str = None,
        limit: int = 20,
        diversity_boost: float = 0.1,
        use_time_decay: bool = True
    ) -> List[Dict[str, Any]]:
        '''í–¥ìƒëœ ë²¡í„° ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ'''
        
        # 1. ì‚¬ìš©ì í–‰ë™ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        user_vector = await self.get_user_behavior_vector(user_id)
        if user_vector is None:
            logger.info(f"No user vector available for {user_id}, falling back to enhanced popular recommendations")
            return await self.get_enhanced_popular_recommendations(category=category, region=region, limit=limit)
        
        # 2. ì¥ì†Œ ë²¡í„°ë“¤ ê°€ì ¸ì˜¤ê¸° (í–¥ìƒëœ ì ìˆ˜ í¬í•¨)
        places = await self.get_place_vectors_enhanced(category=category)
        if not places:
            logger.warning("No place vectors available")
            return []
        
        # 3. ì§€ì—­ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
        if region:
            places = [p for p in places if p['region'] == region]
        
        if not places:
            logger.info(f"No places found for region: {region}")
            return []
        
        # 4. ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
        place_vectors = np.array([p['vector'] for p in places])
        similarities = cosine_similarity(user_vector.reshape(1, -1), place_vectors).flatten()
        
        # 5. í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
        results = []
        for i, place in enumerate(places):
            similarity = float(similarities[i])
            
            # í–¥ìƒëœ ì ìˆ˜ ì‚¬ìš© (ê°€ì¤‘ì¹˜ ë°˜ì˜ë¨)
            popularity_norm = min(place['popularity_score'] / 100.0, 1.0)
            engagement_norm = min(place['engagement_score'] / 100.0, 1.0)
            
            # ì‹œê°„ ê°€ì¤‘ì¹˜ (ë²¡í„° ì—…ë°ì´íŠ¸ ìµœì‹ ì„±)
            time_weight = 1.0
            if use_time_decay and place['vector_updated_at']:
                time_weight = calculate_time_decay_weight(place['vector_updated_at'], decay_rate=0.01)
            
            # í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: ë²¡í„° ìœ ì‚¬ë„ + ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ + ì‹œê°„ ê°ì‡„
            hybrid_score = (
                similarity * 0.6 +              # ë²¡í„° ìœ ì‚¬ë„ 60%
                popularity_norm * 0.25 +        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¸ê¸°ë„ 25%
                engagement_norm * 0.15          # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì°¸ì—¬ë„ 15%
            ) * time_weight                     # ì‹œê°„ ê°ì‡„ ì ìš©
            
            # ë‹¤ì–‘ì„± ë¶€ìŠ¤íŒ… (ì¤‘ê°„ ì¸ê¸°ë„ ì¥ì†Œ í˜œíƒ)
            if 20 <= place['popularity_score'] <= 70:
                hybrid_score += diversity_boost
            
            results.append({
                'place_id': place['place_id'],
                'place_category': place['place_category'],
                'name': place['name'],
                'region': place['region'],
                'city': place['city'],
                'latitude': place['latitude'],
                'longitude': place['longitude'],
                'description': place['description'],
                'image_urls': place['image_urls'],
                'similarity_score': similarity,
                'popularity_score': place['popularity_score'],  # í–¥ìƒëœ ì ìˆ˜
                'engagement_score': place['engagement_score'],  # í–¥ìƒëœ ì ìˆ˜
                'hybrid_score': hybrid_score,
                'time_weight': time_weight,
                'recommendation_type': 'enhanced_vector_based'
            })
        
        # 6. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # 7. ë‹¤ì–‘ì„± í•„í„°ë§ ì ìš©
        filtered_results = self.apply_diversity_filter(results, similarity_threshold=0.95)
        
        logger.info(f"âœ… Generated {len(filtered_results[:limit])} enhanced vector-based recommendations for user {user_id}")
        return filtered_results[:limit]
    
    async def get_enhanced_popular_recommendations(
        self, 
        category: str = None, 
        region: str = None, 
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        '''í–¥ìƒëœ ì¸ê¸° ê¸°ë°˜ ì¶”ì²œ (ë²¡í„°ê°€ ì—†ì„ ë•Œ)'''
        places = await self.get_place_vectors_enhanced(category=category, limit=500)
        
        if region:
            places = [p for p in places if p['region'] == region]
        
        # í–¥ìƒëœ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬
        for place in places:
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ + ì°¸ì—¬ë„ ì¡°í•©
            place['hybrid_score'] = (place['popularity_score'] * 0.7) + (place['engagement_score'] * 0.3)
            place['similarity_score'] = 0.5  # ê¸°ë³¸ê°’
            place['recommendation_type'] = 'enhanced_popular'
        
        places.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        return places[:limit]
    
    def apply_diversity_filter(self, places: List[Dict], similarity_threshold: float = 0.95) -> List[Dict]:
        '''ë‹¤ì–‘ì„± í•„í„°ë§ (ì¹´í…Œê³ ë¦¬/ì§€ì—­ ë‹¤ì–‘ì„± í™•ë³´)'''
        if len(places) <= 10:
            return places
        
        filtered = []
        category_count = {}
        region_count = {}
        max_per_category = max(len(places) // 4, 2)  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ê°œìˆ˜
        max_per_region = max(len(places) // 3, 3)    # ì§€ì—­ë‹¹ ìµœëŒ€ ê°œìˆ˜
        
        for place in places:
            category = place.get('place_category', 'unknown')
            region = place.get('region', 'unknown')
            
            # ì¹´í…Œê³ ë¦¬/ì§€ì—­ ì œí•œ í™•ì¸
            if (category_count.get(category, 0) < max_per_category and 
                region_count.get(region, 0) < max_per_region):
                
                filtered.append(place)
                category_count[category] = category_count.get(category, 0) + 1
                region_count[region] = region_count.get(region, 0) + 1
        
        # ë¹ˆ ìë¦¬ëŠ” ë†’ì€ ì ìˆ˜ ìˆœìœ¼ë¡œ ì±„ì›€
        remaining_slots = len(places) - len(filtered)
        if remaining_slots > 0:
            remaining_places = [p for p in places if p not in filtered]
            filtered.extend(remaining_places[:remaining_slots])
        
        return filtered

# 4. í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì—”ì§„ (ê¸°ì¡´ + ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ê²°í•©)
class HybridRecommendationEngine:
    '''ê¸°ì¡´ íƒœê·¸ ê¸°ë°˜ + ìƒˆë¡œìš´ ë²¡í„° ê¸°ë°˜ ì¶”ì²œì„ ê²°í•©í•˜ëŠ” ì—”ì§„'''
    
    def __init__(self):
        self.legacy_engine = RecommendationEngine()  # ê¸°ì¡´ íƒœê·¸ ê¸°ë°˜
        self.vector_engine = VectorBasedRecommendationEngine()  # ìƒˆë¡œìš´ í–‰ë™ ê¸°ë°˜
        logger.info("ğŸš€ HybridRecommendationEngine initialized")
        
    async def get_hybrid_recommendations(
        self, 
        user_id: str, 
        region: str = None, 
        category: str = None, 
        limit: int = 20,
        algorithm: str = 'balanced'  # 'vector_heavy', 'tag_heavy', 'balanced'
    ) -> List[Dict]:
        '''í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ê²°ê³¼ ìƒì„±'''
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        algorithm_weights = {
            'vector_heavy': {'vector': 0.8, 'tag': 0.2},
            'tag_heavy': {'vector': 0.3, 'tag': 0.7}, 
            'balanced': {'vector': 0.6, 'tag': 0.4}
        }
        weights = algorithm_weights.get(algorithm, algorithm_weights['balanced'])
        
        # 1. ì‚¬ìš©ì í–‰ë™ ë²¡í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_behavior_vector = await self.vector_engine.get_user_behavior_vector(user_id) is not None
        
        if has_behavior_vector:
            # 2-A. í–‰ë™ ë°ì´í„° ìˆëŠ” ê²½ìš°: ë²¡í„° + íƒœê·¸ ê²°í•©
            vector_results = await self.vector_engine.get_vector_based_recommendations_enhanced(
                user_id, category, region, limit * 2  # 2ë°°ë¡œ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´
            )
            tag_results = await self.legacy_engine.get_personalized_recommendations(
                user_id, region, category, limit
            )
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ê²°í•©
            hybrid_results = self.combine_recommendations(vector_results, tag_results, weights)
            
        else:
            # 2-B. í–‰ë™ ë°ì´í„° ì—†ëŠ” ê²½ìš°: íƒœê·¸ ê¸°ë°˜ + í–¥ìƒëœ ì¸ê¸°ë„
            tag_results = await self.legacy_engine.get_personalized_recommendations(
                user_id, region, category, limit * 1.5
            )
            popular_results = await self.vector_engine.get_enhanced_popular_recommendations(
                category, region, limit // 2
            )
            
            # íƒœê·¸ ì¤‘ì‹¬ ê²°í•©
            hybrid_results = self.combine_recommendations(tag_results, popular_results, {
                'primary': 0.8,
                'secondary': 0.2
            })
        
        # 3. ìµœì¢… ë‹¤ì–‘ì„± í•„í„°ë§ ì ìš©
        diversified_results = self.vector_engine.apply_diversity_filter(hybrid_results)
        
        # 4. ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        final_results = sorted(diversified_results, key=lambda x: x.get('hybrid_score', 0), reverse=True)
        
        logger.info(f"âœ… Generated {len(final_results[:limit])} hybrid recommendations for user {user_id} (algorithm: {algorithm})")
        return final_results[:limit]

    def combine_recommendations(
        self, 
        primary_results: List[Dict], 
        secondary_results: List[Dict], 
        weights: Dict[str, float]
    ) -> List[Dict]:
        '''ë‘ ì¶”ì²œ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ê²°í•©'''
        
        # ì¥ì†Œ IDë³„ë¡œ ê·¸ë£¹í™”
        place_scores = {}
        
        # Primary ê²°ê³¼ ì²˜ë¦¬
        primary_weight = list(weights.values())[0]
        for i, place in enumerate(primary_results):
            place_id = place['place_id']
            # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ (1ìœ„: 1.0, ë§ˆì§€ë§‰: 0.1)
            rank_score = 1.0 - (i / len(primary_results)) * 0.9
            
            place_scores[place_id] = {
                'place_data': place,
                'primary_score': rank_score * primary_weight,
                'secondary_score': 0.0
            }
        
        # Secondary ê²°ê³¼ ì²˜ë¦¬  
        secondary_weight = list(weights.values())[1]
        for i, place in enumerate(secondary_results):
            place_id = place['place_id']
            rank_score = 1.0 - (i / len(secondary_results)) * 0.9
            
            if place_id in place_scores:
                place_scores[place_id]['secondary_score'] = rank_score * secondary_weight
            else:
                place_scores[place_id] = {
                    'place_data': place,
                    'primary_score': 0.0,
                    'secondary_score': rank_score * secondary_weight
                }
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        for place_id, scores in place_scores.items():
            hybrid_score = scores['primary_score'] + scores['secondary_score']
            
            result = {
                **scores['place_data'],
                'hybrid_score': hybrid_score,
                'primary_score': scores['primary_score'],
                'secondary_score': scores['secondary_score'],
                'recommendation_source': 'hybrid'
            }
            hybrid_results.append(result)
        
        return hybrid_results

# ì‚¬ìš© ì˜ˆì‹œ (ë¼ìš°í„°ì—ì„œ ì ìš©)
# hybrid_engine = HybridRecommendationEngine()
# recommendations = await hybrid_engine.get_hybrid_recommendations(
#     user_id=user.user_id,
#     region=region,
#     category=category, 
#     limit=limit,
#     algorithm='balanced'  # 'vector_heavy', 'tag_heavy', 'balanced'
# )
"""