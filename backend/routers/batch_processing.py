"""
AWS Batch ì²˜ë¦¬ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
- ë°°ì¹˜ ì‘ì—… ì™„ë£Œ webhook ì²˜ë¦¬
- ì‚¬ìš©ì í–‰ë™ ë²¡í„° ì—…ë°ì´íŠ¸
- ì¥ì†Œ ë²¡í„° ì—…ë°ì´íŠ¸
- ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ
"""
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Request, status
from sqlalchemy.orm import Session
from sqlalchemy import and_, func, desc
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import json
import logging
import boto3
from botocore.exceptions import ClientError

from database import get_db
from models import User, UserAction, UserBehaviorVector, PlaceVector
from auth_utils import get_current_user
import os

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# AWS í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
batch_client = boto3.client('batch', region_name=os.getenv('AWS_REGION', 'ap-northeast-2'))

router = APIRouter(prefix="/batch", tags=["batch-processing"])

# ============= Pydantic ìŠ¤í‚¤ë§ˆë“¤ =============

from pydantic import BaseModel
from typing import Union

class BatchJobCompletionWebhook(BaseModel):
    """AWS Batch ì‘ì—… ì™„ë£Œ webhook ë°ì´í„°"""
    job_id: str
    job_name: str
    job_status: str  # SUCCEEDED, FAILED
    batch_id: str
    processed_records: int
    processing_time_seconds: float
    s3_input_path: str
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class UserVectorUpdate(BaseModel):
    """ì‚¬ìš©ì ë²¡í„° ì—…ë°ì´íŠ¸ ë°ì´í„°"""
    user_id: str
    behavior_vector: List[float]  # 384ì°¨ì› BERT ë²¡í„°
    like_score: float
    bookmark_score: float
    click_score: float
    dwell_time_score: float
    total_actions: int
    total_likes: int
    total_bookmarks: int
    total_clicks: int
    last_action_date: datetime

class PlaceVectorUpdate(BaseModel):
    """ì¥ì†Œ ë²¡í„° ì—…ë°ì´íŠ¸ ë°ì´í„°"""
    place_id: str
    place_category: str
    content_vector: Optional[List[float]] = None
    behavior_vector: Optional[List[float]] = None
    combined_vector: Optional[List[float]] = None
    total_likes: int = 0
    total_bookmarks: int = 0
    total_clicks: int = 0
    unique_users: int = 0
    avg_dwell_time: float = 0.0
    popularity_score: float = 0.0
    engagement_score: float = 0.0

class BatchProcessingStatus(BaseModel):
    """ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì •ë³´"""
    total_unprocessed_actions: int
    oldest_unprocessed_action: Optional[datetime]
    recent_batch_jobs: List[Dict[str, Any]]
    user_vectors_updated: int
    place_vectors_updated: int

# ============= ì›¹í›… ì—”ë“œí¬ì¸íŠ¸ =============

@router.post("/webhook/completion")
async def batch_job_completion_webhook(
    webhook_data: BatchJobCompletionWebhook,
    background_tasks: BackgroundTasks,
    request: Request,
    db: Session = Depends(get_db)
):
    """
    AWS Batch ì‘ì—… ì™„ë£Œ ì‹œ í˜¸ì¶œë˜ëŠ” webhook
    ë°°ì¹˜ ì‘ì—…ì´ ì„±ê³µí•˜ë©´ ì²˜ë¦¬ëœ ì•¡ì…˜ë“¤ì„ batch_processed=Trueë¡œ ì—…ë°ì´íŠ¸
    """
    logger.info(f"ğŸ”” Batch job completion webhook: {webhook_data.job_id} - {webhook_data.job_status}")
    
    try:
        # ìš”ì²­ ë©”íƒ€ë°ì´í„° ë¡œê¹…
        client_ip = request.client.host
        logger.info(f"Webhook from IP: {client_ip}, Job: {webhook_data.job_name}")
        
        if webhook_data.job_status == "SUCCEEDED":
            # ì„±ê³µí•œ ê²½ìš° - í•´ë‹¹ batch_idì˜ ì•¡ì…˜ë“¤ì„ ì²˜ë¦¬ ì™„ë£Œë¡œ ë§ˆí‚¹
            updated_count = db.query(UserAction).filter(
                UserAction.batch_id == webhook_data.batch_id,
                UserAction.batch_processed == False
            ).update({
                UserAction.batch_processed: True,
                UserAction.batch_processed_at: datetime.now()
            })
            
            db.commit()
            
            logger.info(f"âœ… Marked {updated_count} actions as processed for batch {webhook_data.batch_id}")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„° ì—…ë°ì´íŠ¸ ì•Œë¦¼ ì²˜ë¦¬
            background_tasks.add_task(
                process_vector_updates_notification,
                webhook_data.batch_id,
                webhook_data.processed_records
            )
            
            return {
                "success": True,
                "message": f"Batch job {webhook_data.job_id} completed successfully",
                "updated_actions": updated_count,
                "batch_id": webhook_data.batch_id
            }
            
        else:
            # ì‹¤íŒ¨í•œ ê²½ìš° - ë¡œê¹…ë§Œ í•˜ê³  ì¬ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ë‘ 
            logger.error(f"âŒ Batch job {webhook_data.job_id} failed: {webhook_data.error_message}")
            
            return {
                "success": False,
                "message": f"Batch job {webhook_data.job_id} failed",
                "error": webhook_data.error_message,
                "batch_id": webhook_data.batch_id
            }
            
    except Exception as e:
        logger.error(f"âŒ Webhook processing failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Webhook processing failed: {str(e)}"
        )

@router.post("/webhook/vector-updates")
async def batch_vector_updates_webhook(
    user_updates: List[UserVectorUpdate],
    place_updates: List[PlaceVectorUpdate],
    db: Session = Depends(get_db)
):
    """
    AWS Batchì—ì„œ ê³„ì‚°ëœ ë²¡í„° ì—…ë°ì´íŠ¸ë¥¼ ë°›ëŠ” webhook
    ì‚¬ìš©ì ë° ì¥ì†Œ ë²¡í„° ë°ì´í„°ë¥¼ ì¼ê´„ ì—…ë°ì´íŠ¸
    """
    logger.info(f"ğŸ“Š Vector updates webhook: {len(user_updates)} users, {len(place_updates)} places")
    
    try:
        updated_users = 0
        updated_places = 0
        
        # ì‚¬ìš©ì ë²¡í„° ì—…ë°ì´íŠ¸
        for user_update in user_updates:
            # ê¸°ì¡´ ë²¡í„° ë ˆì½”ë“œ ì¡°íšŒ
            existing_vector = db.query(UserBehaviorVector).filter(
                UserBehaviorVector.user_id == user_update.user_id
            ).first()
            
            if existing_vector:
                # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                existing_vector.behavior_vector = user_update.behavior_vector
                existing_vector.like_score = user_update.like_score
                existing_vector.bookmark_score = user_update.bookmark_score
                existing_vector.click_score = user_update.click_score
                existing_vector.dwell_time_score = user_update.dwell_time_score
                existing_vector.total_actions = user_update.total_actions
                existing_vector.total_likes = user_update.total_likes
                existing_vector.total_bookmarks = user_update.total_bookmarks
                existing_vector.total_clicks = user_update.total_clicks
                existing_vector.last_action_date = user_update.last_action_date
                existing_vector.vector_updated_at = datetime.now()
            else:
                # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                new_vector = UserBehaviorVector(
                    user_id=user_update.user_id,
                    behavior_vector=user_update.behavior_vector,
                    like_score=user_update.like_score,
                    bookmark_score=user_update.bookmark_score,
                    click_score=user_update.click_score,
                    dwell_time_score=user_update.dwell_time_score,
                    total_actions=user_update.total_actions,
                    total_likes=user_update.total_likes,
                    total_bookmarks=user_update.total_bookmarks,
                    total_clicks=user_update.total_clicks,
                    last_action_date=user_update.last_action_date
                )
                db.add(new_vector)
            
            updated_users += 1
        
        # ì¥ì†Œ ë²¡í„° ì—…ë°ì´íŠ¸
        for place_update in place_updates:
            # ê¸°ì¡´ ì¥ì†Œ ë²¡í„° ì¡°íšŒ
            existing_place = db.query(PlaceVector).filter(
                and_(
                    PlaceVector.place_id == place_update.place_id,
                    PlaceVector.place_category == place_update.place_category
                )
            ).first()
            
            if existing_place:
                # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                if place_update.content_vector:
                    existing_place.content_vector = place_update.content_vector
                if place_update.behavior_vector:
                    existing_place.behavior_vector = place_update.behavior_vector
                if place_update.combined_vector:
                    existing_place.combined_vector = place_update.combined_vector
                
                existing_place.total_likes = place_update.total_likes
                existing_place.total_bookmarks = place_update.total_bookmarks
                existing_place.total_clicks = place_update.total_clicks
                existing_place.unique_users = place_update.unique_users
                existing_place.avg_dwell_time = place_update.avg_dwell_time
                existing_place.popularity_score = place_update.popularity_score
                existing_place.engagement_score = place_update.engagement_score
                existing_place.vector_updated_at = datetime.now()
                existing_place.stats_updated_at = datetime.now()
            else:
                # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                new_place = PlaceVector(
                    place_id=place_update.place_id,
                    place_category=place_update.place_category,
                    content_vector=place_update.content_vector,
                    behavior_vector=place_update.behavior_vector,
                    combined_vector=place_update.combined_vector,
                    total_likes=place_update.total_likes,
                    total_bookmarks=place_update.total_bookmarks,
                    total_clicks=place_update.total_clicks,
                    unique_users=place_update.unique_users,
                    avg_dwell_time=place_update.avg_dwell_time,
                    popularity_score=place_update.popularity_score,
                    engagement_score=place_update.engagement_score
                )
                db.add(new_place)
            
            updated_places += 1
        
        # ëª¨ë“  ë³€ê²½ì‚¬í•­ ì»¤ë°‹
        db.commit()
        
        logger.info(f"âœ… Vector updates completed: {updated_users} users, {updated_places} places")
        
        return {
            "success": True,
            "updated_users": updated_users,
            "updated_places": updated_places,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ Vector updates failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Vector updates failed: {str(e)}"
        )

# ============= ë°°ì¹˜ ì²˜ë¦¬ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ =============

@router.get("/status", response_model=BatchProcessingStatus)
def get_batch_processing_status(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ
    - ë¯¸ì²˜ë¦¬ ì•¡ì…˜ ìˆ˜
    - ìµœì‹  ë°°ì¹˜ ì‘ì—… ìƒíƒœ
    - ë²¡í„° ì—…ë°ì´íŠ¸ í†µê³„
    """
    try:
        # ë¯¸ì²˜ë¦¬ ì•¡ì…˜ ìˆ˜ ì¡°íšŒ
        total_unprocessed = db.query(UserAction).filter(
            UserAction.batch_processed == False
        ).count()
        
        # ê°€ì¥ ì˜¤ë˜ëœ ë¯¸ì²˜ë¦¬ ì•¡ì…˜ ë‚ ì§œ
        oldest_unprocessed = db.query(UserAction.created_at).filter(
            UserAction.batch_processed == False
        ).order_by(UserAction.created_at.asc()).first()
        
        # ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ë²¡í„° ìˆ˜ (ìµœê·¼ 24ì‹œê°„)
        yesterday = datetime.now() - timedelta(days=1)
        
        user_vectors_updated = db.query(UserBehaviorVector).filter(
            UserBehaviorVector.vector_updated_at >= yesterday
        ).count()
        
        place_vectors_updated = db.query(PlaceVector).filter(
            PlaceVector.vector_updated_at >= yesterday
        ).count()
        
        # AWS Batch ì‘ì—… ìƒíƒœ ì¡°íšŒ (ìµœê·¼ 10ê°œ)
        try:
            response = batch_client.list_jobs(
                jobQueue='witple-fargate-queue',
                maxResults=10
            )
            recent_batch_jobs = []
            for job in response.get('jobSummary', []):
                recent_batch_jobs.append({
                    'job_id': job['jobId'],
                    'job_name': job['jobName'],
                    'status': job['jobStatus'],
                    'created_at': job['createdAt'],
                    'started_at': job.get('startedAt'),
                    'stopped_at': job.get('stoppedAt')
                })
        except ClientError as e:
            logger.warning(f"âš ï¸ Could not fetch AWS Batch job status: {e}")
            recent_batch_jobs = []
        
        return BatchProcessingStatus(
            total_unprocessed_actions=total_unprocessed,
            oldest_unprocessed_action=oldest_unprocessed[0] if oldest_unprocessed else None,
            recent_batch_jobs=recent_batch_jobs,
            user_vectors_updated=user_vectors_updated,
            place_vectors_updated=place_vectors_updated
        )
        
    except Exception as e:
        logger.error(f"âŒ Status check failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Status check failed: {str(e)}"
        )

@router.post("/trigger-processing")
def trigger_batch_processing(
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ìˆ˜ë™ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ íŠ¸ë¦¬ê±° (ê´€ë¦¬ììš©)
    ë¯¸ì²˜ë¦¬ ì•¡ì…˜ë“¤ì„ AWS Batchë¡œ ì „ì†¡í•˜ì—¬ ì²˜ë¦¬ ì‹œì‘
    """
    try:
        # ë¯¸ì²˜ë¦¬ ì•¡ì…˜ ìˆ˜ í™•ì¸
        unprocessed_count = db.query(UserAction).filter(
            UserAction.batch_processed == False
        ).count()
        
        if unprocessed_count == 0:
            return {
                "success": True,
                "message": "No unprocessed actions to process",
                "unprocessed_count": 0
            }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°°ì¹˜ ì‘ì—… ì œì¶œ
        background_tasks.add_task(submit_batch_job, unprocessed_count)
        
        logger.info(f"ğŸš€ Manual batch processing triggered for {unprocessed_count} actions")
        
        return {
            "success": True,
            "message": f"Batch processing triggered for {unprocessed_count} actions",
            "unprocessed_count": unprocessed_count
        }
        
    except Exception as e:
        logger.error(f"âŒ Manual batch trigger failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Manual batch trigger failed: {str(e)}"
        )

# ============= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =============

async def process_vector_updates_notification(batch_id: str, processed_records: int):
    """ë°°ì¹˜ ì‘ì—… ì™„ë£Œ í›„ ì¶”ê°€ ì²˜ë¦¬ ì‘ì—… (ë°±ê·¸ë¼ìš´ë“œ)"""
    try:
        logger.info(f"ğŸ”„ Processing vector updates notification for batch {batch_id}")
        
        # TODO: í•„ìš”ì‹œ ì¶”ê°€ ì•Œë¦¼ ë¡œì§ êµ¬í˜„
        # - ê´€ë¦¬ìì—ê²Œ ì´ë©”ì¼ ì•Œë¦¼
        # - ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì— ë©”íŠ¸ë¦­ ì „ì†¡
        # - ìºì‹œ ë¬´íš¨í™” ë“±
        
        logger.info(f"âœ… Vector updates notification processed for {processed_records} records")
        
    except Exception as e:
        logger.error(f"âŒ Vector updates notification failed: {str(e)}")

async def submit_batch_job(unprocessed_count: int):
    """AWS Batch ì‘ì—… ì œì¶œ (ë°±ê·¸ë¼ìš´ë“œ)"""
    try:
        job_name = f"witple-vectorization-{int(datetime.now().timestamp())}"
        
        # ë°°ì¹˜ ì‘ì—… ì œì¶œ
        response = batch_client.submit_job(
            jobName=job_name,
            jobQueue='witple-fargate-queue',
            jobDefinition='witple-vectorization-job',  # ë‚˜ì¤‘ì— ìƒì„±í•  ì‘ì—… ì •ì˜
            parameters={
                'inputBucket': 'user-actions-data',
                'inputPrefix': 'user-actions/',
                'outputTable': 'user_behavior_vectors',
                'maxRecords': str(unprocessed_count)
            }
        )
        
        job_id = response['jobId']
        logger.info(f"ğŸš€ Batch job submitted: {job_name} (ID: {job_id})")
        
        return {
            "job_id": job_id,
            "job_name": job_name,
            "status": "SUBMITTED"
        }
        
    except ClientError as e:
        logger.error(f"âŒ Batch job submission failed: {str(e)}")
        raise e