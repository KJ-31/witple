from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator
import json
import asyncio
import sys
import os
import hashlib
import re

# LLM_RAG.pyë¥¼ ì„í¬íŠ¸í•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

try:
    from LLM_RAG import (
        get_travel_recommendation,
        get_travel_recommendation_optimized,
        get_travel_recommendation_langgraph,
        get_travel_recommendation_stream_async,
        get_travel_recommendation_langgraph_stream,
        llm_cache,
        current_travel_state,
        get_current_travel_state_ref,
        LANGGRAPH_AVAILABLE
    )
    print("âœ… LLM_RAG module imported successfully")
    print(f"ğŸ”§ LangGraph ì‚¬ìš© ê°€ëŠ¥: {LANGGRAPH_AVAILABLE}")
    print(f"ğŸ”§ get_travel_recommendation í•¨ìˆ˜: {get_travel_recommendation is not None}")
    print(f"ğŸ”§ get_travel_recommendation_langgraph í•¨ìˆ˜: {get_travel_recommendation_langgraph is not None}")
    print(f"ğŸ”§ get_travel_recommendation_stream_async í•¨ìˆ˜: {get_travel_recommendation_stream_async is not None}")
    print(f"ğŸ”§ get_travel_recommendation_langgraph_stream í•¨ìˆ˜: {get_travel_recommendation_langgraph_stream is not None}")
except ImportError as e:
    print(f"âŒ Warning: Could not import LLM_RAG module: {e}")
    print("This is likely due to missing dependencies (langchain_aws, boto3, etc.)")
    import traceback
    traceback.print_exc()
    get_travel_recommendation = None
    get_travel_recommendation_optimized = None
    get_travel_recommendation_langgraph = None
    get_travel_recommendation_stream_async = None
    get_travel_recommendation_langgraph_stream = None
    llm_cache = None
    current_travel_state = None
    LANGGRAPH_AVAILABLE = False
except Exception as e:
    print(f"âŒ Error initializing LLM_RAG module: {e}")
    import traceback
    traceback.print_exc()
    get_travel_recommendation = None
    get_travel_recommendation_optimized = None
    get_travel_recommendation_langgraph = None
    get_travel_recommendation_stream_async = None
    get_travel_recommendation_langgraph_stream = None
    llm_cache = None
    current_travel_state = None
    LANGGRAPH_AVAILABLE = False

router = APIRouter()

# í™•ì¥ëœ ìºì‹œ ë©”ì„œë“œ êµ¬í˜„
def _generate_cache_key(query: str, cache_type: str = "response") -> str:
    """ì¿¼ë¦¬ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
    # ì¿¼ë¦¬ ì •ê·œí™” (ê³µë°±, ëŒ€ì†Œë¬¸ì, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
    normalized_query = re.sub(r'\s+', ' ', query.strip().lower())
    normalized_query = re.sub(r'[^\w\sê°€-í£]', '', normalized_query)

    # í•´ì‹œ ìƒì„±
    query_hash = hashlib.md5(normalized_query.encode('utf-8')).hexdigest()[:12]
    return f"llm:{cache_type}:{query_hash}"

def cache_full_response(cache_instance, query: str, response_data: dict, expire: int = 3600) -> bool:
    """ì „ì²´ ChatResponse ë°ì´í„° ìºì‹±"""
    if not cache_instance or not cache_instance.enabled or not response_data:
        return False

    try:
        cache_key = _generate_cache_key(query, "full")
        data_json = json.dumps(response_data, ensure_ascii=False, default=str)
        success = cache_instance.redis.set(cache_key, data_json, ex=expire)

        if success:
            print(f"ğŸ’¾ ì „ì²´ ì‘ë‹µ ìºì‹œ ì €ì¥: {cache_key}")

        return success

    except Exception as e:
        print(f"âš ï¸ ì „ì²´ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

def get_cached_full_response(cache_instance, query: str) -> Optional[dict]:
    """ì „ì²´ ChatResponse ë°ì´í„° ì¡°íšŒ"""
    if not cache_instance or not cache_instance.enabled:
        return None

    try:
        cache_key = _generate_cache_key(query, "full")
        cached_data = cache_instance.redis.get(cache_key)

        if cached_data:
            print(f"ğŸ¯ ì „ì²´ ìºì‹œ íˆíŠ¸: {cache_key}")
            return json.loads(cached_data)
        else:
            print(f"âŒ ì „ì²´ ìºì‹œ ë¯¸ìŠ¤: {cache_key}")
            return None

    except Exception as e:
        print(f"âš ï¸ ì „ì²´ ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None

def process_response_for_frontend(response: str) -> tuple[str, List[str]]:
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì‘ë‹µì„ ì—¬ëŸ¬ í˜•íƒœë¡œ ë³€í™˜"""
    
    # HTML í˜•íƒœ ë³€í™˜ (\n -> <br>)
    response_html = response.replace('\n', '<br>')
    
    # ì¤„ë³„ ë°°ì—´ í˜•íƒœ ë³€í™˜
    response_lines = []
    for line in response.split('\n'):
        # ë¹ˆ ì¤„ì€ ìœ ì§€í•˜ë˜ ê³µë°± ë¬¸ìì—´ë¡œ ë³€í™˜
        response_lines.append(line.strip() if line.strip() else "")
    
    return response_html, response_lines

class ChatMessage(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    success: bool
    error: Optional[str] = None
    travel_plan: Optional[dict] = None  # êµ¬ì¡°í™”ëœ ì—¬í–‰ ì¼ì •
    action_required: Optional[str] = None  # í˜ì´ì§€ ì´ë™ ë“± í•„ìš” ì•¡ì…˜
    formatted_response: Optional[dict] = None  # UIìš© êµ¬ì¡°í™”ëœ ì‘ë‹µ
    response_html: Optional[str] = None  # HTML í˜•íƒœ ì‘ë‹µ (ê°œí–‰ ì²˜ë¦¬)
    response_lines: Optional[List[str]] = None  # ì¤„ë³„ ë°°ì—´ í˜•íƒœ ì‘ë‹µ
    redirect_url: Optional[str] = None  # ë¦¬ë‹¤ì´ë ‰íŠ¸ URL
    places: Optional[List[dict]] = None  # ì§€ë„ í‘œì‹œìš© ì¥ì†Œ ì •ë³´
    

@router.post("/chat", response_model=ChatResponse)
async def chat_with_llm(chat_message: ChatMessage):
    """
    ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LangGraph ê¸°ë°˜ ì—¬í–‰ ì¶”ì²œ ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    LangGraphê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•ŒëŠ” ê¸°ì¡´ RAG ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
    """
    try:
        if get_travel_recommendation is None and get_travel_recommendation_langgraph is None:
            # ëª¨ë“  RAG ì‹œìŠ¤í…œì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ ê¸°ë³¸ ì‘ë‹µ
            default_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì—¬í–‰ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ğŸ“\n\n'{chat_message.message}'ì— ëŒ€í•œ ë‹µë³€ì„ ìœ„í•´ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"
            default_html, default_lines = process_response_for_frontend(default_message)
            
            return ChatResponse(
                response=default_message,
                success=True,
                response_html=default_html,
                response_lines=default_lines
            )
        
        print(f"ğŸ” Processing travel query: {chat_message.message}")

        # ğŸš€ Redis ìºì‹œ í™•ì¸ (ìš°ì„ ìˆœìœ„ 1) - í™•ì • í‚¤ì›Œë“œ ì œì™¸
        confirmation_keywords = ["í™•ì •", "í™•ì •í•´ì¤˜", "í™•ì •í• ê²Œ", "ì´ ì¼ì •ìœ¼ë¡œ í™•ì •", "ë„¤ í™•ì •", "yes", "ok"]
        is_confirmation = any(keyword in chat_message.message.lower() for keyword in confirmation_keywords)

        if llm_cache and llm_cache.enabled and not is_confirmation:
            # í™•ì¥ëœ ìºì‹œ ë°ì´í„° ì¡°íšŒ
            cached_data = get_cached_full_response(llm_cache, chat_message.message)
            if cached_data:
                print("âš¡ ìºì‹œëœ ì „ì²´ ì‘ë‹µ ë°˜í™˜!")

                # ìºì‹œëœ ë°ì´í„°ì—ì„œ ì—¬í–‰ ìƒíƒœ ë³µì›
                if cached_data.get('travel_plan') and current_travel_state is not None:
                    import time
                    current_travel_state.update({
                        "last_query": chat_message.message,
                        "travel_plan": cached_data['travel_plan'],
                        "places": cached_data.get('places', []),
                        "context": cached_data.get('response', ''),
                        "timestamp": time.time()
                    })
                    print("ğŸ“‹ ìºì‹œëœ ì‘ë‹µìœ¼ë¡œ ì—¬í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

                return ChatResponse(
                    response=cached_data.get('response', ''),
                    success=True,
                    travel_plan=cached_data.get('travel_plan'),
                    action_required=cached_data.get('action_required'),
                    formatted_response=cached_data.get('formatted_response'),
                    response_html=cached_data.get('response_html', ''),
                    response_lines=cached_data.get('response_lines', []),
                    redirect_url=cached_data.get('redirect_url'),
                    places=cached_data.get('places')
                )

            # ê¸°ì¡´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìºì‹œ í´ë°±
            cached_response = llm_cache.get_cached_response(chat_message.message)
            if cached_response:
                print("âš¡ ìºì‹œëœ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜!")
                response_html, response_lines = process_response_for_frontend(cached_response)

                return ChatResponse(
                    response=cached_response,
                    success=True,
                    response_html=response_html,
                    response_lines=response_lines
                )

        # LangGraph ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìš°ì„  ì‚¬ìš©
        if LANGGRAPH_AVAILABLE and get_travel_recommendation_langgraph:
            print("ğŸš€ Using LangGraph workflow for enhanced travel recommendation")
            
            # ê°„ë‹¨í•œ ì„¸ì…˜ ID (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ìë³„ ê³ ìœ  ID ì‚¬ìš©)
            # í˜„ì¬ëŠ” ë°ëª¨ìš©ìœ¼ë¡œ ê³ ì • ì„¸ì…˜ ID ì‚¬ìš©
            session_id = "demo_session"
            
            result = get_travel_recommendation_langgraph(chat_message.message, session_id=session_id)

            print(f"âœ… LangGraph result: {result.get('response', '')[:100]}...")

            response_text = result.get('response', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            response_html, response_lines = process_response_for_frontend(response_text)

            # tool_resultsì—ì„œ redirect_urlê³¼ places ì •ë³´ ì¶”ì¶œ
            tool_results = result.get('raw_state', {}).get('tool_results', {})

            # ğŸ’¾ ì‘ë‹µ ìºì‹± (LangGraph ê²°ê³¼ - í™•ì¥ëœ ë°ì´í„°)
            if llm_cache and llm_cache.enabled and response_text and result.get('success', True):
                # ì „ì²´ ChatResponse ë°ì´í„° êµ¬ì„±
                full_response_data = {
                    'response': response_text,
                    'travel_plan': result.get('travel_plan', {}),
                    'action_required': result.get('action_required'),
                    'formatted_response': result.get('raw_state', {}).get('formatted_ui_response'),
                    'response_html': response_html,
                    'response_lines': response_lines,
                    'redirect_url': tool_results.get('redirect_url'),
                    'places': tool_results.get('places')
                }

                # í™•ì¥ëœ ìºì‹œ ì €ì¥
                cache_full_response(llm_cache, chat_message.message, full_response_data, expire=3600)  # 1ì‹œê°„

                # ê¸°ì¡´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìºì‹œë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì €ì¥
                llm_cache.cache_response(chat_message.message, response_text, expire=3600)  # 1ì‹œê°„
                print("ğŸ’¾ LangGraph í™•ì¥ ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ")

            return ChatResponse(
                response=response_text,
                success=result.get('success', True),
                travel_plan=result.get('travel_plan', {}),
                action_required=result.get('action_required'),
                error=result.get('error'),
                formatted_response=result.get('raw_state', {}).get('formatted_ui_response'),
                response_html=response_html,
                response_lines=response_lines,
                redirect_url=tool_results.get('redirect_url'),
                places=tool_results.get('places')
            )
        
        # LangGraph ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
        else:
            print("âš ï¸ LangGraph ì‚¬ìš© ë¶ˆê°€ëŠ¥, ê³ ì† RAG ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±")

            # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ë¹ ë¥¸ RAG í˜¸ì¶œ
            import asyncio
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(get_travel_recommendation, chat_message.message, False),
                    timeout=60.0  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ (ì´ˆê¸° ë¡œë”© ê³ ë ¤)
                )
                print(f"âœ… Got fast RAG response: {response[:100]}..." if len(response) > 100 else f"âœ… Got response: {response}")

                # ğŸ’¾ ì‘ë‹µ ìºì‹± (ê¸°ì¡´ RAG ê²°ê³¼)
                if llm_cache and llm_cache.enabled and response:
                    llm_cache.cache_response(chat_message.message, response, expire=3600)  # 1ì‹œê°„
                    print("ğŸ’¾ RAG ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ")

            except asyncio.TimeoutError:
                response = "â° ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                print("âŒ RAG response timeout")

            response_html, response_lines = process_response_for_frontend(response)

            return ChatResponse(
                response=response,
                success=True,
                response_html=response_html,
                response_lines=response_lines
            )
        
    except Exception as e:
        print(f"âŒ Chat API error: {e}")
        import traceback
        traceback.print_exc()
        error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        error_html, error_lines = process_response_for_frontend(error_message)
        
        return ChatResponse(
            response=error_message,
            success=False,
            error=str(e),
            response_html=error_html,
            response_lines=error_lines
        )

@router.get("/chat/health")
async def chat_health():
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        # Redis ìƒíƒœ í™•ì¸
        redis_status = "disconnected"
        redis_info = {}
        if llm_cache:
            try:
                cache_stats = llm_cache.get_cache_stats()
                redis_status = "connected" if cache_stats.get("enabled") else "disabled"
                redis_info = cache_stats
            except Exception as e:
                redis_status = f"error: {str(e)}"

        # LLM ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        llm_status = "healthy"
        if get_travel_recommendation is None:
            llm_status = "unhealthy"
            return {
                "status": "unhealthy",
                "message": "LLM RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ",
                "redis_status": redis_status,
                "redis_info": redis_info
            }

        return {
            "status": "healthy",
            "message": "LLM RAG ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘",
            "redis_status": redis_status,
            "redis_info": redis_info,
            "llm_status": llm_status,
            "langgraph_available": LANGGRAPH_AVAILABLE
        }

    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"LLM RAG ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}",
            "redis_status": "unknown",
            "redis_info": {}
        }

class ScheduleItem(BaseModel):
    """ì¼ì • í•­ëª© ëª¨ë¸"""
    time: str
    place_name: str
    description: str
    category: Optional[str] = None
    place_info: Optional[dict] = None

class DayItinerary(BaseModel):
    """í•˜ë£¨ ì¼ì • ëª¨ë¸"""
    day: int
    schedule: List[ScheduleItem] = []

class PlaceInfo(BaseModel):
    """ì¥ì†Œ ì •ë³´ ëª¨ë¸"""
    name: str
    category: Optional[str] = None
    region: Optional[str] = None
    city: Optional[str] = None
    description: Optional[str] = None
    similarity_score: Optional[float] = None

class TravelPlanData(BaseModel):
    """í™•ì¥ëœ ì—¬í–‰ ì¼ì • ë°ì´í„° ëª¨ë¸"""
    region: str
    cities: List[str] = []
    duration: str
    categories: List[str] = []
    itinerary: List[DayItinerary] = []
    places: List[PlaceInfo] = []
    raw_response: str
    status: str
    plan_id: Optional[str] = None
    created_at: Optional[str] = None
    total_places: Optional[int] = None
    confidence_score: Optional[float] = None

class TravelPlanResponse(BaseModel):
    """ì—¬í–‰ ì¼ì • ì‘ë‹µ ëª¨ë¸"""
    success: bool
    message: str
    plan_id: Optional[str] = None
    redirect_url: Optional[str] = None
    error: Optional[str] = None

@router.post("/chat/confirm-plan", response_model=TravelPlanResponse)
async def confirm_travel_plan(plan_data: TravelPlanData):
    """
    í™•ì •ëœ ì—¬í–‰ ì¼ì •ì„ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ê³  ì¼ì • ìƒì„± í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸í•  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        print(f"ğŸ‰ ì—¬í–‰ ì¼ì • í™•ì • ìš”ì²­: {plan_data.region} {plan_data.duration}")
        
        # plan_idê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        plan_id = plan_data.plan_id
        if not plan_id:
            import uuid
            import time
            timestamp = str(int(time.time()))[-6:]
            unique_id = str(uuid.uuid4())[:8]
            plan_id = f"plan_{timestamp}_{unique_id}"
        
        # ì—¬í–‰ ì¼ì • ë°ì´í„° ê²€ì¦
        if not plan_data.region or not plan_data.duration:
            raise HTTPException(
                status_code=400, 
                detail="ì—¬í–‰ ì§€ì—­ê³¼ ê¸°ê°„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤."
            )
        
        # í™•ì •ëœ ì¼ì • ë°ì´í„° êµ¬ì„±
        confirmed_data = {
            "plan_id": plan_id,
            "region": plan_data.region,
            "duration": plan_data.duration,
            "itinerary": [item.dict() for item in plan_data.itinerary],
            "places": [place.dict() for place in plan_data.places],
            "categories": plan_data.categories,
            "status": "confirmed",
            "confirmed_at": plan_data.created_at,
            "total_places": plan_data.total_places,
            "confidence_score": plan_data.confidence_score
        }
        
        # URL íŒŒë¼ë¯¸í„° êµ¬ì„±
        url_params = f"plan_id={plan_id}&region={plan_data.region}&duration={plan_data.duration}"
        if plan_data.cities:
            url_params += f"&cities={','.join(plan_data.cities)}"
        
        redirect_url = f"/travel-planning?{url_params}"
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì¼ì • ì €ì¥
        # save_travel_plan_to_db(confirmed_data, user_id)
        
        print(f"âœ… ì—¬í–‰ ì¼ì • ì €ì¥ ì™„ë£Œ. Plan ID: {plan_id}")
        
        # ìƒì„¸ í™•ì • ë©”ì‹œì§€ ìƒì„±
        places_summary = ""
        if plan_data.places:
            place_names = [place.name for place in plan_data.places[:3]]
            places_summary = f"ì£¼ìš” ë°©ë¬¸ì§€: {', '.join(place_names)}"
            if len(plan_data.places) > 3:
                places_summary += f" ì™¸ {len(plan_data.places) - 3}ê³³"
        
        confirmation_message = f"""
ğŸ‰ **{plan_data.region} {plan_data.duration} ì—¬í–‰ ì¼ì •ì´ í™•ì •ë˜ì—ˆìŠµë‹ˆë‹¤!**

ğŸ“‹ **í™•ì • ì •ë³´:**
â€¢ ì¼ì • ìˆ˜: {len(plan_data.itinerary)}ì¼
â€¢ {places_summary}
â€¢ ì‹ ë¢°ë„: {f"{plan_data.confidence_score:.2f}" if plan_data.confidence_score else "N/A"}

âœˆï¸ ì—¬í–‰ ê³„íš í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ì„¸ë¶€ ì¡°ì •ì„ ì§„í–‰í•˜ì„¸ìš”!
        """.strip()
        
        return TravelPlanResponse(
            success=True,
            message=confirmation_message,
            plan_id=plan_id,
            redirect_url=redirect_url
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ì—¬í–‰ ì¼ì • í™•ì • ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return TravelPlanResponse(
            success=False,
            message="ì—¬í–‰ ì¼ì • í™•ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            error=str(e)
        )

@router.get("/chat/cache/stats")
async def get_cache_stats():
    """
    Redis ìºì‹œ í†µê³„ ì¡°íšŒ
    """
    try:
        if llm_cache and llm_cache.enabled:
            stats = llm_cache.get_cache_stats()
            return {
                "success": True,
                "cache_stats": stats
            }
        else:
            return {
                "success": False,
                "message": "ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "cache_stats": {"enabled": False}
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"ìºì‹œ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}",
            "cache_stats": {"enabled": False, "error": str(e)}
        }

@router.post("/chat/cache/clear")
async def clear_cache():
    """
    LLM ìºì‹œ ì´ˆê¸°í™” (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
    """
    try:
        if llm_cache and llm_cache.enabled:
            # LLM ê´€ë ¨ í‚¤ë§Œ ì‚­ì œ
            llm_keys = llm_cache.redis.keys("llm:*")
            if llm_keys:
                deleted_count = llm_cache.redis.delete(*llm_keys)
                return {
                    "success": True,
                    "message": f"{deleted_count}ê°œì˜ ìºì‹œ í‚¤ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            else:
                return {
                    "success": True,
                    "message": "ì‚­ì œí•  ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤."
                }
        else:
            return {
                "success": False,
                "message": "ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"ìºì‹œ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}"
        }

@router.post("/chat/cache/benchmark")
async def benchmark_cache_performance():
    """
    ìºì‹œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    """
    try:
        if not llm_cache or not llm_cache.enabled:
            return {
                "success": False,
                "message": "ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            }

        import time

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ë¶€ì‚° 2ë°• 3ì¼ ì—¬í–‰ ì¶”ì²œ",
            "ì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ",
            "ì„œìš¸ í•«í”Œë ˆì´ìŠ¤",
            "ê°•ë¦‰ ë°”ë‹¤ ì—¬í–‰",
            "ê²½ì£¼ ì—­ì‚¬ ì—¬í–‰"
        ]

        benchmark_results = {
            "cache_enabled": True,
            "test_results": [],
            "summary": {}
        }

        total_cache_hits = 0
        total_cache_misses = 0
        cache_time_total = 0
        miss_time_total = 0

        for query in test_queries:
            # ìºì‹œ ë¯¸ìŠ¤ í…ŒìŠ¤íŠ¸ (ìºì‹œ ì´ˆê¸°í™” í›„)
            cache_key = llm_cache._generate_cache_key(query)
            llm_cache.redis.delete(cache_key)

            miss_start = time.time()
            cached_response = llm_cache.get_cached_response(query)
            miss_time = time.time() - miss_start

            # í…ŒìŠ¤íŠ¸ ì‘ë‹µ ìºì‹±
            test_response = f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {query}ì— ëŒ€í•œ ìƒ˜í”Œ ì—¬í–‰ ì¶”ì²œì…ë‹ˆë‹¤."
            llm_cache.cache_response(query, test_response, expire=300)  # 5ë¶„

            # ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸
            hit_start = time.time()
            cached_response = llm_cache.get_cached_response(query)
            hit_time = time.time() - hit_start

            if cached_response:
                total_cache_hits += 1
                cache_time_total += hit_time
            else:
                total_cache_misses += 1
                miss_time_total += miss_time

            benchmark_results["test_results"].append({
                "query": query,
                "cache_hit": cached_response is not None,
                "hit_time_ms": round(hit_time * 1000, 2),
                "miss_time_ms": round(miss_time * 1000, 2),
                "speedup": round(miss_time / hit_time, 1) if hit_time > 0 else 0
            })

        # ìš”ì•½ í†µê³„
        avg_cache_time = (cache_time_total / total_cache_hits) if total_cache_hits > 0 else 0
        avg_miss_time = (miss_time_total / total_cache_misses) if total_cache_misses > 0 else 0

        benchmark_results["summary"] = {
            "total_tests": len(test_queries),
            "cache_hits": total_cache_hits,
            "cache_misses": total_cache_misses,
            "avg_cache_time_ms": round(avg_cache_time * 1000, 2),
            "avg_miss_time_ms": round(avg_miss_time * 1000, 2),
            "average_speedup": round(avg_miss_time / avg_cache_time, 1) if avg_cache_time > 0 else 0,
            "cache_hit_rate": f"{(total_cache_hits / len(test_queries)) * 100:.1f}%"
        }

        return {
            "success": True,
            "benchmark": benchmark_results
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"
        }

@router.get("/chat/current-state")
async def get_current_travel_state():
    """
    í˜„ì¬ ì—¬í–‰ ìƒíƒœ ì¡°íšŒ (ìƒˆ ì¶”ì²œì‹œ ë®ì–´ì“°ê¸° ë°©ì‹)
    """
    try:
        # í•¨ìˆ˜ë¥¼ í†µí•´ ìµœì‹  ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        if get_current_travel_state_ref is None:
            return {
                "success": False,
                "message": "ì—¬í–‰ ìƒíƒœ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        # í˜„ì¬ ìƒíƒœ ë°˜í™˜
        state_copy = get_current_travel_state_ref().copy()

        return {
            "success": True,
            "current_state": state_copy,
            "has_travel_plan": bool(state_copy.get("travel_plan")),
            "places_count": len(state_copy.get("places", [])),
            "last_query": state_copy.get("last_query", ""),
            "timestamp": state_copy.get("timestamp")
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
        }

@router.post("/chat/clear-state")
async def clear_current_travel_state():
    """
    í˜„ì¬ ì—¬í–‰ ìƒíƒœ ì´ˆê¸°í™”
    """
    try:
        if current_travel_state is None:
            return {
                "success": False,
                "message": "ì—¬í–‰ ìƒíƒœ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        # ìƒíƒœ ì´ˆê¸°í™”
        current_travel_state.clear()
        current_travel_state.update({
            "last_query": "",
            "travel_plan": {},
            "places": [],
            "context": "",
            "timestamp": None
        })

        return {
            "success": True,
            "message": "ì—¬í–‰ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"ìƒíƒœ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}"
        }

@router.get("/chat/travel-plan/{plan_id}")
async def get_travel_plan(plan_id: str):
    """
    ì €ì¥ëœ ì—¬í–‰ ì¼ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        # plan_data = get_travel_plan_from_db(plan_id)
        
        # í˜„ì¬ëŠ” ëª¨ì˜ ë°ì´í„° ë°˜í™˜
        mock_plan = {
            "plan_id": plan_id,
            "region": "ì œì£¼ë„",
            "duration": "2ë°• 3ì¼",
            "locations": ["í•œë¼ì‚°", "ì„±ì‚°ì¼ì¶œë´‰", "í˜‘ì¬í•´ìˆ˜ìš•ì¥"],
            "status": "confirmed",
            "created_at": "2025-09-13",
            "message": "ê³„íšëœ ì—¬í–‰ ì¼ì •ì…ë‹ˆë‹¤."
        }
        
        return {
            "success": True,
            "data": mock_plan
        }
        
    except Exception as e:
        print(f"âŒ ì—¬í–‰ ì¼ì • ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=404, 
            detail=f"ì—¬í–‰ ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Plan ID: {plan_id}"
        )