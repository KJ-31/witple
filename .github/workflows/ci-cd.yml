name: CI/CD Pipeline - Full Stack

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ap-northeast-2
  ECR_REPOSITORY_BACKEND: witple-backend
  ECR_REPOSITORY_FRONTEND: witple-frontend
  EKS_CLUSTER_NAME: witple-cluster
  EKS_NAMESPACE: witple

permissions:
  id-token: write   # OIDC í† í° ìƒì„± ê¶Œí•œ
  contents: read    # ì½”ë“œ ì½ê¸° ê¶Œí•œ

jobs:
  # ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸
  test-backend:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install Dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run Tests
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379
        SECRET_KEY: test-secret-key
      run: |
        cd backend
        echo "Running backend tests..."
        pytest

  # í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸
  test-frontend:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Cache npm dependencies
      uses: actions/cache@v3
      with:
        path: ~/.npm
        key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
    
    - name: Install Dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run Tests
      run: |
        cd frontend
        npm run lint
        # npm run test (í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¶”ê°€ ì‹œ)

  # ë°±ì—”ë“œ ë¹Œë“œ ë° í‘¸ì‹œ
  build-backend:
    needs: test-backend
    if: ${{ github.ref == 'refs/heads/main' }}
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS ìê²© ì¦ëª… êµ¬ì„± (OIDC ë°©ì‹)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-backend-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR ë¡œê·¸ì¸
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # ë°±ì—”ë“œ ë¹Œë“œ ë° í‘¸ì‹œ
    - name: Build and push Backend image
      run: |
        cd backend
        docker build -t witple-backend .
        docker tag witple-backend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}:latest
        docker tag witple-backend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}:${{ github.sha }}
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_BACKEND }}:${{ github.sha }}

  # í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ ë° í‘¸ì‹œ
  build-frontend:
    needs: test-frontend
    if: ${{ github.ref == 'refs/heads/main' }}
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS ìê²© ì¦ëª… êµ¬ì„± (OIDC ë°©ì‹)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-frontend-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR ë¡œê·¸ì¸
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ ë° í‘¸ì‹œ
    - name: Build and push Frontend image
      run: |
        cd frontend
        docker build -t witple-frontend .
        docker tag witple-frontend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_FRONTEND }}:latest
        docker tag witple-frontend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_FRONTEND }}:${{ github.sha }}
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_FRONTEND }}:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_FRONTEND }}:${{ github.sha }}

  # í†µí•© ë°°í¬
  deploy:
    needs: [build-backend, build-frontend]
    if: ${{ github.ref == 'refs/heads/main' && always() && !cancelled() && !failure() }}
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS ìê²© ì¦ëª… êµ¬ì„± (OIDC ë°©ì‹)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-deploy-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR ë¡œê·¸ì¸
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # EKS ì„¤ì • ë° Access Entries ì ìš©
    - name: Configure EKS access
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        aws sts get-caller-identity
        
        # Check if EKS cluster is accessible via AWS CLI
        echo "Checking EKS cluster accessibility..."
        if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /dev/null 2>&1; then
          echo "EKS cluster is accessible via AWS CLI"
        else
          echo "Cannot access EKS cluster via AWS CLI"
          exit 1
        fi
        
        # Get current AWS identity
        CURRENT_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
        echo "Current AWS identity: $CURRENT_ARN"
        
        # Create EKS Access Entry for GitHub Actions role (if not exists)
        echo "Creating EKS Access Entry for GitHub Actions role..."
        aws eks create-access-entry \
          --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
          --region ${{ env.AWS_REGION }} \
          --principal-arn "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role" \
          --type STANDARD || {
          echo "Access entry creation failed or already exists"
        }
        
        # Associate access policy to the access entry
        echo "Associating cluster admin policy to access entry..."
        aws eks associate-access-policy \
          --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
          --region ${{ env.AWS_REGION }} \
          --principal-arn "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role" \
          --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
          --access-scope type=cluster || {
          echo "Policy association failed or already exists"
        }
        
        # Configure kubectl authentication with AWS CLI
        echo "Configuring kubectl authentication..."
        aws eks get-token --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        
        # Test kubectl access
        echo "Testing kubectl access..."
        kubectl get nodes
    
    # AWS Load Balancer Controller ServiceAccount ìƒì„±
    - name: Create AWS Load Balancer Controller ServiceAccount
      run: |
        cat > aws-load-balancer-controller-sa.yaml << EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          annotations:
            eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/aws-load-balancer-controller
        EOF
        kubectl apply -f aws-load-balancer-controller-sa.yaml --validate=false
    
    # Helm ì„¤ì¹˜
    - name: Install Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        helm version
    
    # AWS Load Balancer Controller ì„¤ì¹˜
    - name: Install AWS Load Balancer Controller
      run: |
        # Helmì„ ì‚¬ìš©í•˜ì—¬ AWS Load Balancer Controller ì„¤ì¹˜
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # VPC ID ê°€ì ¸ì˜¤ê¸°
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        echo "VPC ID: $VPC_ID"
        
        # AWS Load Balancer Controller ì„¤ì¹˜ (ê¸°ì¡´ ì„¤ì¹˜ê°€ ìˆë‹¤ë©´ ì—…ê·¸ë ˆì´ë“œ)
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n kube-system \
          --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
          --set serviceAccount.create=false \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$VPC_ID \
          --wait --timeout=300s || echo "ALB Controller might already be installed"
        
        # Controllerê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s || echo "ALB Controller pods might already be ready"
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    - name: Set environment variables
      run: |
        echo "ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_ENV
        echo "ECR_REPOSITORY_BACKEND=${{ env.ECR_REPOSITORY_BACKEND }}" >> $GITHUB_ENV
        echo "ECR_REPOSITORY_FRONTEND=${{ env.ECR_REPOSITORY_FRONTEND }}" >> $GITHUB_ENV
        echo "SECRET_KEY=${{ secrets.SECRET_KEY }}" >> $GITHUB_ENV
        
        # DOMAIN_NAME ì²˜ë¦¬ (ë¹„ì–´ìˆìœ¼ë©´ ALB DNSë§Œ ì‚¬ìš©)
        DOMAIN_NAME="${{ secrets.DOMAIN_NAME }}"
        if [ -z "$DOMAIN_NAME" ] || [ "$DOMAIN_NAME" = "" ]; then
          echo "DOMAIN_NAME is empty, will use ALB DNS only"
          echo "USE_DOMAIN=false" >> $GITHUB_ENV
          echo "DOMAIN_NAME=" >> $GITHUB_ENV
        else
          echo "DOMAIN_NAME=$DOMAIN_NAME" >> $GITHUB_ENV
          echo "USE_DOMAIN=true" >> $GITHUB_ENV
        fi
        
        # VPC ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Ingressìš©)
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        PUBLIC_SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=*public*" "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
        ALB_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*alb*" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text)
        
        echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
        echo "PUBLIC_SUBNET_IDS=$PUBLIC_SUBNET_IDS" >> $GITHUB_ENV
        echo "ALB_SECURITY_GROUP_ID=$ALB_SECURITY_GROUP_ID" >> $GITHUB_ENV
        
        # ACM ì¸ì¦ì„œ ARN ì„¤ì • (HTTPSìš©)
        echo "CERTIFICATE_ARN=${{ secrets.CERTIFICATE_ARN }}" >> $GITHUB_ENV
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ë° ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ìƒì„±
    - name: Create namespace and database secret
      run: |
        echo "Creating namespace..."
        kubectl apply -f k8s/namespace.yaml
        
        echo "Creating database secret..."
        
        # kubectlì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì‹œí¬ë¦¿ ìƒì„± (ê¸°ì¡´ ì‹œí¬ë¦¿ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±)
        kubectl delete secret db-secret -n ${{ env.EKS_NAMESPACE }} --ignore-not-found=true
        kubectl delete secret app-secret -n ${{ env.EKS_NAMESPACE }} --ignore-not-found=true
        
        kubectl create secret generic db-secret \
          --namespace=${{ env.EKS_NAMESPACE }} \
          --from-literal=DATABASE_URL="${{ secrets.DATABASE_URL }}" \
          --from-literal=REDIS_URL="${{ secrets.REDIS_URL }}"
        
        kubectl create secret generic app-secret \
          --namespace=${{ env.EKS_NAMESPACE }} \
          --from-literal=SECRET_KEY="${{ secrets.SECRET_KEY }}"
        
        echo "Secrets created successfully"
        
        # ECR Pull Secret ìƒì„± (ì´ë¯¸ì§€ Pullì„ ìœ„í•´)
        echo "Creating ECR pull secret..."
        kubectl delete secret regcred -n ${{ env.EKS_NAMESPACE }} --ignore-not-found=true
        
        kubectl create secret docker-registry regcred \
          --namespace=${{ env.EKS_NAMESPACE }} \
          --docker-server=${{ steps.login-ecr.outputs.registry }} \
          --docker-username=AWS \
          --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})
        
        echo "ECR pull secret created successfully"
    
    # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš©
    - name: Deploy to EKS
      run: |
        # í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜
        envsubst < k8s/configmap.yaml > k8s/configmap-generated.yaml
        
        # ê³µí†µ ë¦¬ì†ŒìŠ¤ ë°°í¬
        echo "Applying shared resources..."
        kubectl apply -f k8s/namespace.yaml
        kubectl apply -f k8s/configmap-generated.yaml
        kubectl apply -f k8s/hpa.yaml
        
        # ServiceAccount ë°°í¬
        echo "Applying ServiceAccount..."
        kubectl apply -f k8s/backend/serviceaccount.yaml
        
        # Ingress ìƒì„± (ë„ë©”ì¸ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        echo "=== Ingress Generation ==="
        echo "USE_DOMAIN: $USE_DOMAIN"
        echo "DOMAIN_NAME: $DOMAIN_NAME"
        
        if [ "$USE_DOMAIN" = "true" ]; then
          echo "Creating Ingress with domain: $DOMAIN_NAME"
          envsubst < k8s/ingress.yaml > k8s/ingress-generated.yaml
        else
          echo "Creating Ingress without domain (ALB DNS only)"
          cat > k8s/ingress-generated.yaml << 'EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: witple-ingress
          namespace: witple
          annotations:
            kubernetes.io/ingress.class: "alb"
            alb.ingress.kubernetes.io/scheme: "internet-facing"
            alb.ingress.kubernetes.io/target-type: "ip"
            alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
        EOF
          # í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜ìœ¼ë¡œ ì„œë¸Œë„·ê³¼ ë³´ì•ˆ ê·¸ë£¹ ì¶”ê°€
          cat >> k8s/ingress-generated.yaml << EOF
            alb.ingress.kubernetes.io/subnets: "${PUBLIC_SUBNET_IDS}"
            alb.ingress.kubernetes.io/security-groups: "${ALB_SECURITY_GROUP_ID}"
        spec:
          ingressClassName: alb
          rules:
          - http:
              paths:
              - path: /api
                pathType: Prefix
                backend:
                  service:
                    name: witple-backend-service
                    port:
                      number: 80
              - path: /
                pathType: Prefix
                backend:
                  service:
                    name: witple-frontend-service
                    port:
                      number: 80
        EOF
        fi
        
        # ë°°í¬ (ê° ë‹¨ê³„ë³„ë¡œ í™•ì¸)
        echo "Applying ConfigMap..."
        kubectl apply -f k8s/configmap-generated.yaml
        
        # IMAGE_TAG ì„¤ì • (ì»¤ë°‹ SHA íƒœê·¸)
        export IMAGE_TAG=${GITHUB_SHA}

        # ë°±ì—”ë“œ ë°°í¬ (Rolling Update ì‚¬ìš©)
        echo "Deploying backend (rolling update)..."
        
        # ë°±ì—”ë“œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜
        envsubst < k8s/backend/deployment.yaml > k8s/backend/deployment-generated.yaml
        envsubst < k8s/backend/service.yaml > k8s/backend/service-generated.yaml
        
        # ë°±ì—”ë“œ ë°°í¬ (Rolling Update)
        kubectl apply -f k8s/backend/deployment-generated.yaml
        kubectl apply -f k8s/backend/service-generated.yaml
        
        # í”„ë¡ íŠ¸ì—”ë“œ ë°°í¬
        echo "Deploying frontend..."
        
        # í”„ë¡ íŠ¸ì—”ë“œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜
        envsubst < k8s/frontend/deployment.yaml > k8s/frontend/deployment-generated.yaml
        envsubst < k8s/frontend/service.yaml > k8s/frontend/service-generated.yaml
        if [ "$USE_DOMAIN" = "true" ]; then
          echo "Creating frontend Ingress with HTTPS (domain mode)"
          envsubst < k8s/frontend/ingress.yaml > k8s/frontend/ingress-generated.yaml
        else
          echo "Creating frontend Ingress with HTTP only (no domain mode)"
          # HTTP ì „ìš© Ingress ìƒì„± (ë„ë©”ì¸ ì—†ìŒ)
          {
            echo "apiVersion: networking.k8s.io/v1"
            echo "kind: Ingress"
            echo "metadata:"
            echo "  name: witple-frontend-ingress"
            echo "  namespace: witple"
            echo "  annotations:"
            echo "    kubernetes.io/ingress.class: \"alb\""
            echo "    alb.ingress.kubernetes.io/scheme: \"internet-facing\""
            echo "    alb.ingress.kubernetes.io/target-type: \"ip\""
            echo "    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}]'"
            echo "    alb.ingress.kubernetes.io/subnets: \"${PUBLIC_SUBNET_IDS}\""
            echo "    alb.ingress.kubernetes.io/security-groups: \"${ALB_SECURITY_GROUP_ID}\""
            echo "    alb.ingress.kubernetes.io/healthcheck-path: \"/\""
            echo "    alb.ingress.kubernetes.io/healthcheck-port: \"3000\""
            echo "    alb.ingress.kubernetes.io/healthcheck-protocol: \"HTTP\""
            echo "    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \"10\""
            echo "    alb.ingress.kubernetes.io/healthcheck-interval-seconds: \"15\""
            echo "    alb.ingress.kubernetes.io/healthy-threshold-count: \"2\""
            echo "    alb.ingress.kubernetes.io/unhealthy-threshold-count: \"5\""
            echo "    alb.ingress.kubernetes.io/success-codes: \"200\""
            echo "spec:"
            echo "  ingressClassName: alb"
            echo "  rules:"
            echo "  - http:"
            echo "      paths:"
            echo "      - path: /"
            echo "        pathType: Prefix"
            echo "        backend:"
            echo "          service:"
            echo "            name: witple-frontend-service"
            echo "            port:"
            echo "              number: 80"
          } > k8s/frontend/ingress-generated.yaml
        fi
        
        # í”„ë¡ íŠ¸ì—”ë“œ ë°°í¬
        kubectl apply -f k8s/frontend/deployment-generated.yaml
        kubectl apply -f k8s/frontend/service-generated.yaml
        kubectl apply -f k8s/frontend/ingress-generated.yaml
        
        # Ingress ìƒíƒœ í™•ì¸
        echo "Checking frontend Ingress status..."
        kubectl get ingress witple-frontend-ingress -n ${{ env.EKS_NAMESPACE }} || echo "Frontend Ingress not found"
        
        # ë°°í¬ ìƒíƒœ í™•ì¸ (ë³‘ë ¬ë¡œ ì²´í¬)
        echo "Waiting for deployments to be ready..."
        
        # ë°±ì—”ë“œ ë°°í¬ ìƒíƒœ í™•ì¸
        echo "Checking backend deployment..."
        if ! kubectl rollout status deployment/witple-backend -n ${{ env.EKS_NAMESPACE }} --timeout=300s; then
          echo "Backend deployment failed! Debugging..."
          kubectl describe deployment witple-backend -n ${{ env.EKS_NAMESPACE }}
          kubectl get pods -n ${{ env.EKS_NAMESPACE }} -l app=witple-backend
          for pod in $(kubectl get pods -n ${{ env.EKS_NAMESPACE }} -l app=witple-backend -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Pod: $pod ---"
            kubectl logs $pod -n ${{ env.EKS_NAMESPACE }} --tail=100 || echo "No logs available for $pod"
            kubectl describe pod $pod -n ${{ env.EKS_NAMESPACE }}
          done
          exit 1
        fi
        echo "âœ… Backend deployment successful"
        
        # í”„ë¡ íŠ¸ì—”ë“œ ë°°í¬ ìƒíƒœ í™•ì¸
        echo "Checking frontend deployment..."
        if ! kubectl rollout status deployment/witple-frontend -n ${{ env.EKS_NAMESPACE }} --timeout=300s; then
          echo "Frontend deployment failed! Debugging..."
          kubectl describe deployment witple-frontend -n ${{ env.EKS_NAMESPACE }}
          kubectl get pods -n ${{ env.EKS_NAMESPACE }} -l app=witple-frontend
          for pod in $(kubectl get pods -n ${{ env.EKS_NAMESPACE }} -l app=witple-frontend -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Pod: $pod ---"
            kubectl logs $pod -n ${{ env.EKS_NAMESPACE }} --tail=100 || echo "No logs available for $pod"
            kubectl describe pod $pod -n ${{ env.EKS_NAMESPACE }}
          done
          exit 1
        fi
        echo "âœ… Frontend deployment successful"
        
        echo "Checking pods status..."
        kubectl get pods -n ${{ env.EKS_NAMESPACE }}
        
        # í”„ë¡ íŠ¸ì—”ë“œ ALB DNS í™•ì¸
        echo "Waiting for frontend Ingress to be ready..."
        FRONTEND_ALB_DNS=""
        for i in {1..60}; do
          echo "Attempt $i/60: Checking frontend Ingress status..."
          FRONTEND_ALB_DNS=$(kubectl get ingress witple-frontend-ingress -n ${{ env.EKS_NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$FRONTEND_ALB_DNS" ]; then
            echo "âœ… Frontend Ingress is ready! ALB DNS: $FRONTEND_ALB_DNS"
            break
          fi
          if [ $i -eq 60 ]; then
            echo "âŒ Frontend Ingress failed to become ready after 10 minutes"
            kubectl describe ingress witple-frontend-ingress -n ${{ env.EKS_NAMESPACE }}
            exit 1
          fi
          echo "â³ Frontend Ingress not ready yet, waiting 10 seconds..."
          sleep 10
        done
        
        # ë„ë©”ì¸ ì„¤ì •ì— ë”°ë¥¸ URL ì¶œë ¥
        echo "=== Deployment URLs ==="
        
        if [ "$USE_DOMAIN" = "true" ]; then
          echo "ğŸŒ Application URL: https://$DOMAIN_NAME"
          echo "ğŸ”— Backend API URL: https://$DOMAIN_NAME/api (proxied through frontend)"
        else
          echo "ğŸŒ Application URL: http://$FRONTEND_ALB_DNS"
          echo "ğŸ”— Backend API URL: http://$FRONTEND_ALB_DNS/api (proxied through frontend)"
          echo "ğŸ“ Note: All traffic goes through frontend. Frontend proxies /api to backend internally."
        fi
        
        # ë‚´ë¶€ ì„œë¹„ìŠ¤ ì •ë³´
        echo ""
        echo "ğŸ—ï¸ Internal Services:"
        echo "ğŸ”§ Frontend Service: witple-frontend-service:80 (exposed via ALB)"
        echo "ğŸ”§ Backend Service: witple-backend-service:80 (ClusterIP only)"
        echo ""
        echo "ğŸ“Š Load Balancer:"
        echo "âš¡ Frontend ALB: $FRONTEND_ALB_DNS (handles all traffic)"
        
        echo ""
        echo "ğŸ‰ Deployment completed successfully!"
        echo "ğŸ“Š Summary:"
        echo "- Backend: Deployed"
        echo "- Frontend: Deployed"
        echo "- K8s: Updated"
